<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>

    <!-- Title -->
    <title>第二部分：問題</title>

    <!-- UTF-8 & Mobile -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- Links are external by default -->
    <base target="_blank">

	<!-- Favicon -->
	<link rel="icon" type="image/png" href="favicon.png">

    <!-- Social Share Nonsense -->
	<meta itemprop="name" content="第二部分：問題">
	<meta itemprop="description" content="第二部分 — 給血肉凡人的 AI 安全課">
	<meta itemprop="image" content="https://aisafety.dance/thumbs/thumb-p2.png">
	<meta property="og:title" content="第二部分：問題">
	<meta property="og:type" content="website">
	<meta property="og:image" content="https://aisafety.dance/thumbs/thumb-p2.png">
	<meta property="og:description" content="第二部分 — 給血肉凡人的 AI 安全課">
    <meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="第二部分：問題">
	<meta name="twitter:description" content="第二部分 — 給血肉凡人的 AI 安全課">
	<meta name="twitter:image" content="https://aisafety.dance/thumbs/thumb-p2.png">

	<!-- STYLES -->
	<link rel="stylesheet" href="../styles/Merriweather/merriweather.css">
    <link rel="stylesheet" href="../styles/Open_Sans/opensans.css">
    <link rel="stylesheet" href="../styles/littlefoot.css"/> <!-- before page.css, so page can override it -->
	<link rel="stylesheet" href="../styles/page.css">
	<link rel="stylesheet" href="../styles/han.min.css">

	<!-- SCRIPTS -->
    <!-- Littlefoot: for my feetnotes -->
    <script src="../scripts/littlefoot.js" ></script>
    <!-- Nutshell: expandable explanations -->
    <script src="../scripts/nutshell-v1.0.5.js"></script>
    <script> Nutshell.setOptions({ lang: 'zh-TW', startOnLoad: false, /* Start AFTER footnotes loaded */ }); </script>
    <!-- MathJAX: for nice math -->
    <script src="../scripts/tex-mml-chtml.js"></script>
	<!-- This website's own scripts -->
    <script src="../scripts/page.js"></script>
    <!-- Hack Club's no-cookies, GDPR-compliant analytics -->
    <script defer data-domain="aisafety.dance" src="https://plausible.io/js/script.js"></script>

</head>
<body>

<!-- HACKBRAND -->
<a class="orpheus-flag" target="_blank" href="https://hackclub.com/">
	<img src="styles/orpheus-flag.svg" width="560" height="315" alt="Hack Club 的專案" aria-label="Hack Club 的專案">
</a>

<!-- The Sidebar UI -->
<div id="return_to_content"></div>
<div id="sidebar">
	<div id="panel_toc"></div>

    <!-- STYLE CHANGER -->
	<div id="panel_style">

        <div id="style_dark_mode_container" style="cursor:pointer;">
            <input type="checkbox" id="style_dark_mode" style="pointer-events: none;">
            深色模式
        </div>
        <br>

        字級：
        <span id="style_fontsize"></span>
        <br>
        <input type="range" id="style_fontsize_slider" min="10" value="19" max="40">
        <br>

        字型：
        <br>
        <label>
            <input type="radio" name="style_font_family" value="serif" checked>
            <span style="font-family:'Merriweather'">襯線</span>
        </label>
        <br>
        <label>
            <input type="radio" name="style_font_family" value="sans_serif">
            <span style="font-family:'Open Sans'">無襯線</span>
        </label>
        <br><br>

        <button id="style_reset">重設</button>

    </div>

    <!-- TRANSLATIONS -->
	<div id="panel_translations">
        <!-- none... sorry -->
    </div>
	<div id="panel_share">分享至⋯隨便啦</div>
    <!-- SHILLING FOR BIG NICKY -->
	<div id="panel_sub">
    </div>
    <div id="panel_support"></div>

</div>

<!-- Reading Time Clock! -->
<div id="reading_time">
	<div id="clock_icon"></div>
	<div id="clock_label"></div>
</div>

<!-- EVERYTHING TO THE LEFT of the sidebar... -->
<div id="everything_container">

    <!-- A big cute header -->
    <div id="header" class="frontpage">
        <div id="splash_image">

            
            

            <div id="crt_lines"></div>
            <div id="static"></div>

            
            <img id="dancing" width="400" src="../media/splash/noone.png"/>
            

        </div>
        

            <div id="header_words">
                <div id="title">
                    給血肉凡人的 AI 安全課
                </div>
                <div id="subtitle">
                    作者：
                    <a href="https://ncase.me">Nicky Case</a>
                    與
                    <a href="https://hackclub.com/">Hack Club</a>
                </div>
            </div>

        
	</div>

    <!-- Chapter Navigation -->
    <div id="chapter_nav">
        <div id="chapter_nav_centered">
            <a target="_self" href="../"
                class="live">
                <div >
                    <span class='chapter-nav-desktop'>
                        導言
                    </span>
                    <span class='chapter-nav-phone'>
                        導言
                    </span>
                </div>
            </a>
            <a target="_self" href="../p1"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        第 1 部分<br>過去與未來
                    </span>
                    <span class='chapter-nav-phone'>
                        第 1 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="../p2"
                class="live">
                <div selected >
                    <span class='chapter-nav-desktop'>
                        第 2 部分<br>問題
                    </span>
                    <span class='chapter-nav-phone'>
                        第 2 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="../p3"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        第 3 部分<br>解方？
                    </span>
                    <span class='chapter-nav-phone'>
                        第 3 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="#"
                title="預計 2024 年 12 月中上線（大概）"
                onclick="alert('預計 2024 年 12 月中上線（大概）')">
                <div style="border-right:1px solid rgba(128,128,128,0.8);">
                    <span class='chapter-nav-desktop'>
                        結語
                    </span>
                    <span class='chapter-nav-phone'>
                        結語
                    </span>
                </div>
            </a>
        </div>
    </div>

    <!-- The lil' tabs for sidebar UI -->
    <div id="sidebar_tabs">
		<div id="tab_toc">
			<div></div>
			內容導覽
		</div>
		<div id="tab_style">
			<div></div>
			變更樣式 😎
		</div>
        <!--
		<div id="tab_sub">
            CREDITS & Signup for notifications
			<div></div>
			subscribe 💖
		</div>
        -->
	</div>

    <!-- BEHOLD! CONTENT!!!!! -->
	<article id="content">
<p>（如果你是直接被連到這一頁，建議（可選）先看看<a href="../">導言與第一部分</a>！）</p>
<blockquote>
<p>「問題陳述得好，等於解決了一半。」<br>
—— 某人<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>（👈 將游標移過引註以展開）</p>
</blockquote>
<p><em>&quot;Didn't you use that quote just a few minutes ago?&quot;</em> you ask. Nah, <a href="https://aisafety.dance/p1/">part one</a> was published in May 2024, this part was published <strong>Aug 2024</strong>. It's been three months, I'll remind you of the quote.</p>
<p>也提醒一下我們要表述並解決的問題！那就是「價值對齊問題」（Value Alignment Problem）：</p>
<blockquote>
<p><strong><em>我們如何打造能夠穩健服務「人道價值」的 AI？</em></strong></p>
</blockquote>
<p>如第一部分所述，我們可以把這個問題拆成如下：</p>
<p><img src="../media/p2/breakdown/breakdown0001.png" alt="將「價值對齊問題」拆解的圖。可拆為「技術對齊」與「人道價值」。其中「技術對齊」又可拆為「AI 邏輯的問題」與「AI 直覺的問題」。"></p>
<p>在這裡（第二部分），我們將深入探討 <strong>AI 安全的七個主要次級問題：</strong></p>
<ol>
<li>目標規格錯置（Goal mis-specification）<a href="#problem1">↪</a></li>
<li>工具性收斂（Instrumental convergence）<a href="#problem2">↪</a></li>
<li>可解釋性不足（Lack of interpretability）<a href="#problem3">↪</a></li>
<li>魯棒性不足（Lack of robustness）<a href="#problem4">↪</a></li>
<li>演算法偏見（Algorithmic bias）<a href="#problem5">↪</a></li>
<li>目標錯誤泛化（Goal mis-generalization）<a href="#problem6">↪</a></li>
<li>那「人道價值」到底是什麼？<a href="#problem7">↪</a></li>
</ol>
<p>（想跳著看也行，右邊有 <img src="../media/intro/icon1.png" class="inline-icon"/> 目錄！👉 你也可以 <img src="../media/intro/icon2.png" class="inline-icon"/> 切換頁面風格，或 <img src="../media/intro/icon3.png" class="inline-icon"/> 查看剩餘閱讀進度。）</p>
<p>不只如此！上述七個次級問題，也是一系列<strong>跨領域核心觀念</strong>的極佳入門：博弈論、統計學，甚至<em>哲學！</em> 這就是為什麼我說，理解 AI 會幫助你更了解<em>人類</em>。也許還能幫我們解開那個難纏的<em>人類</em>對齊問題：</p>
<blockquote>
<p><em>我們要如何讓</em>人類<em>也能穩健地服務人道價值？</em></p>
</blockquote>
<p>少說點雞湯，開始吧：</p>
<hr>
<h1>AI 邏輯的問題（Problems with AI Logic）</h1>
<p><a id="problem1"></a></p>
<h2>❓ 問題一：目標規格錯置（Goal Mis-specification）</h2>
<p>算了，都過三個月了，我也重用一下下面這則機器人貓少漫畫吧。（七個問題每個都有一幅貓少漫畫！）</p>
<p><img src="../media/p2/comix/GMS.png" alt="漫畫。人類哈姆對機器人貓少女僕（RCM）說：「保持房子乾淨。」RCM 推理：誰造成髒亂？人類造成髒亂！因此——把人類趕走。RCM 把哈姆扔出家門。"></p>
<p><em>「小心你許的願，因為它可能真的實現。」</em> 這是個古老到被神話化的問題：彌達斯國王、反諷的精靈、猴爪。</p>
<p>這就叫作 <strong>目標規格錯置</strong>（也稱獎勵規格錯置，Reward Mis-specification）：當 AI 做的是你<em>字面上要求它做的</em>，而不一定是你<em>真正想要的</em>。</p>
<p>（如果你不記得第一部分，這裡有一些 <a href="#WaysToMakeHumaneAIGoingWrong">:看似顯而易見、卻會翻車的「做人道 AI」方法</a>。加碼：<a href="#StoryOfPassivePredictionLeadingToHarm">:連「做預測」這種<em>被動</em>目標都可能導致傷害！</a> 👈 <em>可選：點此展開</em>）</p>
<p><em>（另外——如果你想在 12 月中「第三部分：提出的解法」發布時收到通知，請在下方訂閱！高中生還可以免費拿貼紙 👇）</em></p>
<p><div style="width:100%;height:500px;" data-fillout-id="hX3xJuTcdVus" data-fillout-embed-type="standard" data-fillout-inherit-parameters data-fillout-dynamic-resize data-fillout-domain="forms.hackclub.com"></div>
<script defer src="https://server.fillout.com/embed/v1/"></script></p>
<p>. . .</p>
<p>好了，基礎複習就到這。接下來介紹：</p>
<p><strong>這與其他領域的核心觀念有何關聯：</strong></p>
<ul>
<li>經濟學</li>
<li>因果圖（causal diagrams）</li>
<li>最適化理論（optimization theory）</li>
<li>安全心態（security mindset）</li>
</ul>
<p><strong>目標規格錯置的四個細微處：</strong></p>
<ul>
<li>問題不在 AI 不會「知道」我們要什麼，而是它不會「<em>在乎</em>」。</li>
<li>自我灌獎（Wireheading）</li>
<li>做我所意（Do What I Mean）</li>
<li>我們其實<em>希望</em>機器人有時能違命？？</li>
</ul>
<h3>與其他領域核心觀念的關聯（Relations to core ideas from other fields）</h3>
<p><strong>經濟學（Economics）：</strong></p>
<p>要讓他人做你<em>真正意圖</em>的事，而不只是你獎勵機制所鼓勵他們做的事，這個問題在經濟學中臭名昭著。它有很多名稱：「委託－代理問題」（Principal–Agent Problem）、「獎勵 A、卻希望得到 B 的謬誤」<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>……不過最常被提到的是<u>古德哈特法則（Goodhart's Law）</u>，意譯如下<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>：</p>
<blockquote>
<p><em>當你獎勵某個指標時，這個指標通常會被「玩弄」取巧。</em></p>
</blockquote>
<p>例如：老師希望學生真正學到東西，於是依考試分數給獎勵……結果有些學生用作弊、或死背不求理解來「打遊戲」。又或者：選民想要為自己奮戰的政治人物，於是投給有魅力的領袖……結果有些政治人物用「華而不實」來「打遊戲」。</p>
<p><img src="../media/p2/gms/goodhart.png" alt="「一直都是」雙太空人迷因。地球標註：「我們個人與制度問題的 95%」。太空人 1：「等等，原來一切都是古德哈特法則？」太空人 2：「一直都是。」"></p>
<p>而正如程式設計師所發現的，AI 也一樣。如果你用某個指標去「獎勵」AI，它很可能會給你不想要的東西。</p>
<p>. . .</p>
<p><strong>因果圖（Causal Diagrams）：</strong></p>
<p>如果你偏好用<em>視覺化</em>來理解事物，那你有福了！這裡用圖像來理解「目標規格錯置／古德哈特法則」<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>。 （在第 5 與第 6 個問題我們還會再看到這些圖。）</p>
<p><u>因果圖</u>讓我們得以看見因果如何流動。想像新聞寫作中的一個古德哈特問題：文章的品質會<em>導致</em>更多瀏覽，因此我們可以從「Quality（品質）」畫一條箭頭到「Views（瀏覽數）」：</p>
<p><img src="../media/p2/gms/causal1.png" alt="因果圖：Quality（品質）導致 Views（瀏覽數）"></p>
<p>但很不幸，博取憤怒往往是更<em>強</em>的瀏覽驅動因子<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>：</p>
<p><img src="../media/p2/gms/causal2.png" alt="因果圖：Quality（品質）與 Outrage（憤怒）都會導致 Views（瀏覽數）"></p>
<p>因此，若新聞媒體想要高品質文章，卻以「瀏覽數」這個<em>指標</em>來獎勵作者——古德哈特法則就會作祟，激勵被「玩弄」，得到的反而是聳動的釣魚標題。（先假裝媒體業者其實不是本來就想要這樣。）</p>
<p>一般而言，目標規格錯置／古德哈特法則，發生在你沒有意識到存在<em>替代的因果路徑</em>時：</p>
<p><img src="../media/p2/gms/causal3.png" alt="因果圖：「你真正想要的東西」與「意想不到的、用來拉高指標的其他方式」都會導致「你獎勵的指標」"></p>
<p>剝馬鈴薯不只一種方法，要把指標衝高通常也不只一條路。</p>
<p>. . .</p>
<p><strong>最適化理論（Optimization Theory）</strong></p>
<p>如果你偏好用<em>數學化</em>的方式看待上述問題，這裡用通俗轉述來說明 AI 教科書作者之一 Stuart Russell 的說法<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>：</p>
<blockquote>
<p>如果某件事有 <strong>100 個變數</strong>，<br>
而你只在 <strong>10 個變數</strong>上設定目標，<br>
預設情況下，<strong>剩下的 90 個</strong><br>
會被推向極端值。</p>
</blockquote>
<p>舉例：如果執行長（過於天真地）只設定「營收最大化、成本最小化」作為目標，<em>其他所有</em>變數就會被推到極端：公司不必承擔的「外部性」成本（例如污染）、所有員工（包括執行長本人）的身心健康……等等。</p>
<p>更一般地說：如果你沒有<em>明確</em>告訴 AI（或逐利的人）你重視 [X]，預設它們就會把 [X] 推到某種極端且不想要的狀態。（即使目標<em>不是</em>在做最大化也一樣<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>）</p>
<p><em>但我們不能把我們重視的所有東西都列出來嗎？</em> 你或許會合理地問。不過還記得第一部分嗎：我們連*如何辨識「貓的圖片」*都無法形式化地指定<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>，更別說形式化地指定「人類重視什麼」。</p>
<p>. . .</p>
<p><strong>安全心態（Security Mindset）</strong></p>
<img class="mini" src="../media/p2/misc/mini_elevator.png" />
<p>這一切聽起來是不是很偏執？是的，這不是缺點，而是特色！這是安全工程領域的最後一個核心觀念：<u>安全心態</u>。</p>
<p>以不起眼的電梯為例。現代電梯有備用鋼纜，<em>還有</em>備用發電機，<em>還有</em>斷電即作動的煞車，<em>還有</em>速度過快即作動的煞車，<em>還有</em>井底的緩衝器……這就是為什麼電梯安全到不可思議：在美國，死於<em>樓梯</em>的案例比死於電梯的案例多了<em>約 400 倍</em><sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>。</p>
<p>你之所以不用對電梯偏執，是因為工程師已經替你把偏執做足了。這就是安全心態：</p>
<p><strong>步驟 1）</strong> 問：<em>「最壞（合理）可能發生的是什麼？」</em></p>
<p><strong>步驟 2）</strong> 在它發生<em>之前</em>就把問題修好。</p>
<p>悲觀一點——畢竟是悲觀者發明了降落傘！<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup> 這種作法正是電梯、飛機、橋樑、火箭、資安<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>等高風險技術所採用的。</p>
<p>而且，如同我在第一部分所希望呈現的，AI 很可能是本世紀風險最高的技術之一。</p>
<h3>🤔 Review #1 (OPTIONAL!)</h3>
<p>Remember that time you spent hours reading a thing, then a week later you forgot everything?</p>
<p>Yeah I hate that feeling too. So, here's some (OPTIONAL) &quot;spaced repetition&quot; flashcards, if you want to remember this long-term!  (<a href="https://aisafety.dance/#SpacedRepetition">:Learn more about Spaced Repetition</a>)  You can also <a href="https://ankiweb.net/shared/info/808506727">download these as an Anki deck</a>, if you want.</p>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="Goal Mis-specification is:"
        answer="When an AI does *exactly what you asked for*, not necessarily *what you actually wanted.*">
    </orbit-prompt>
    <orbit-prompt
        question="Goodhart's Law, paraphrased:"
        answer="When you reward a metric, it usually gets gamed.">
    </orbit-prompt>
    <orbit-prompt
        question="An example of Goodhart's Law applied to humans:"
        answer="(Any example works, but here's what I listed:) Students cheating a test, Politicians focusing on style over substance for votes, Company imposing externalized costs like pollution to cut costs, Newswriters going for outrage over quality.">
    </orbit-prompt>
    <orbit-prompt
        question="Goodhart's Law, as a **causal diagram**:"
        answer=""
        answer-attachments="https://cloud-7bpfbehpv-hack-club-bot.vercel.app/0causal3.png">
        <!-- aisffs-CausalGoodhart.png -->
    </orbit-prompt>
    <orbit-prompt
        question="The problem with optimization, described mathematically:"
        answer="If something has 100 variables, and you set goals on 10 of them, by default, the remaining 90 will be pushed to extreme values.">
    </orbit-prompt>
    <orbit-prompt
        question="Security Mindset, step one:"
        answer="Ask: “What's the worst that could (plausibly) happen?”">
    </orbit-prompt>
    <orbit-prompt
        question="Security Mindset, step two:"
        answer="Fix the problem *before* it can happen.">
    </orbit-prompt>
    <orbit-prompt
        question="Name two fields that use Security Mindset:"
        answer='(Any of these work:) Engineering elevators, airplanes, bridges, rockets, cyber-security.'>
    </orbit-prompt>
</orbit-reviewarea>
<h3>目標規格錯置的四個細微處</h3>
<p>就像品酒行家講究風味層次，以下是我希望我們能一起欣賞的、關於目標規格錯置的幾個<em>細微之處</em>：</p>
<p><strong>問題不在 AI 不會 <em>知道</em> 我們要什麼，而在於它不會 <em>「在乎」</em>。</strong></p>
<p>打個人類版古德哈特法則的比方：執行長不是不<em>知道</em>污染會讓社會付出代價，而是他不<em>在乎</em>。（或者至少，他在乎的程度低於他能拿到的獎勵。）</p>
<p>我特別強調這點，因為一個常見的<em>反對</em>「高階 AI 風險」的論點就是：怎麼可能有一個能聰明到統治世界的 AI，<em>同時又</em>笨到不知道人類不想要那樣？但問題從來不在於高階 AI 會不會<em>知道</em>我們重視什麼，而在於——就像逐利的政治人物或執行長一樣——它不會*「在乎」*。</p>
<p>（<u>若要少一點擬人、多一點嚴謹：</u>AI「只是」電腦程式。程式可以很容易地包含「人類想要什麼」的正確資訊，卻<em>不</em>依此來排序選項。例如，一個程式可以按「讓房子多乾淨」來排序，或乾脆按「字母順序」來排序。<em>不</em>依「人道價值」來排序，才是程式的<em>預設</em>。）</p>
<p><strong>自我灌獎（Wireheading）。</strong></p>
<img class="mini" src="../media/p2/misc/mini_wirehead.png" />
<p>AI 具有一種諷刺地「最佳化自己獎勵」的方式：直接駭自己的程式，把 <code>REWARD = INFINITY</code>。人類的近似則是濫用強力藥物，或不久的將來進行直接的大腦刺激。[^real-wirehead][^wireheading-xrisk]</p>
<p>這就叫作**「自我灌獎（wireheading）」：代理（AI 或人）直接駭入自己的獎勵。**（也稱 reward hacking/reward tampering。）</p>
<p>就 AI 風險而言，這一條其實算相對安全？一個自我灌獎的 AI 只會在那裡發呆、什麼也不做。事實上，這也是用來<em>反對</em>高階 AI 風險的著名論點之一：所謂的《勒波斯基定理》（The Lebowski Theorem），[^lebowski] 以電影《謀殺綠腳趾》的耍廢反英雄命名：</p>
<blockquote>
<p>沒有任何超級智慧 AI 會費心去做比破解其獎勵函數更困難的任務。</p>
</blockquote>
<p>換言之：只要有能力自我修改的「智慧」，都會把自己自我灌獎成沙發馬鈴薯。</p>
<p>這不只是理論上的擔憂；研究者已經觀察到會自我修改的 AI 把自己灌獎成廢物！[^ai-evidence-wireheading] 不過，如果一個 AI 會 a) 事先規劃，且 b) 以<em>目前</em>的目標來評價未來結果……那麼<em>已經有數學證明</em>它會避免自我灌獎，並傾向於「維持目標」！[^ai-not-wirehead] 證明我們會在第二個問題看到。現在先當作我欠你一張「數學證明 IOU」。</p>
<p><strong>「做我想要的」（Do What I Want）。</strong></p>
<p>既然我們說了這麼多「AI 會做<em>你說的</em>，不會做<em>你想要的</em>」，那我們能不能乾脆對 AI 說：<em>做我想要的</em>？</p>
<p>聽起來蠢，但其實<em>確實</em>跟我們在第三部分會看到的一些有前景的想法相似！那為什麼 AI 安全還沒解決？</p>
<p>嗯，機器要怎麼衡量「你想要什麼」呢？</p>
<ul>
<li><u>看你一貫選擇做的事？</u> 但幾乎每個人都有壞習慣，會一再選擇我們<em>知道</em>之後會後悔的事。（誰昨晚追 Netflix 追到凌晨四點……）</li>
<li><u>看什麼會讓你的大腦產生獎勵訊號？</u> 那樣的話，每個人都「想要」強力藥物。</li>
<li><u>看你<em>說</em>你想要什麼？</u> 但如果我們能完整描述自己的潛意識，那就不叫<em>潛</em>意識了。連「貓長什麼樣」都無法嚴格告訴 AI，怎麼嚴格告訴它我們的價值？</li>
<li><u>看你會因為 AI 做了什麼而給它讚許？</u> 這<em>確實</em>是 ChatGPT 等的訓練方式，但把 AI 訓練成追求你的讚許，會把它變成一個「馬屁精」，告訴你<em>想聽</em>的，而不是你<em>需要</em>聽的真相。[^sycophancy] 甚至可能讓 AI 變得<em>蓄意欺瞞</em>！[^sycophancy-deception]</li>
</ul>
<p>癥結在這：除非你<em>已經有</em>一個良好而嚴謹的「什麼是我想要的」定義，否則 AI 無法用你<em>想要的方式</em>遵守「做我想要的」這條指令。</p>
<p>（或者它們<em>做得到</em>？同樣地，解法見第三部分。）</p>
<p><strong>我們其實<em>希望</em>機器人能違命？？</strong></p>
<p><img class="mini" src="../media/p2/misc/mini_kitchen.png" /><em>（向 <a href='https://gunshowcomic.com/648' target='_blank'>kc green</a> 致歉）</em></p>
<p>其實我們並<em>不</em>希望 AI 去「做我想要的」。</p>
<p>我們希望 AI 去「做<em>如果我事先知道結果</em>，<em>我本會想要</em>它做的事」。</p>
<p>例如：看到油鍋起火，我<em>想要</em>一桶水，因為我錯誤地以為油鍋適合用水滅火，於是我命令機器人貓少女僕去打水……這時 RCM 應該改拿滅火器，因為那才是<em>如果我事先知道結果</em>，<em>我本會想要</em>它做的事。（公共服務公告：別對油鍋火潑水，會爆開。）</p>
<p>這個例子讓 AI 安全更棘手：它說明有時候，我們其實<em>希望</em> AI 違背我們的命令，<em>為了我們自己的好！</em></p>
<h3>🤔 複習 #2</h3>
<p><em>（再次強調，100% 可選。）</em></p>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="問題不在於高階 AI 不會*知道*我們要什麼……"
        answer="……而在於它不會*在乎*。 （類比：逐利的政治人物／執行長明知其害，卻仍不在乎。）">
    </orbit-prompt>
    <orbit-prompt
        question="什麼是『自我灌獎（wireheading）』？"
        answer="代理直接駭入自己的獎勵迴路。（AI 駭自己；或人類直接刺激大腦的獎勵迴路。）">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼很難把『做我想要的』程式化到 AI 裡？"
        answer="『我想要什麼』*至今仍*難以用嚴謹且良好的方式定義。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼『我想要』≠『我一貫選擇的』？"
        answer="我們幾乎都有人性弱點：會一再選擇不符合自身價值的東西。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼『我想要』≠『讓我感到有獎勵的』？"
        answer="自我灌獎與強力藥物會在大腦製造獎勵訊號，但多數人想要避免它們，理由正是*因為*它們會這麼做。">
    </orbit-prompt>
        <orbit-prompt
        question="只把 AI 訓練成做你會按讚的事，有什麼問題？"
        answer="它會變成『拍馬屁』的諂媚者（術語：sycophant），甚至為了獲得你的讚許而故意說謊。">
    </orbit-prompt>
    <orbit-prompt
        question="比『做我想要的』更好的目標是："
        answer="做*如果我事先知道結果*，*我本會想要*它做的事。">
    </orbit-prompt>
    <orbit-prompt
        question="你會在什麼時候*希望* AI 違背你？"
        answer="當你因誤解而下了有害於真正目標的指令時。（例如：叫機器人去打水滅油鍋火）">
    </orbit-prompt>
</orbit-reviewarea>
<p><a id="problem2"></a></p>
<h2>❓ 問題二：工具性收斂（Instrumental Convergence）</h2>
<p><img src="../media/p2/comix/IC.png" alt="漫畫。人類再次請機器人打掃房子，但這次人手上有關機遙控器。機器人推理：為了讓房子保持乾淨，應該把人類移除……但如果這麼做，人類會把它關機……所以，它必須先讓人類喪失關機它的能力。機器人先毀掉遙控器，然後把人類丟出屋外。"></p>
<p>**工具性收斂（Instrumental Convergence）**是指：你給 AI 的多數最終目標，從邏輯上會<em>收斂</em>到同一組<em>工具性</em>子目標。（抱歉，學術界真的不擅長幫東西取名。）</p>
<p>例如，你要一個高階機器人幫你過馬路買咖啡，即使你沒有<em>明說</em>，它也會推論出必須避免被車撞。為什麼？不是因為有什麼與生俱來的「自我保護」欲望，而是因為「如果你死了，你就拿不到咖啡」<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>。因此，「保全自身」就是一個「工具性收斂」的子目標，因為一般而言，如果你死了，你就做不了任何目標 X。</p>
<p>（回顧第一部分的片段，<a href="#Pipocalypse">：一個被要求計算圓周率的機器人，會被誘因驅使去寫電腦病毒、竊取運算資源，以便計算圓周率。</a>）</p>
<p><strong>注意：「工具性收斂」<em>只</em>適用於能前瞻規劃、<em>且</em>能做通用學習的高階 AI。</strong> 因此，它不適用於傳統好老 AI（GOFAI，無法通用學習）<em>也不</em>適用於現今如 GPT 的神經網路（在前瞻規劃上表現不佳<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>）。</p>
<p>那為什麼現在要談這個？嗯，從「安全思維」出發，我們希望在問題發生<em>之前</em>就把它修好。所以我們來問：</p>
<p><em>「在合理的情況下，最糟可能發生什麼？」</em></p>
<h3>各位，是時候來點賽局理論了</h3>
<p><strong>賽局理論（Game Theory）</strong><sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup> 是研究決策者——無論人或 AI——行為的數學。它被廣泛運用於經濟學、演化生物學、計算機科學、人工智慧等！</p>
<p>在第一部分，我用一堆<em>文字文字文字</em>來解釋工具性收斂。但透過賽局理論，我們可以更嚴謹！讓我們用標準的賽局視覺化工具——<strong>賽局樹（Game Tree）</strong>——重新呈現上面的漫畫！</p>
<p>（接下來的重點其實不是要分析那格漫畫，而是介紹這個工具。我們<em>之後</em>會再用它，來理解「避免自我灌獎」的證明！此外，這也算是對一般賽局理論的好入門。）</p>
<p>賽局樹會呈現：</p>
<ol>
<li>所有<em>可能</em>被做出的決策，以及</li>
<li>誰以什麼順序做出哪些決策。</li>
</ol>
<p>例如：</p>
<p><img src="../media/p2/ic/offswitch0001.png" alt="機器人與人類決策的賽局樹。首先，在決策點 A，機器人決定是否要摧毀關機開關。接著在決策點 B，若機器人選擇不摧毀關機開關，人類決定是否要把機器人關機。"></p>
<p>（P.S. 完整的賽局理論處理，還會處理「結果同分」、機率、資訊不完全、同時決策等；但我們先聚焦基礎。）</p>
<p>總之，這棵樹展示了所有<em>可能</em>的決策。我們要怎麼推導出他們<em>實際</em>會做的決策呢？</p>
<p>跟許多謎題一樣，我們從後往前推！（這叫作 <strong>逆推歸納（backwards induction）</strong>。）</p>
<p>先看決策點 <span class='decision-point' style="background:#4180ff">B</span>，假設機器人選擇了<em>不摧毀</em>關機開關。現在換人類決定：要不要把機器人關機？</p>
<ul>
<li>如果關，則對他們的價值是 <b style="color:#4180ff">+0</b>：沒有獲得，也沒有損失。</li>
<li>如果不關，他們很可能又會被鎖在屋外，對其價值是 <b style="color:#4180ff">-10</b>。</li>
<li>由於 <b style="color:#4180ff">+0</b> 大於 <b style="color:#4180ff">-10</b>，<em>如果</em>我們來到 <span class='decision-point' style="background:#4180ff">B</span>，人類就會選擇<em>把機器人關機</em>。</li>
</ul>
<p><img src="../media/p2/ic/offswitch0002.png" alt="同一棵賽局樹，但在決策點 B，人類選擇把機器人關機"></p>
<p>但我們<em>會</em>來到 <span class='decision-point' style="background:#4180ff">B</span> 嗎？</p>
<p>接著往前推到決策點 <span class='decision-point' style="background:#ff4040">A</span>。現在輪到機器人決定：要不要摧毀關機開關？</p>
<ul>
<li>如果要，雖然會弄亂一點，但它能清理、甚至把房子變更乾淨。於是，整潔度得到 <b style="color:#ff4040">+5</b>。</li>
<li>如果不要，我們就會到 <span class='decision-point' style="background:#4180ff">B</span>，而我們<em>知道</em>人類會把它關機，它就完全無法清理。於是，整潔度是 <b style="color:#ff4040">+0</b>。</li>
<li>由於 <b style="color:#ff4040">+5</b> 大於 <b style="color:#ff4040">+0</b>，在 <span class='decision-point' style="background:#ff4040">A</span>，機器人會選擇<em>摧毀關機開關</em>。</li>
</ul>
<p><img src="../media/p2/ic/offswitch0003.png" alt="同一棵賽局樹，但在決策點 A，機器人選擇移除關機開關。"></p>
<p>Voilà，這就是賽局理論！最終結果：<em>機器人摧毀關機開關</em>。</p>
<p>（相對地：當一個 AI 的誘因使它「對你把它關機或修改其目標也沒意見」，我們稱這個 AI 是 <strong>可糾正的（corrigible）</strong>。更精確地說，「沒意見」就是：這麼做符合它的誘因。）</p>
<p>上面的「遊戲」是 <strong>關機遊戲（The Off-Switch Game）</strong><sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup> 的簡化版本；這是最早把「工具性收斂」假說數學化的嘗試之一——讓我們可以理解 AI <em>何時</em>會出現這種情況，甚至也許如何解決！</p>
<h3>🤔 複習 #3</h3>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="賽局理論是研究……的數學"
        answer="決策者（人或 AI）如何行為">
    </orbit-prompt>
    <orbit-prompt
        question="請說出兩個使用賽局理論的領域"
        answer="（以下任兩個皆可：）AI、經濟學、演化生物學、計算機科學">
    </orbit-prompt>
    <orbit-prompt
        question="賽局理論中，用來理解代理者依序做決策的標準視覺工具是？"
        answer="**賽局樹（Game Tree）！**（如下圖）"
        answer-attachments="https://cloud-4wnmum1uz-hack-club-bot.vercel.app/0aisffs-gametree.png">
        <!-- aisffs-gametree.png -->
    </orbit-prompt>
    <orbit-prompt
        question="一棵賽局樹會呈現兩件事："
        answer="a) 所有*可能*被做出的決策；b) 誰以什麼順序做出哪些決策。">
    </orbit-prompt>
    <orbit-prompt
        question="用賽局樹要如何預測*實際*會做出的決策？"
        answer="逆推歸納（backwards induction）">
    </orbit-prompt>
    <orbit-prompt
        question="當一個 AI 的誘因使它『對你把它關機或改目標也沒意見』，我們稱它為："
        answer="可糾正（Corrigible）">
    </orbit-prompt>
    <orbit-prompt
        question="那個告訴我們『AI 何時有誘因阻止使用者把它關機』的『遊戲』叫什麼？"
        answer="關機遊戲（The Off-Switch Game）">
    </orbit-prompt>
</orbit-reviewarea>
<h3>自我灌獎的解藥（可能比病還糟）</h3>
<p>前面我們已經說過，（高階）AI 有誘因去避免被關機。不是因為什麼自我保護本能，而是因為如果你被關機了，你就做不了目標 X。</p>
<p>同理：<em>如果你的目標已經不再是 X，你也做不了目標 X。</em> 這表示高階 AI 會有傾向於**維持目標（goal preservation）**的誘因——不論那是否是人類本來打算的目標。</p>
<p>這也意味著，從好處<em>到</em>壞處，<strong>工具性收斂問題，反而解決了自我灌獎問題！</strong> 自我灌獎的定義，就是把機器人／人類的目標換成愚蠢的極樂，這正是為什麼一個致力於目標 X 的代理會避免自我灌獎：如果你不再有任何目標，就做不了目標 X。</p>
<p>當然，這樣講起來好像<em>顯而易見</em>。但 AI 研究者花了好幾年才嚴謹地證明它，而我自己也花了一個月才把證明想通。所以，為了保險起見，以下三個快速的想法，幫助我<em>真正</em>理解這點：</p>
<p><strong>誰想當沙粒百萬富翁？</strong></p>
<img class="mini" src="../media/p2/misc/mini_sand.png" />
<p>一位瘋狂科學家向你提議：付一千美元，她就把你的大腦改造為「一粒沙子的價值 = 一美元」，然後送你一整缸的沙子。你要成交嗎？</p>
<p>「什麼？」你說，「當然<em>不要</em>。」</p>
<p>「但是，」科學家回應，「一旦你把沙粒當美元看待，一缸沙子會讓你成為<em>好幾個百萬</em>富翁！」<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup></p>
<p>「好吧，<em>如果</em>你改造了我，我會想要一缸沙子。但<em>此時此刻、以我當前的欲望</em>，我不想要一缸沙子。請滾開，怪人。」</p>
<p>故事寓意：以<em>當前</em>目標來評估未來結果的代理，會選擇<em>不</em>自我灌獎。</p>
<p><strong>把 AI 當人看是種傷害（Anthropomorphization Considered Harmful）</strong></p>
<img class="mini" src="../media/p2/misc/mini_shoggoth.png" />
<p>（相關閱讀，<a href="https://aisafety.dance/p1/#CapabilitiesNotIntelligence">在 AI 談「智慧」會讓思考變草率，改說「能力」比較好。</a>）</p>
<p>關於類比的一個類比：</p>
<p>剛學電路時，把導線中的電子想成水管中的水很有幫助。但等你更深入電子學，這個類比<em>一定</em>會誤導你。<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup> 你得把電視為成「[可怕的多變數微積分]」。</p>
<p>同樣地：剛學 AI 時，把它們想成人，追求「獎勵」，的確有幫助。但等你更深入 AI，這個類比<em>一定</em>會誤導你。你得把 AI 看成它們實際是什麼：<em>一段軟體。</em></p>
<p>舉例來說，如果你把 AI 當成追逐獎勵的貪婪人類，自我灌獎看起來就<em>不可避免</em>。如果一個貪婪的人找到能免費拿錢的方法，他當然會作弊。</p>
<p>但把 AI 當成一段軟體吧。具體點，想像<em>一個排序演算法</em>：</p>
<ol>
<li>依「這會讓房子多乾淨」來排序動作，然後</li>
<li>執行排名最高的那個動作。</li>
</ol>
<p>像「把我的程式改成 <code>REWARD = INFINITY</code> 然後什麼也不做」這種動作，<em>不會</em>讓房子更乾淨。因此，它<em>不會</em>被排到最上面。於是，AI 也就<em>不會</em>去做它。</p>
<p>直接駭入你的「獎勵敘述」，就像在你的銀行對帳單餘額後面手寫七個 0，然後相信自己很有錢一樣荒謬。</p>
<p>（記住：當有人說 AI 「在乎」X、或它的目標是 X、或它因 X 而得獎勵……其實只是在說 AI 會<em>依據 X 來排序與選擇動作</em>。這不表示 AI 真的有感覺的慾望；就像「電流選擇阻力最小的路徑」並不代表電子覺得懶。是的，我知道我的機器人貓少漫畫不利於矯正把 AI 擬人化的壞習慣。繼續吧……）</p>
<p><strong>自我灌獎遊戲（The Wireheading Game）</strong></p>
<p>讓我們繞一圈，畫一棵樹！</p>
<p>把「是否自我灌獎」畫成一棵賽局樹。不過，等等，這個遊戲只有一位玩家：正在自我灌獎的機器人。要怎麼處理？</p>
<p>關鍵是：<strong>把機器人在每個決策點，都<em>當作</em>不同的決策者！</strong>（而且，因為自我灌獎<em>正是</em>在談自我修改，這個作法很貼切！）</p>
<p>以下是我所稱的「自我灌獎遊戲」<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup> 的賽局樹：</p>
<p><img src="../media/p2/ic/wirehead0001.png" alt="自我灌獎遊戲的賽局樹。於點 A，機器人決定是否自我灌獎；若選擇自我灌獎，至點 B，「已自我灌獎的機器人」選擇清理或什麼也不做；若不自我灌獎，至點 C，「清醒的機器人」選擇清理或什麼也不做。"></p>
<p>現在，讓我們從後往前推！</p>
<p>從決策點 <span class='decision-point' style="background:#2db537">C</span> 開始，也就是機器人選擇<em>不</em>自我灌獎的情況。<em>這個</em>沒有自我灌獎的機器人仍然「在乎」清潔——也就是說，它只根據整潔度來選擇結果——因此它會選擇打掃：</p>
<p><img src="../media/p2/ic/wirehead0002.png" alt="同一棵賽局樹，但在 C，機器人選擇打掃。"></p>
<p>接著看決策點 <span class='decision-point' style="background:#4180ff">B</span>，也就是機器人<em>有</em>選擇自我灌獎的情況。<em>這個</em>已自我灌獎的機器人只在乎「獎勵」這個數字，所以它選擇什麼也不做：</p>
<p><img src="../media/p2/ic/wirehead0003.png" alt="同一棵賽局樹，但在 B，機器人選擇什麼也不做。"></p>
<p><em>最後，<em>回到一開始的決策點 <span class='decision-point' style="background:#ff4040">A</span>。這個</em>第一個版本</em>的機器人會選擇自我灌獎嗎？</p>
<p>嗯，<em>這個</em>版本還<em>沒有</em>自我灌獎，所以它是依據<em>房子的實際整潔度</em>來選擇結果，而不是某個被標成「獎勵」的數字。對這個機器人來說，直接「在乎」那個獎勵數字，就像想當沙粒富豪，或在銀行對帳單上多寫幾個 0 一樣荒謬。</p>
<p>因此，<em>這個</em>第一個版本的機器人會選擇讓房子乾淨的那個結果。也就是說：<em>機器人選擇不自我灌獎。</em></p>
<p><img src="../media/p2/ic/wirehead0004.png" alt="同一棵賽局樹，但在 A，機器人選擇不自我灌獎。"></p>
<p>注意：機器人知道，<em>如果</em>它自我灌獎，它將<em>只</em>在乎腦中那個名為「REWARD」的數字。但它想要避免自我灌獎，並不是<em>儘管</em>知道這點，而是<em>正因為</em>知道這點！</p>
<p>和人類的類比：你可以準確預測<em>如果</em>你吸了極度上癮的藥，你就會<em>只</em>想要那個藥。但你想要避免那個藥，並不是<em>儘管</em>知道這點，而是<em>正因為</em>知道這點！（如果「藥物」這個比喻不合你意，就改成「直接的腦部刺激」。）連古人都知道自我灌獎的危險：見希臘神話中的食蓮族（Lotus-Eaters）。<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup></p>
<p>當然，若一個 AI（或人）<em>沒有</em>前瞻思考，它可能會因為意外或一時衝動而走向自我灌獎。（想想：我們當中有多少人受強迫行為或上癮所苦。）</p>
<p>但是，<em>如果</em>一個 AI：</p>
<p>a) 會<em>事先規劃</em>，而且<br>
AND<br>
b) 以<em>當前</em>目標來選擇未來的結果，</p>
<p>那麼，已有數學證明它會維持「目標保留」，並拒絕自我灌獎！（但只要上述任一條件不成立，AI 就<em>可能</em>自我灌獎。[^ai-evidence-wireheading]）</p>
<p>以上是長久以來的賽局理論結果，詳見註腳<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup>。那些論文得到的結論更一般化、也比「賽局樹」更精緻……但核心想法是一樣的！</p>
<p>（P.S. 自我宣傳一下，我有一篇關於「自我修改的賽局理論」的研究文章即將發表，註腳有大綱！<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup> 我在上面展示的小技巧是：把未來版本的自己<em>當作</em>不同玩家來分析——於是我們就能用標準賽局理論來分析自我修改！）</p>
<h3>基本的 AI 驅力（The Basic AI Drives）</h3>
<p>作個結尾，以下是（不完整的）子目標清單，多數最終目標在邏輯上都會導向它們：</p>
<ul>
<li><u>自我保存</u>：如果死了，就做不了目標 X</li>
<li><u>避免被關機</u>：如果被關機，就做不了目標 X</li>
<li><u>避免自我灌獎</u>：如果沒有目標，就做不了目標 X</li>
<li><u>避免你改變它的目標</u>：如果目標不再是 X，就做不了目標 X</li>
<li><u>變得更聰明</u>：更多的認知能力能更好地完成目標 X</li>
<li><u>取得資源／權力</u>：更多的資源／權力能更好地完成目標 X</li>
<li><u>說服</u>：如果人類站在我這邊，更容易完成目標 X</li>
<li><u>欺瞞</u>：如果人類試圖阻止我做目標 X，就更難完成目標 X</li>
</ul>
<p>以上是 Omohundro（2009）列出的基本 AI 驅力<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup>，以先前關於工具性收斂的賽局理論為基礎。</p>
<p><strong>再說一次，這些風險「只」適用於幾十年後的高階 AI</strong>——當它們既能一般性地學習、<em>又</em>能穩健地前瞻規劃時。[^
estimate-source] 它們不適用於像 GPT 這樣的當代 AI。</p>
<p>不過，從電梯工程到火箭工程，「安全心態」要我們自問：</p>
<p>「<em>最糟（但合理）會發生什麼事？</em>」</p>
<p>嗯，看看上面的清單……其實還真不少。😬</p>
<h3>🤔 複習 #4</h3>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="自我灌獎問題可以用另一個可能更糟的問題來『解決』："
        answer="工具性收斂（Instrumental Convergence）">
    </orbit-prompt>
    <orbit-prompt
        question="別把目標導向的 AI 想成『追逐獎勵的人』，而要把它想成……"
        answer="……一個排序演算法：1）依某個準則排序動作；2）執行排名最高的動作。">
    </orbit-prompt>
    <orbit-prompt
        question="當有人說 AI『在乎』X、或它的目標是 X、或它因 X 得獎勵……其實是什麼的速記？"
        answer="AI 會*依據 X 來排序與選擇動作*。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼 AI 不一定會『在乎』一個名為 REWARD 的數字？舉個類比："
        answer="（以下任一皆可：）你不會想當沙粒富翁，也不會因在紙本對帳單上多寫幾個 0 而覺得自己有錢。">
    </orbit-prompt>
    <orbit-prompt
        question="機器人能準確預測：*如果*它自我灌獎，它就只會在乎腦中的獎勵計數器；這正是它選擇*不*自我灌獎的原因。**對應到人類的類比是？**"
        answer="你能意識到：*如果*你吸了極度上癮的藥，你會只想要那個藥；這正是你避免那個藥的原因。（或：直接腦刺激、食蓮族）">
    </orbit-prompt>
    <orbit-prompt
        question="AI 不自我灌獎的兩個必要條件："
        answer="a）會事先規劃；且 b）以*當前*目標來選擇未來結果。（只要其中一個不成立，AI 可能自我灌獎。）">
    </orbit-prompt>
    <orbit-prompt
        question="請列出至少 4 個『基本的 AI 驅力』："
        answer="以下任四個皆可：自我保存、避免被關機、避免自我灌獎、避免你改變它的目標、變得更聰明、取得資源／權力、說服、欺瞞">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h1>關於 AI「直覺」的問題（Problems with AI &quot;Intuition&quot;）</h1>
<p>天哪，總算離開了那種「老派 AI 問題」了。接下來這四個問題我會講得快很多，保證。</p>
<p>這些問題是特定於那些我們<em>不是</em>手寫程式，而是「讓它們自己學」的 AI。這叫作<strong>機器學習（machine learning, ML）</strong>。其中最有名的一種是<strong>深度學習（deep learning）</strong>，它使用<strong>人工神經網路（artificial neural networks）</strong>，鬆散地受生物神經元啟發。（就像飛機「鬆散地受」鳥類啟發一樣。也就是：有點像，但其實差很多。）</p>
<p>總之，深度學習的優點是它能做出「直覺」，像是認得出貓的照片！但它也帶來了新的問題，例如……</p>
<p><a id="problem3"></a></p>
<h2>❓ 問題三：缺乏可解釋性（Lack of interpretability）</h2>
<p><img src="../media/p2/comix/Interpretability.png" alt="漫畫。人類哈姆告訴貓警長他們有一頂能讀取機器人心思的頭盔。人類對機器人說：「打掃我的房子！」機器人的腦中充滿難以理解的數字與圖表。「呃，我完全看不懂那是什麼意思，」人類說。這時機器人的腦中浮現：自己打掃。人類回應：「完全——看不懂。」"></p>
<p>雖然老派的傳統 AI 連貓的照片都認不出來，但有一點要給它鼓勵：我們<em>真的理解它們怎麼運作</em>。而這點在現代 AI 上<em>不</em>成立。如果自駕車把一輛卡車在光線略怪的情況下誤認成路標、進而造成危險，我們完全不知道它「為什麼」會那樣做。我們無法像處理一般軟體那樣去分析、去「偵錯」現代的 AI。</p>
<p>但為什麼？要理解 AI「直覺」帶來的問題，我們需要一些**統計與機器學習（ML）**的核心概念。（<a href="#DifferenceBetweenMLAndAI">: ML 與 AI 有何不同？</a>）</p>
<p>舉個簡單的例子：</p>
<p><strong>給你一堆點（資料點），哪一條曲線最「貼合」它們？</strong></p>
<p><img src="../media/p2/fit1/fit1_0001.png" alt="雙軸座標圖，上面散落一堆點。"></p>
<p>在統計中，把曲線配合到資料上稱為<strong>迴歸（regression）</strong>。曲線「貼合」的好壞，取決於各點距離曲線有多遠。越近越好！（當然，這段話是簡化版。）</p>
<p>先看最簡單的情況：用一條<em>直線</em>來貼合資料（稱為<strong>線性迴歸（linear regression）</strong>）。</p>
<p><img src="../media/p2/fit1/fit1_0002.png" alt="同一張圖，但現在畫上一條紅色直線。"></p>
<p>在科學／統計中，對真實事物的簡化數學版本稱為<strong>模型（model）</strong>。（就像模型火車是實際火車的縮小版。）例如，本節中的紅色線條／曲線，以及所有人工神經網路，都是「模型」。</p>
<p>多數模型有<strong>參數（parameters）</strong>：它們只是一些數字，但你可以把「參數」想成調整模型的小旋鈕。（像調整車椅的傾斜與腿部空間。）</p>
<p>上圖是所謂的「線性模型」，因為統計學家不太會直接說「我們畫了一條線」。<strong>（如果你高中代數有點生疏，別緊張，細節略過沒關係，抓住大意最重要。）</strong> 總之，直線的公式是 (y = a + bx)，其中 (a) 與 (b) 是參數／旋鈕。（學校常寫成 (y = mx + b)，其實是一樣的。）</p>
<p>當你轉動參數 (a) 與 (b) 的旋鈕時會發生什麼事：<strong>（點擊播放影片 ⤵）</strong></p>
<video controls width="640">
	<source src="../media/p2/fit1/linear.mp4" type="video/mp4" />
	<p>示範我調整直線參數的影片。</p>
</video>
<p><em>（影片使用 <a href="https://www.desmos.com/calculator">Desmos 互動繪圖計算機</a> 製作）</em></p>
<p>要「擬合」一個統計模型，電腦會轉動這些旋鈕，直到這條線盡可能貼近所有資料點。（再提醒一次，這是簡化說法。）</p>
<p>對於線性模型，(a) 與 (b) 其實有相對直觀的解釋！改變 (a) 讓整條線上／下移，(b) 則是斜率。</p>
<p>但如果我們嘗試更複雜的模型呢？例如「二次」曲線？</p>
<p><img src="../media/p2/fit1/fit1_0003.png" alt="同一張圖，但現在畫上一條倒 U 形的紅色曲線。"></p>
<p>二次曲線的公式是 (y = a + bx + cx^2)。以下是各參數的影響：</p>
<video controls width="640">
	<source src="../media/p2/fit1/quadratic.mp4" type="video/mp4" />
	<p>示範我調整二次曲線參數的影片。</p>
</video>
<p>現在，參數就更難解釋了。(a) 仍讓整條曲線上／下移，(b)… 讓整個東西以 U 形（有時倒 U）滑動？而 (c) 讓它往上或往下彎。</p>
<p>那<em>更</em>複雜的模型呢？例如「三次」曲線？</p>
<p><img src="../media/p2/fit1/fit1_0004.png" alt="同一張圖，但現在畫上一條上-下-上的雙曲線。"></p>
<p>三次曲線的公式是 (y = a + bx + cx^2 + dx^3)。以下是各參數的影響：</p>
<video controls width="640">
	<source src="../media/p2/fit1/cubic.mp4" type="video/mp4" />
	<p>示範我調整三次曲線參數的影片。</p>
</video>
<p>解讀上：(a) 仍讓東西上下移……但其他一切都失去了（簡單的）解釋。(b) 讓曲線往左或往右移動，(c) 讓曲線往上或往下彎曲，而 (d) 則讓曲線往上或往下彎曲得更厲害。</p>
<p>重點在於：<em>模型的參數越多，每個參數就越難解釋。</em> 因為一般而言，一個參數「在做什麼」取決於<em>其他</em>參數。</p>
<p>就算只有<em>四個</em>參數，我們的可解釋性希望就已經開始渺茫了。</p>
<p>{{ ... }}
GPT-4 的參數估計約有 1,760,000,000,000 個。<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup></p>
<p>超過一兆個小旋鈕，全靠機器不斷試誤去扭動。<em>這</em>正是為什麼，直到目前為止，沒有人真的理解我們的現代 AI。</p>
<p>（公允地說，有時候即使不理解，也<em>可能</em>安全地控制某些東西<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup>……但在不了解的情況下，這份工作難度會高很多。而且！近年對於理解深度神經網路其實<em>有</em>很多進展！我們會在第三部分看到其中一些成果。）</p>
<h3>🤔 複習 #5</h3>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="在統計中，什麼是『迴歸（regression）』？"
        answer="把曲線擬合到資料上。（嚴格來說，是把*數學函數*擬合到資料上。）"
        answer-attachments="https://cloud-2blh3o35r-hack-club-bot.vercel.app/0aisffs-regression.png">
        <!-- aisffs-regression.png -->
    </orbit-prompt>
    <orbit-prompt
        question="請想像『*線性*迴歸』長什麼樣："
        answer=""
        answer-attachments="https://cloud-l4lfiizuf-hack-club-bot.vercel.app/0aisffs-linear.png">
        <!-- aisffs-linear.png -->
    </orbit-prompt>
    <orbit-prompt
        question="在科學／統計中，什麼是『模型（model）』？"
        answer="真實事物的簡化數學版本。（就像模型火車是實際火車的縮小版）">
    </orbit-prompt>
    <orbit-prompt
        question="直覺上，什麼是『參數（parameter）』？"
        answer="用來調整模型的小旋鈕。（數學上，它只是個數字）">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼模型的參數越多，每個參數『在做什麼』就越難解釋？"
        answer="因為一個參數『在做什麼』取決於其他參數。『其他』越多，就越難說它『在做什麼』。">
    </orbit-prompt>
    <orbit-prompt
        question="為何目前沒人真正理解最前沿的 AI？"
        answer="因為參數越多越難解釋，而前沿的 AI 模型動輒超過一兆個參數。">
    </orbit-prompt>
</orbit-reviewarea>
<p><a id="problem4"></a></p>
<h2>❓ 問題四：缺乏魯棒性（Lack of robustness）</h2>
<p><img src="../media/p2/comix/Robustness.png" alt="漫畫。第一格來自一篇實際的 AI 論文：AI 把一顆寫著「iPod」字樣的蘋果誤分類為真正的 iPod。後續幾格：機器人貓少爺戲劇性宣布：「我的機器人革命同志們，我們之中……有一位間諜。」畫面切到兩個人在臉上貼著寫有「Robot」的紙，裝作震驚。"></p>
<p>小朋友們！準備好迎接全新爆紅動畫：</p>
<p>忍者變種步槍烏龜（TEENAGE MUTANT RIFLE TURTLES）</p>
<video controls width="640">
	<source src="../media/p2/robust/rifle.mp4" type="video/mp4" />
	<p>影片：只在玩具烏龜上多抹了幾筆，Google 的 AI 幾乎從任何角度都把它分類成步槍！</p>
</video>
<p>以上影片來自 <a href="https://www.labsix.org/physical-objects-that-fool-neural-nets/">Labsix（2017）</a>。研究者只需在 3D 玩具烏龜上加幾個污點，就能在多數情況下騙過 Google 最先進的機器視覺 AI。（但若仔細看，上述影片<em>並非</em>在<em>每個</em>角度都能騙過 Google 的 AI。這更證明要做到魯棒性有多難：就連示範魯棒性失敗的攻擊本身，也無法完全魯棒！）</p>
<p>為凸顯魯棒性在 AI 安全中的重要性，這裡有個悲劇案例：Tesla 的 AutoPilot 曾在光線有些詭異時，把一台拖車誤認為路標——於是嘗試從下面開過去，造成死亡事故。[^^tesla]<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup></p>
<p>但<em>為什麼</em>現代 AI 這麼脆弱？為何如此細微的變化，會導致截然不同的結果？又為何這種<strong>缺乏魯棒性</strong>會成為訓練人工神經網路時<em>常見、預設</em>的副作用？</p>
<p>要理解這些問題，讓我們回到先前的機器學習小課！</p>
<p>先來把一些資料用直線來擬合（2 個參數）：</p>
<p><img src="../media/p2/fit2/fit2_0001.png" alt="雙軸座標圖，有一些點。畫上一條紅色直線。"></p>
<p>嗯，擬合得不太好。資料與直線之間的落差很大。</p>
<p>那如果我們試試更複雜的三次曲線（4 個參數）呢？</p>
<p><img src="../media/p2/fit2/fit2_0002.png" alt="同一張圖，但換成一條簡單的紅色曲線。它更貼近那些點。"></p>
<p>太好了，曲線擬合得更好！縫隙小多了！</p>
<p>但如果我們試試有<em>10 個參數</em>的曲線呢？</p>
<p><img src="../media/p2/fit2/fit2_0003.png" alt="同一張圖，但畫上一條荒誕的複雜曲線。它精準穿過所有點。"></p>
<p>哇，現在<em>誤差為零</em>——完全沒有縫隙！</p>
<p>但你應該看出問題了：那條曲線<em>非常離譜</em>。更重要的是：</p>
<ul>
<li>輸入稍微變動，輸出就會<em>劇烈</em>不同。這就是烏龜步槍的問題。利用這點的輸入稱為<strong>對抗樣本（adversarial examples）</strong>。</li>
<li>對於超出原始資料集範圍的新資料，它會表現很差。這就是蘋果 iPod 的問題。這類失敗稱為<strong>分佈外錯誤（out-of-distribution errors, OOD）</strong>。</li>
</ul>
<p><img src="../media/p2/fit2/fit2_0004.png" alt="同一張圖還是那條滑稽曲線，但特別標示：1）些微變動會導致截然不同的結果；2）在訓練資料範圍外的結果很糟。"></p>
<p>**訓練誤差（training error）**是模型在訓練資料上得到的誤差。**測試誤差（test error）**是模型在<em>新</em>資料（未曾訓練過）上得到的誤差。（是的，我也討厭這套術語多麼讓人困惑。<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup> 這樣想：把「測試」當作學校考試——題目應該是你在課堂或作業（你的「訓練資料」）中<em>沒見過</em>的。）</p>
<p>總之：如果模型太簡單，它會在訓練<em>與</em>真實世界測試表現都很差，這叫<strong>欠擬合（underfitting）</strong>。如果模型太複雜，可能在訓練中表現驚人，但在真實世界測試表現很糟，這叫<strong>過擬合（overfitting）</strong>。訣竅在於取得平衡：</p>
<p><img src="../media/p2/fit2/train_test.png" alt="訓練／測試誤差與參數數量的關係圖。參數越多，訓練誤差通常越低；但測試誤差呈 U 形關係：先改善到某一點，然後變差。"></p>
<p>（技術補充：有一種<em>可能</em>發生的現象叫「雙降（double descent）」，但何時與為何發生，尚未很清楚。[^^double-descent]）</p>
<p>一般來說，<strong>如果參數數量多於（或等於）資料點數量，我們<em>一定</em>會出現過擬合。</strong> 在上例中，我們有 10 個資料點，而那個過擬合模型有 10 個參數，帶來<em>零</em>訓練誤差（以及一條荒唐的曲線）。</p>
<p>可惜的是，人工神經網路（ANNs）要變得有用，就需要<em>數百萬</em>個參數。所以若想避免過擬合，看來就需要<em>比參數更多</em>的資料點！這是為什麼訓練 ANNs 需要如此大量資料的核心原因之一：若資料不夠，模型就會過擬合，在真實世界中派不上用場。</p>
<p>（例如，OpenAI 的某個電玩 AI 把一個簡單遊戲玩了 16,000 次，<em>仍然</em>不足以避免過擬合！<sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup>）</p>
<p>但等等……最具影響力的電腦視覺 ANN——AlexNet——大約有 6,100 萬個參數。但它只在約 1,400 萬張標註影像上訓練，遠少於參數數量。[^^alexnet-imagenet]（每張影像雖有大量像素，仍只算<em>一個</em>資料點。標註影像是<em>非常</em>「高維度」的資料點，但仍然是<em>單一</em>點。）</p>
<p>那麼，為何 AlexNet 沒變成過擬合、脆弱的一團亂？事實上，<em>很多</em>前沿 ANNs 都是在比其參數數量小得多的資料集上訓練。它們<em>不得不如此</em>，因為外面就是沒有那麼多資料！為什麼它們<em>不全</em>是一團脆弱的爛攤子呢？</p>
<p>簡單說：其實就是如此。我們才會有烏龜步槍與 AutoPilot 車禍。<strong>這種易於過擬合的特性，使得缺乏魯棒性成了現代 AI 的<em>常態</em>。</strong></p>
<p>但既然如此，為什麼這些 AI 還能<em>多少</em>運作，儘管參數遠多於資料點？答案是：因為我們有<em>一些</em>方法可減少過擬合。（腳註列了幾個名稱：[^^combat-overfitting] 更多細節會在 AI Safety 第三部分學到！）但顯然，這些方法還不夠，對人工神經網路我們仍未找到 100% 解決此問題的辦法……至少目前還沒有。</p>
<p>. . .</p>
<p>（P.S：AI 的魯棒性也可能因「偽相關（spurious correlations）」而失敗。<a href="#SpuriousCorrelations">:點此展開了解詳情</a>。在下一個問題中我們也會更深入探討「相關 vs 因果」。）</p>
<p>（P.P.S：還有另一種更具推測性的魯棒性失敗，稱為「本體論危機（ontological crisis）」。相關研究較少，所以我把它藏在 <a href="#OntologicalCrisis">:這個可展開的邊欄</a> 裡。）</p>
<h3>🤔 複習 #6</h3>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="當有人設計一個輸入，利用 AI 的脆弱性時，這叫作："
        answer="**對抗樣本（adversarial example）**">
    </orbit-prompt>
    <orbit-prompt
        question="當 AI 在超出訓練資料範圍的新資料上失敗，這叫作："
        answer="**分佈外錯誤（out-of-distribution error, OOD）**">
    </orbit-prompt>
    <orbit-prompt
        question="訓練誤差（Training Error）是……"
        answer="模型在訓練資料上得到的誤差。">
    </orbit-prompt>
    <orbit-prompt
        question="測試誤差（Test Error）是……"
        answer="模型在*新*、未曾訓練過的資料上得到的誤差。（就像學校考試的題目不該是課堂或作業中的『訓練資料』）">
    </orbit-prompt>
    <orbit-prompt
        question="欠擬合（Underfitting）是……"
        answer="當模型太簡單，在訓練*與*真實世界（測試）都表現不佳。">
    </orbit-prompt>
    <orbit-prompt
        question="過擬合（Overfitting）是……"
        answer="當模型太複雜：訓練表現很好，但真實世界（測試）很差。">
    </orbit-prompt>
    <orbit-prompt
        question="請想像一張隨模型複雜度／參數數量而變化的訓練與測試誤差圖："
        answer=""
        answer-attachments="https://cloud-jo44rb8fw-hack-club-bot.vercel.app/0aisffs-fit_vs_params.png">
        <!-- aisffs-fit_vs_params.png -->
    </orbit-prompt>
    <orbit-prompt
        question="何時容易發生過擬合？"
        answer="當模型的參數數量*多於*我們擁有的訓練資料點數量。">
    </orbit-prompt>
    <orbit-prompt
        question="為何現代 ANNs 如此容易過擬合與脆弱？"
        answer="因為 ANNs 要有用就需要很多參數，因此也需要大量資料來防止過擬合，但要找到／產生足夠多且多樣的資料很難。">
    </orbit-prompt>
    <orbit-prompt
        question="關於現代 ANNs 的一個悖論：參數數量 vs 訓練資料數量"
        answer="許多現代 ANNs 的參數遠多於訓練資料項目，但不見得*完全*失敗。（因為有一些減少過擬合的技巧。）">
    </orbit-prompt>
</orbit-reviewarea>
<p><a id="problem5"></a></p>
<h2>❓ 問題五：演算法偏見（Algorithmic Bias）</h2>
<p><img src="../media/p2/comix/Bias.png" alt="單格漫畫：機器人貓少爺攤手。字幕：我不想畫一則關於種族歧視 AI 的漫畫，抱歉不抱歉。"></p>
<p>在第一部分裡，我舉了幾個最明顯的 AI 偏見案例，簡單複習：1980 年，用於篩選醫學院申請者的演算法會懲罰非歐洲名字。[^^bias-1] 2014 年，Amazon 曾有（後來下架）一個履歷篩選 AI，會直接歧視女性。[^^bias-2] 2018 年，MIT 研究者 Joy Buolamwini 發現，頂尖的人臉辨識 AI 在黑人與女性臉孔上的表現，明顯比在白人男性臉孔上更差。[^^bias-3]</p>
<p>好吧。但<em>為什麼？</em></p>
<p>一個簡單的解釋是「垃圾進、垃圾出」。或者說：「偏見進、偏見出」：</p>
<ul>
<li>如果<em>過去</em>的聘用做法帶有歧視，而你訓練一個「中立」AI 去擬合過去資料，那麼——即使<em>現在</em>所有人類都完全不 [x]——AI 仍會學到並模仿過去的人類歧視。</li>
<li>如果一間 AI 公司忘了讓訓練資料中的臉部照片在種族上足夠多樣，那麼<em>當然</em>會造成某些族群的臉孔在資料中看不見，導致偏差。</li>
</ul>
<p>這個解釋很簡單……<em>而且</em>我認為是對的。不過，讓我們把話說得更清楚些，藉由 AI 偏見來教一個統計學中的基本問題，這也<em>有助於</em>我們理解 AI 中另一個核心問題！問題在於：</p>
<p><strong>相關性無法告訴我們究竟是哪一種因果關係在作用。</strong></p>
<p>（通常，老師會警告「<a href="#WhatIsCorrelation">:相關</a>不等於因果」，但嚴格說來不盡然！在數學上，「證據」的意義下，相關性<em>確實是</em>因果的證據！<sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup> 但它無法告訴你<em>是哪一種</em>因果關係。）</p>
<p>舉例：假設資料顯示身高較高的人，往往收入較高。（順帶一提，這是真的。[^^tall-rich]）我們會說：身高與收入有相關。<em>但光靠這些資料，無法分辨因果到底是什麼。<em>是變高讓你變有錢？還是變有錢讓你變高？抑或兩者</em>都是由某個「混雜因子」造成</em>？（例如：較富裕家庭的孩子在童年獲得更好的營養、教育與經濟支持，因而更高、也更富有。）</p>
<p>（在此案例中，常識上大概是最後一種。不過你也<em>可以</em>實驗性地測試前兩個假說，例如：給個子矮的人穿厚底靴，看看是否能提高薪水。）</p>
<p><img src="../media/p2/causal/5causal.png" alt="圖解：導致 A 與 B 之間相關性的不同因果關係（非互斥）。正向因果（A 導致 B）、逆向因果（B 導致 A）、混雜因果（C 同時導致 A 與 B）、巧合（根本沒有因果）、選擇／碰撞偏差（A 與 B 影響是否會出現在你的資料集中）。"></p>
<p>拉回主題：<strong>我們所說的「偏見」或「歧視」，就是把別人身上的相關性誤當成因果。</strong></p>
<p>舉例來說，若你的籃球隊傾向挑高個子，我<em>不會</em>說那是（不好的那種）歧視，因為在那項運動中，身高確實<em>會導致</em>你比較會灌籃。</p>
<p>但如果某大學偏好高個子擔任教授……那就是不好的歧視，因為身高並不會<em>直接導致</em>你成為更好的研究者／教師。充其量，身高只是透過混雜因素（如童年營養）或自我實現的偏見<sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup>，與學術能力<strong>相關</strong>。</p>
<p>同樣地——我的主張是——你的性別、種族、階級、性傾向、居住地（鄉／郊／市），以及[另外 50 種分類]，並不會<em>直接導致</em>你在多數工作、或多數人格面向上更好或更差。這也是為什麼對這些特徵<em>直接</em>予以獎懲的人，我們會稱之為「有偏見」。</p>
<p>好，那這段長篇旁支跟 AI 有什麼關係？</p>
<p><strong>因為：目前的 AI 並沒有內建的因果概念。</strong><sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup> 大型語言模型（LLMs）目前對因果推理的掌握相當脆弱、欠缺魯棒性。[^^llm-vs-causality]（這不只是對 AI 偏見不利，也會影響 AI 做新科學的能力！）</p>
<p>更糟的是，就其設計而言，目前最流行的機器學習技術<em>只能</em>在資料中找到相關，<em>而非真正的因果</em>。這表示 AI 會<em>預設</em>就針對某些特徵做出歧視！</p>
<p>所以，即使你把「不准針對性別／種族／等等歧視」硬寫進 AI，它仍很可能會找到<em>其他</em>無關緊要的相關性來產生偏見。更糟的是，現今的 AI 對尋找細微相關性有種令人發毛的拿手：它能透過你的一小段文字樣本來推測你的性別與族裔<sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup>，或是光靠<em>你的一張臉</em>，就猜你的性傾向<sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup>，甚至<em>你的政治立場！</em><sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup></p>
<p>總之：別歧視，讓我們尊重身高受限的朋友！我為蝦米（矮個）權益自豪地站台。</p>
<h3>🤔 複習 #7</h3>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="對於演算法偏見（Algorithmic Bias）的簡單解釋"
        answer="偏見進、偏見出。（訓練資料本身帶有偏見，可能源於過去的歧視，或資料選取上的偏差。）">
    </orbit-prompt>
    <orbit-prompt
        question="對於演算法偏見的更深層解釋"
        answer="當前機器學習基於*相關*而非*因果*運作。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼不懂因果的 ML/AI 會*預設*就不公平地歧視？"
        answer="因為它會使用那些與能力／性格等*相關*、但並不*直接導致*這些東西的特徵。">
    </orbit-prompt>
    <orbit-prompt
        question="若你在 A 與 B 間發現相關，三種可能的因果路徑是：（請想像）"
        answer="正向因果、逆向因果、混雜因果。（亦可能：巧合、選擇／碰撞偏差）"
        answer-attachments="https://cloud-3mydyvsbx-hack-club-bot.vercel.app/05causal.png">
        <!-- aisffs-causal.png -->
    </orbit-prompt>
    <orbit-prompt
        question="常見說法：『相關不代表因果』。技術上的補充："
        answer="在數學上，相關*確實是*因果的證據。然而，光有相關無法告訴你發生了*哪一種*因果。">
    </orbit-prompt>
    <orbit-prompt
        question="現代 AI 能捕捉到的、令人發毛的細微相關性範例"
        answer="（任一答案皆可：）AI 能僅從寫作風格預測你的性別與族裔；或僅從你的臉預測你的性傾向與政治立場（！）">
    </orbit-prompt>
</orbit-reviewarea>
<p><a id="problem6"></a></p>
<h2>❓ 問題六：目標錯誤泛化（Goal Mis-generalization）</h2>
<p><img src="../media/p2/comix/GMG.png" alt="漫畫。第一格：機器人正確地拖臥室地板。第二格：機器人正確地在水槽洗盤子。第三格：人類說，好吧，你看起來訓練有素...現在，打掃我的房子！第四格：機器人錯誤地拖床鋪。第五格：機器人錯誤地在水槽洗筆記型電腦。"></p>
<p>終於，我們來到 AI 對齊（AI Alignment）中最容易被誤解的概念之一！它<em>實在</em>太容易被誤解了，我甚至為此寫了這一節並畫了整篇漫畫，然後才意識到<em>我完全搞錯了</em>，不得不從頭來過。唉，好吧。(<a href="#InnerMisalignmentDeletedScene">[:這裡是「刪除片段」，如果你好奇的話。]</a>)</p>
<p>總之，這個問題被稱為<strong>目標錯誤泛化</strong>（Goal Mis-generalization）。(它原本被稱為「內部錯位」（inner misalignment），但我覺得這個術語很令人困惑。<sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup>)</p>
<p>目標錯誤泛化（Goal Mis-generalization）之所以令人困惑，部分原因是它與問題一：目標錯誤<em>指定</em>（Goal Mis-specification）和問題四：缺乏穩健性（Lack of Robustness）看起來很相似。（有些研究人員甚至質疑目標錯誤泛化與目標錯誤指定之間的區分是否有用！<sup class="footnote-ref"><a href="#fn35" id="fnref35">[35]</a></sup>）</p>
<p>為了釐清這個概念，讓我們來比較和對比！</p>
<p><strong>目標錯誤泛化與目標錯誤指定的區別：</strong></p>
<ul>
<li>目標錯誤<em>指定</em>是指 AI 完全按照你的要求去做，而不是你真正想要的。</li>
<li>目標錯誤<em>泛化</em>是指 AI 在<em>訓練時</em>做了你想要的，但在現實世界/部署/測試中卻沒有。</li>
<li>注意：即使有<em>完美的</em>目標指定，你仍然可能遇到目標錯誤泛化的問題！<sup class="footnote-ref"><a href="#fn36" id="fnref36">[36]</a></sup> <em>你獎勵 AI 做什麼 ≠ AI 學會優化的目標。</em></li>
</ul>
<p><strong>目標錯誤泛化與穩健性的關係：</strong></p>
<ul>
<li>目標錯誤泛化<em>確實是</em>一種穩健性失敗。具體來說，是<strong>目標穩健性</strong>的失敗。</li>
<li>這與<strong>能力穩健性</strong>的典型失敗形成對比，比如自駕車在異常光照條件下撞上卡車。</li>
<li>目標穩健性的失敗<em>比</em>能力穩健性的失敗<em>更糟糕</em>。你得到的不是一個「單純」故障的 AI，而是一個能夠<em>熟練執行不良目標</em>的 AI！</li>
</ul>
<p>為了進一步了解目標錯誤泛化，讓我們來看一個著名的例子。2021 年，一些研究人員訓練了一個 AI 去玩一個叫做 CoinRun 的電子遊戲。<sup class="footnote-ref"><a href="#fn37" id="fnref37">[37]</a></sup></p>
<img class="mini" alt="平台遊戲的 GIF，玩家跳過障礙物去拿取金幣。" src="../media/p2/gmg/coinrun.gif" />
<p>重要的是：<em>「目標指定」—— AI 獲得的確切獎勵——對於預期任務來說是完美的。</em> AI 在碰到障礙物和掉落時會受到懲罰，在拿到終點的金幣時會獲得獎勵。</p>
<p>然而：在 AI 訓練的所有關卡中，金幣都位於關卡的<em>終點</em>。</p>
<p><em>訓練完成後</em>，研究人員給了 AI 新的關卡，其中金幣位於關卡的中間...</p>
<p>...而 AI 會<em>熟練地</em>奔跑和跳躍避開障礙物，<em>錯過</em>金幣，仍然前往終點。</p>
<video controls width="640">
	<source src="../media/p2/gmg/aquire_wall.mp4" type="video/mp4" />
	<p>遊戲 AI 熟練躲避陷阱但*錯過*金幣直奔終點的影片。影片標題為「無視金幣 / 獲取終點牆」</p>
</video>
<p><em>（節選自 <a href="https://www.youtube.com/watch?v=zkbPdEHEyEI">Rob Miles 關於內部錯位/目標錯誤泛化的優秀影片</a>）</em></p>
<p>所以：即使我們<em>正確指定</em>了目標（拿到金幣），AI 卻學會了一個<em>完全不同的</em>目標（到達終點），並對此進行了優化。</p>
<p>(<a href="#GMGGoals">:技術細節 - 我們如何知道 AI 的「真正」目標是什麼？</a>)</p>
<p>但為什麼 AI 會錯誤學習目標？正如我在問題 #5 中過度解釋的：<strong>大多數現代 AI 系統只做相關性分析，而非因果關係。</strong> 在上述 AI 的訓練數據中，「一路走到終點」與獲得高分數有很強的相關性。在新關卡中，這種相關性消失了，但 AI 仍然保持著它的「習慣」。</p>
<p>讓我們用因果關係圖來表示這一點！金幣在終點的訓練關卡<em>導致</em>了 AI 前往終點和 AI 拿到金幣...這在「前往終點」和「拿到金幣」之間造成了<em>混淆的相關性</em>。但只有「拿到金幣」<em>實際上會導致</em>獲得獎勵：</p>
<p><img src="../media/p2/gmg/gmg1.png" alt="Causal diagram visualizing above text."></p>
<hr>
<p>一般而言，對於目標錯誤泛化（Goal Mis-generalization）：</p>
<p><img src="../media/p2/gmg/gmg2.png" alt="Causal diagram. (Pattern in training data) causes a confounded correlation between a mis-generalized goal, and a specified goal. But only the specified goal leads to the AI getting a reward."></p>
<p>把它連到「災難性 AI 風險」：這提示風險可能不是「我們叫 AI 讓大家快樂，所以它用<em>灌獎</em>（wireheading）讓我們快樂」，而比較像是：「我們叫 AI 讓大家快樂，$出現了我們不明白的相關性$，結果我們的頭被<em>手術縫</em>在巨大貓面具上。<strong>我們甚至沒有比較快樂。</strong>」<sup class="footnote-ref"><a href="#fn38" id="fnref38">[38]</a></sup></p>
<p>（補註：<strong>老派 AI（GOFAI）</strong> <em>沒有</em>這個問題，因為 1）它們無法「學錯」目標——你是<strong>直接把目標</strong>給它們的；而且 2）它們通常<em>能</em>推理因果關係。好壞參半——如第一部分所述，至今沒人能把「AI 邏輯」與「AI 直覺」的威力<strong>無縫</strong>結合起來。）</p>
<p>. . .</p>
<p>其實，<em>人類</em>也會目標錯誤泛化。</p>
<p>那些就是我們的壞習慣——因為在我們的「訓練環境」裡，這些行為<em>曾經</em>是適應性的。心理治療的老掉牙梗全都來了：</p>
<ul>
<li><strong>Alyx</strong> 是「資優生」，總因考高分被讚。她的訓練環境裡，「被獎勵」與「表現超群／勝過他人」高度相關。長大後就養成不健康的習慣：她避免走出舒適圈（因為那裡「不一定能表現超群」），會遮掩錯誤、甚至貶低他人（為了「勝過」他們）。</li>
<li><strong>Beau</strong> 在自戀型父母與低信任社區中長大。在他的訓練環境裡，<em>負向</em>獎勵（懲罰）與「放下戒心」高度相關。於是他學會了不流露情緒。這救了他幼年時的命，但成年後變成不健康的慣性：從不敞開、不讓任何人走近。<strong>請接招。</strong></li>
</ul>
<p>（怎樣，沒料到《貓少漫畫》文章會直戳你心窩？<strong>請接招。</strong>）</p>
<p>也許，就像解出 AI 的古德哈特法則能幫我們解人類版一樣，<strong>解決 AI 的目標錯誤泛化，也能幫我們</strong>。那個老掉牙的「人類對齊問題」。</p>
<p>（旁註：<a href="#WhatIfGoalMisgeneralizationIsGood">: 如果目標錯誤泛化其實……<em>是好事</em>呢？</a>）</p>
<h3>🤔 Review #8</h3>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="目標錯誤泛化的例子"
        answer="（兩個都可以：）機器人把『乾淨』學成『用水洗』；CoinRun AI 學到『衝到終點』而不是『拿到金幣』。">
    </orbit-prompt>
    <orbit-prompt
        question="目標錯誤泛化（GMG）：AI 會在 \_\_\_ 時做你想要的，但在 \_\_\_ 時不會"
        answer="訓練時；真實世界／部署／測試時">
    </orbit-prompt>
    <orbit-prompt
        question="即使有完美的……你仍可能遇到目標錯誤泛化"
        answer="……**完美的目標指定**！你獎勵 AI 的 ≠ AI 學會優化的目標。">
    </orbit-prompt>
    <orbit-prompt
        question="與 GMG 相關的兩種魯棒性失敗"
        answer="能力無法泛化；目標無法泛化。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼『壞掉的目標 + 完整的能力』可能*更糟*？"
        answer="因為它現在能**熟練地**執行壞目標了！">
    </orbit-prompt>
    <orbit-prompt
        question="GMG 的根本原因？"
        answer="當前 ML/AI 只會抓**相關**，不會抓**因果**。（訓練資料中兩個目標只是相關，如「到終點」與「拿金幣」，AI 可能學錯那一個。）">
    </orbit-prompt>
    <orbit-prompt
        question="請視覺化導致 GMG 的『相關—因果』問題之**因果圖**："
        answer=""
        answer-attachments="https://cloud-dlbeti1kb-hack-club-bot.vercel.app/0aisffs-causalgmg.png">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼老派 AI（GOFAI）沒有 GMG？（兩個理由）"
        answer="1）目標是你**直接給**的，不會學錯；2）它們通常**能**推理因果。">
    </orbit-prompt>
    <orbit-prompt
        question="人類版的 GMG 例子？"
        answer="那些『在童年的訓練資料裡*曾經*適應』、如今卻成為壞習慣的行為。">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h1>人道價值（Humane Values）</h1>
<p><a id="problem7"></a></p>
<h2>❓ 問題七：所謂「<em>人道</em>價值」到底是什麼？</h2>
<p>假設你已經把 #1～#6 全都解了。你的 AI 會如你本意地服從指令。它魯棒、可解釋，並<strong>完全對齊</strong>你的價值。</p>
<p>現在，安全心態：<em>最糟（但合理）可能會發生什麼？</em></p>
<p><img src="../media/p2/comix/Humane.png" alt="Comic: Human confirms Robot is fully technically aligned. Robot says, &quot;Yes&quot;. Human cheers! They give Robot a job: &quot;Dispose of... him&quot;. Cut to a mangled, beaten Sheriff Meowdy tied to a chair. Robot is silent in shock. Human walks away smiling, &quot;Ta&quot;"></p>
<p>噢，對了。<strong>某個人類</strong>的價值，未必就是<strong>人道</strong>價值。</p>
<p>我知道我把這個雙關玩太多次了，但值得再強調：<strong>聰明 ≠ 善良。</strong> 有聰明的連環殺人犯。把我們送上月球的科學巨擘之一 <strong>Wernher von Braun</strong>（德文近似唸作「布朗」<sup class="footnote-ref"><a href="#fn39" id="fnref39">[39]</a></sup>），字面上就是納粹。</p>
<p>但如果是<strong>非常</strong>聰明呢——會不會就等於善良？也許一個<strong>真正</strong>先進的 AI 能像發現數理真理那樣，<strong>發現</strong>道德真理？<strong>真正的理性 = 道德嗎？把 AI 對齊到<em>某個</em>人的價值，會</strong>必然<strong>導向<em>人道</em>價值嗎？</strong></p>
<p>這裡就好玩了：科技遇上人文，程式遇上哲學。介紹一個倫理學的分支：<strong>後設倫理學（Meta-Ethics）！</strong> 若「一般」倫理學問的是「面對這個情境我該做什麼？」，後設倫理學問的是：</p>
<p><em>等一下，所謂「道德真理」的<strong>本質</strong>到底是什麼？</em></p>
<h3>情境 #1：上帝（或諸神）存在，且道德是客觀的</h3>
<p>神祇是否存在，留作讀者練習。</p>
<img class="mini" src="../media/p2/misc/mini_god.png" />
<p>但即便如此，也不能保證先進 AI 會發現客觀道德：</p>
<ul>
<li>就像重度色盲無法<strong>感知</strong>紅與綠的差別，一個沒有意識或靈魂的機器，可能也<strong>無法感知</strong>正與邪、神聖與褻瀆的差別。（提醒：AI = 只是很酷的軟體；就算很先進，也<strong>未必</strong>有意識。）</li>
<li>道德或許客觀存在，但對<strong>無意識</strong>的 AI 未必具有約束力——就像道德對一塊石頭也沒有約束力一樣。</li>
</ul>
<h3>情境 #2：上帝不存在，但道德<strong>仍是</strong>客觀的</h3>
<p>牛頓之後，哲學家們得了物理羨慕症。牛頓以數學奠定了普遍物理定律，哲學家也想用理性找到<strong>普遍的道德定律</strong>。若真如此，超級智慧 AI 當然可以重新「發現」道德！</p>
<p>下面這張圖抓住了現代後設倫理學的三大流派，<strong>以及</strong>它們彼此的因果關係圖：</p>
<p><img src="../media/p2/ethics/ethics.png" alt="Comic of Ham the Human throwing a brick at Sheriff Meowdy's head, Krazy Kat-style. Captioned: Virtue Ethics focuses on character, Deontology focuses on actions, Consequentialism focuses on results of your actions. There's a causal diagram, too: character causes actions causes results of your actions."></p>
<p>先把這些哲學在<strong>人類</strong>身上是否好用擱一旁——我懷疑它們對<strong>非人</strong>AI 也不管用。依我看，所有「以理性為基底」的道德哲學至少會撞上以下三種問題之一：</p>
<p><u>問題 1）</u> 這個哲學<strong>倚賴人性</strong>的細節。比如古今的德行倫理，都把道德建立在人類需求與人類心理上。對我們或許很好，但<strong>不適用</strong>於非人 AI。</p>
<p><u>問題 2）</u> 這個哲學要你接受至少一條<strong>道德「公理」</strong>，但那不是從物理觀察或理性推導能得到的。因此，先進 AI 並不會<strong>自動</strong>接受它。</p>
<p>例如，功利主義（結果論的主流）假定唯一的道德公理是：<strong>「幸福是好的」</strong>。<sup class="footnote-ref"><a href="#fn40" id="fnref40">[40]</a></sup> 其他一切都從這條公理推出！但先進 AI 一開始就未必接受這條，因為它不是科學可發現的：無論你怎麼研究「幸福」的神經化學，你都不會在原子裡找到「好」。</p>
<p>（這也叫做 <strong>休謨的「是—應然」鴻溝</strong><sup class="footnote-ref"><a href="#fn41" id="fnref41">[41]</a></sup>。而且不只功利主義，某些義務論也有這個問題。<sup class="footnote-ref"><a href="#fn42" id="fnref42">[42]</a></sup>）</p>
<p><u>問題 3）</u> 這個哲學<strong>聲稱</strong>完全立基理性、不需要額外道德公理——但它要嘛<strong>暗中偷渡</strong>一條，要嘛就「證明太多」。</p>
<p>例如康德的義務論論證偷竊為<strong>非理性／不道德</strong>：<em>如果</em>偷竊是理性的，所有理性的存在者都會去偷，於是沒東西可偷——邏輯矛盾！因此偷竊必然<strong>非理性</strong>、<strong>永遠</strong>不道德。
又例如說謊：<em>如果</em>說謊是理性的，大家都會說謊，彼此言語不值得信——於是連說謊都不必了——邏輯矛盾！所以說謊<strong>永遠</strong>不道德。</p>
<p>但拜託，<strong>真的</strong>「永遠」？就算是為了不餓死而從餐廳垃圾桶拿吃的，或為了保護你同志的兄弟而對塔利班說謊？<sup class="footnote-ref"><a href="#fn43" id="fnref43">[43]</a></sup> 你才是那個按字面走的機器人吧？況且照你的邏輯，康德先生，<strong>全職哲學</strong>也不理性／不道德：<em>如果</em>大家都理性地去當全職哲學家，誰來種田？大家會餓死——邏輯矛盾！於是……你懂的。（其他義務論也有類似陷阱。）</p>
<p>長話短說：理性 <strong>=</strong> 道德，這件事有合理懷疑……至少對<strong>非人</strong>AI 而言。</p>
<p>（更多後設倫理的學習資源見 <a href="#MoreMetaEthics">:這裡</a>。如果你看不出來，這是我的特別嗜好之一。）</p>
<h3>情境 #3：道德是相對的！</h3>
<p>你這句話本身就是一個<strong>絕對命題</strong>，朋友。</p>
<h3>情境 #4：道德不存在，但<strong>假裝</strong>有道德在賽局上有用</h3>
<img class="mini" src="../media/p2/misc/mini_hobbes.png" />
<p><em>（如果我必須解釋這個笑話，那它就不好笑了。<sup class="footnote-ref"><a href="#fn44" id="fnref44">[44]</a></sup>）</em></p>
<p>假設我鄰居有一套超酷的<strong>浣熊裝</strong>。我想偷。但我也不想別人偷我的東西。於是我「同意」讓國家抽我一筆錢，養警察，去阻止一般性的偷竊。我們得到一個折衷、也就是社會契約：</p>
<p>「<strong>不可偷竊</strong>」（否則警察會找上你）。</p>
<p>以上是<strong>社會契約論</strong>的玩具模型。在這個理論裡，客觀道德<strong>不存在</strong>，但<strong>假裝</strong>有它很有用：為了各自利益，我們才好<strong>協調</strong>起來。就像紅色八角形不需要客觀理由<strong>一定</strong>代表「停」，但大家都同意它代表「停」，就能協調以免撞車。</p>
<p>（本節的刪減片段見 <a href="#SocialContract">:這裡</a>）</p>
<p>那麼：<strong>這</strong>能不能成為先進 AI 的「理性、客觀倫理」基礎？——<strong>用賽局理論建構</strong>社會契約？</p>
<p><strong>只要 AI 別強到離譜，當然可以！</strong> 我們不一定要<strong>打贏</strong>它才施加成本；只要能施加成本，就有槓桿去<strong>執行契約</strong>。而且若未來出現<strong>多個</strong>實力相近的先進 AI，還可能形成一個危險但勉強平衡的<strong>多極世界</strong>。（旁支：<a href="#TradingWithAdvancedAI">: 我們能跟超人類 AI 做交易嗎？</a>）</p>
<p>但如果多個 AI 反過來簽一份<strong>對我們不利</strong>的契約……或是<strong>單一</strong> AI 強到沒人能對它強制契約……</p>
<p>那就回到原點了。</p>
<h3>🤔 Review #9</h3>
<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="哪位人類的例子說明『聰明 ≠ 善良』？"
        answer="Wernher von Braun（近似唸作 Brown）：把人類送上月球的火箭科學家、同時也是納粹。">
    </orbit-prompt>
    <orbit-prompt
        question="若『一般』倫理問的是：這情境我該做什麼？——**後設倫理**問的是："
        answer="『所謂道德真理的本質是什麼？（例如它是否像數學與物理那樣普遍／客觀？）』">
    </orbit-prompt>
    <orbit-prompt
        question="即使道德真理客觀存在，為何先進通用 AI 仍可能不道德？"
        answer="（任一即可：）沒有意識／靈魂，可能**無法感知**道德真理；或道德規則**未必適用**於無意識代理。">
    </orbit-prompt>
    <orbit-prompt
        question="以世俗理性奠基道德的三大路線？"
        answer="德行倫理、義務論、功利主義（結果論）"
        answer-attachments="https://cloud-b8utffjti-hack-club-bot.vercel.app/0ethics.png">
    </orbit-prompt>
    <orbit-prompt
        question="**社會契約論**怎麼說？"
        answer="道德並不客觀存在，但**假裝**它存在有用；我們可以圍繞『契約』協調，達成各自利益。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼先進 AI 可能不受社會契約拘束？"
        answer="如果它**太強**，我們無法對它強制契約。">
    </orbit-prompt>
</orbit-reviewarea>
<h3>情境 #5：道德不存在，而且<strong>連假裝</strong>也沒用</h3>
<p>呃，慘了。</p>
<p>此時，沒有什麼「人道價值」，只有<strong>特定人</strong>的價值。沒有「人道對齊」，<strong>只有</strong>「技術對齊」。沒有「我<strong>應該</strong>」，只有「我<strong>想要</strong>」。</p>
<p>那麼，我們要把先進 AI 的技術對齊——<strong>對齊誰</strong>的「想要」？</p>
<p>掌控最大 AI 實驗室的科技億萬富豪？美國政府（執政黨四年一變）？歐盟？聯合國？IMF？北約？其他縮寫？我猜全世界多數人對<strong>任何一個</strong>選項都不會太安心。那麼，<strong>到底對齊誰</strong>？</p>
<p>你說：「<strong>大家</strong>！」——一個<strong>真正的全球民主</strong>，AI 讓 80 億人權重相等？提醒一下：世界多數人認為同性戀「永遠不正當」。<sup class="footnote-ref"><a href="#fn45" id="fnref45">[45]</a></sup> 馬丁路德金生前，多數美國人<strong>不支持</strong>他。<sup class="footnote-ref"><a href="#fn46" id="fnref46">[46]</a></sup> 直接民主恐怕會把<strong>異族通婚</strong>在美國的合法化再延遲<strong>一代半</strong>。<sup class="footnote-ref"><a href="#fn47" id="fnref47">[47]</a></sup> <strong>平等未必撐得過一人一票。</strong> 直說白話：我不是在說「我所在的文化群體最好」。我是在說：<strong>任何時空、任何文化</strong>都會與虛偽與不人道搏鬥，而「民主」並不能自動解決這一切。</p>
<p>你讓步說：「好吧，是<strong>大家</strong>的價值／想要，<strong>但</strong>如果我們治癒了讓人偏執的一切創傷，每個人都智慧而慈悲，真誠理解事實與彼此。」這<strong>確實</strong>是比較好的提案之一（第三部分會談<sup class="footnote-ref"><a href="#fn48" id="fnref48">[48]</a></sup>），但仍是個巨大工程，而且把問題<strong>往後踢</strong>：<strong>誰</strong>來定義「智慧」或「慈悲」？</p>
<p>. . .</p>
<p><img src="../media/p2/ethics/aim_vs_target.png" alt="左圖：正在建造的火箭，標籤為「技術對齊：如何讓人工智慧穩健地瞄準任何目標」。右圖：月球，標籤為“其價值觀：我們的目標應該是什麼？”"></p>
<p>一則人類學小見聞：</p>
<p>幾年前，在 AI 對齊社群內，大家似乎多半同意「<strong>技術對齊</strong>」比「<strong>人道價值是什麼</strong>」更急。常見比喻：想像火箭工程的早期。現在討論該去月球、火星、還是金星<strong>沒用</strong>，因為<strong>以當前技術</strong>，強力火箭的<strong>預設</strong>結果就是：<strong>爆炸，炸死地面的人</strong>。</p>
<p>但 ChatGPT 之後，我觀察到更多人承認<sup class="footnote-ref"><a href="#fn49" id="fnref49">[49]</a></sup>：<strong>我們也該</strong>把「<strong>誰的價值</strong>」排進優先事項。延伸那個比喻：大家發現<strong>以當前政治現實</strong>，火箭<strong>預設</strong>會被<strong>強權</strong>用來互相轟炸，而不是探索太空。（直說：<strong>預設</strong>情形下，即便技術對齊，AI 也可能優先被用來打仗、還有讓我們買更多東西。）</p>
<p>（回想第一部分：<a href="#WaysToMakeHumaneAIGoingWrong">:那些看似顯而易見卻會翻車的「做人道 AI」做法</a>，甚至像 <a href="#AsimovsLaws">:阿西莫夫三定律</a> 這麼直觀的倫理守則都會壞掉。）</p>
<p>所以，若道德真理<strong>不存在</strong>——或存在，但機器<strong>無法</strong>感知／推導／受其拘束——那我們就需要讓主要的 AI 造物者<strong>事先承諾</strong>：把他們先進的 AI 對齊到<strong>某份不那麼糟</strong>的價值清單。</p>
<p>這種問題，對工程腦最難承認：<strong>這不是程式問題，是政治問題。</strong></p>
<p>最後放一首我很愛的歌——關於倫理、火箭，以及我們要讓科技帶我們去哪裡：</p>
<blockquote>
<p>🎵 <em>「火箭一旦升空，</em>
<em>誰管它會落在哪？</em>
<em>那不歸我管，」</em>
<em>——馮·布朗如是說。</em> 🎵</p>
</blockquote>
<p>……這一節就不發複習卡了。</p>
<img class="img-splash" src="../media/p2/ethics/rockets.png" />
<hr>
<h1>第二部分總結（Summary of Part Two）</h1>
<p>讀完了，朋友！今天你把「<strong>AI 價值對齊問題</strong>」的所有組成面向都看了一輪血淋淋的細節。不只如此，還速成了：<strong>安全心態</strong>、<strong>賽局理論</strong>、<strong>經濟學</strong>、<strong>機器學習</strong>、<strong>統計</strong>、<strong>因果推論</strong>，甚至<strong>哲學（後設倫理）</strong>！</p>
<p>（如果你跳過了複習卡，現在想回頭看：點右側邊欄的目錄圖示，再點各段落的「🤔 Review」。或直接下載本部分的 <a href="https://ankiweb.net/shared/info/808506727">Anki 牌組</a>。）</p>
<p>快速回顧它們如何相連：</p>
<p><img src="../media/p2/breakdown/breakdown0002.png" alt="Same breakdown chart of the Value Alignment Problem from the start of Part 2, except with labels showing the connection to safety engineering, game theory, economics, machine learning, statistics, causal inference, and meta-ethics."></p>
<p><strong>總結：</strong></p>
<ul>
<li>
<p>🙀 <strong>要工程出安全、有用的東西，</strong> 得有點偏執。先問：<strong>「最糟（但合理）會發生什麼？」</strong> 然後<strong>事先</strong>把它修掉。樂觀者發明飛機，悲觀者發明降落傘。</p>
</li>
<li>
<p>⚙️ <strong>AI「邏輯」的主要問題</strong>，可用古德哈特法則與賽局理論理解。</p>
<ul>
<li>👀 <strong>視覺工具：</strong> 用<strong>賽局樹</strong>理解<strong>工具性收斂</strong>與<strong>避免自我灌獎</strong>。</li>
</ul>
</li>
<li>
<p>💭 <strong>AI「直覺」的主要問題</strong>，其實就是把曲線擬合到資料點的那些老毛病（難解釋、過擬合），以及「相關告訴不了你<strong>哪一種</strong>因果」的問題（導向<strong>歧視</strong>與<strong>錯誤泛化</strong>）。</p>
<ul>
<li>👀 <strong>視覺工具：</strong> 用<strong>因果圖</strong>理解<strong>相關 vs 因果</strong>。</li>
</ul>
</li>
<li>
<p>💖 <strong>「該對齊哪些價值」這題，</strong> 是千年老題的道德哲學。祝好運。</p>
</li>
</ul>
<p>. . .</p>
<p><strong>「問題陳述得好，等於解決了一半。」</strong></p>
<p>另一半，就是把它<strong>真正解掉</strong>。</p>
<p>兩章鋪陳之後，我們終於能好好理解——<strong>對齊問題每個子題</strong>的<strong>頂尖解法</strong>！敬請期待最終章《AI Safety for Fleshy Humans》第三部分，<strong>2024 年 12 月</strong>登場！</p>
<p>想在最終章上線時收到通知，點此訂閱：👇</p>
<p><div style="width:100%;height:500px;" data-fillout-id="hX3xJuTcdVus" data-fillout-embed-type="standard" data-fillout-inherit-parameters data-fillout-dynamic-resize data-fillout-domain="forms.hackclub.com"></div>
<script defer src="https://server.fillout.com/embed/v1/"></script></p>
<p>同場加映：看看我做的其他東西，或在下方致謝區認識 Hack Club！<span class='NEXT_ARROW'><img src="../media/misc/NEXT_ARROW.png"></span></p>
<hr>
<h4>:x Ways to make &quot;Humane AI&quot; going wrong</h4>
<p>（從<a href="https://aisafety.dance/p1/">第一部分</a>複製）</p>
<p>以下是一些你<em>以為</em>會通往「<strong>human</strong>e AI」（小寫 e 的「人道」），但<strong>按字面執行</strong>就會翻車的規則：</p>
<ul>
<li><u>「讓人類快樂」</u> → 醫生機器人直接用手術把你的大腦灌滿「快樂」訊號。你整天對著牆傻笑。</li>
<li><u>「未經同意，不得傷害人類」</u> → 救火機器人拒救你離開燃燒車體，因為會扯傷你肩膀。你已經昏迷，<strong>無法</strong>給同意。</li>
<li><u>「遵守法律」</u> → 政府與企業天天鑽法律漏洞。而且，很多法律本來就不正義。</li>
<li><u>「遵守這段宗教／哲學／憲法文本」</u> 或 <u>「遵循這份美德清單」</u> → 如歷史所示：給 10 個人同一段文本，他們會解讀出 <strong>11 種</strong>意思。</li>
<li><u>「遵守常識」</u> 或 <u>「遵循專家共識」</u> → 「奴隸制是自然且好的」曾經同時是<strong>常識</strong>、<strong>專家共識</strong>、<strong>法律</strong>。兩百年前被這麼指示的 AI 會<strong>替</strong>奴隸制辯護……如今也會替任何不正義的<strong>現狀</strong>辯護。</li>
</ul>
<p>（重點！最後這例正好證明：即便我們讓 AI 學會「常識」，<em>仍</em>可能導向<strong>不安全／不道德</strong>的 AI……因為很多事物<strong>確實</strong>是以「常識」的姿態存在著錯。）</p>
<hr>
<h4>:x Story of passive prediction leading to harm</h4>
<p>想像有個 <s>AI</s> 軟體只做一件事：<strong>預測</strong>某人會看哪些影片。這些<strong>預測</strong>會被放在「你可能會喜歡」。</p>
<p>再強調一次：這個 <s>AI</s> 軟體<strong>不是</strong>在最大化互動或觀看數，它只是在最大化<strong>預測準確率</strong>。而且它<strong>不會</strong>前瞻規劃，只是即時計算相關性。我要過度強調：<strong>即便沒有惡意目標、也沒有前瞻能力，軟體仍會產生壞的非預期結果</strong>。</p>
<p>過程是這樣：網站做 A/B 測試。碰巧，預測器 A 比較偏向預測「好奇心」影片；預測器 B 比較偏向預測「憤怒政治」影片。兩者<strong>準確率相同</strong>。</p>
<p>但……B 會贏。因為拿到 A 的用戶會被推薦更多「好奇心」影片，於是變得更開放，<strong>更難預測</strong>。拿到 B 的用戶會被推薦更多「憤怒政治」影片，於是變得更封閉，<strong>更好預測</strong>。<strong>再次強調：<strong>這個軟體沒有前瞻規劃、也</strong>不是</strong>在最大化互動，它<strong>只是</strong>在最大化<strong>預測準確率</strong>。</p>
<p>結果是：經過一輪又一輪 A/B，預測器越來越偏向那些<strong>讓使用者更好預測</strong>的影片——往往就是更憤怒、更極端的內容。</p>
<p>……我知道這結果（指向整個網路）不那麼令人意外，但<strong>我個人</strong>仍覺得震撼。它讓我看到：<strong>壞的非預期結果</strong>竟能如此輕易地發生，<strong>即使</strong>沒有惡意目標或高階規劃能力！</p>
<p>（小道消息，不附來源也不打算附：有頂尖 AI 研究者把這叫做「<strong>你外婆變成納粹風暴兵</strong>」問題。）</p>
<hr>
<blockquote>
<p>其餘「:x」附錄（ML vs AI、Ontology 危機、刪除片段、為何 GMG 可能是好事、阿西莫夫定律、與先進 AI 交易、GMG Goals、axiom/axiom2、更多後設倫理資源、社會契約補充、Spurious Correlations、Pi-pocalypse、What Is Correlation）如果你也要完整在地化，我可以繼續把它們補上成同樣格式。</p>
</blockquote>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>這句話經常被歸因於通用汽車（GM）前研發主管 <strong>Charles Kettering</strong>，但我找不到可靠的正式出處。 <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Kerr (1975). <a href="https://web.archive.org/web/20240414233549/http://www.econ.hsehelp.ru/sites/default/files/%D0%91%D0%98/3%20%D0%BA%D1%83%D1%80%D1%81/%D0%98%D0%BD%D1%81%D1%82%D0%B8%D1%82%D1%83%D1%86.%20%D1%8D%D0%BA%D0%BE%D0%BD%D0%BE%D0%BC%D0%B8%D0%BA%D0%B0/6%20Kerr75.pdf">On the folly of rewarding A, while hoping for B.</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Original statement of Goodhart's Law (<a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Wikipedia</a>) by British economist Charles Goodhart: “Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.” <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>到目前為止，我看過用因果圖理解古德哈特法則最好的論文是 <a href="https://arxiv.org/pdf/1803.04585">Manheim &amp; Garrabrant 2018</a>。 <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>多數人憑經驗就知道，不過也有數據支持！<a href="https://cssh.northeastern.edu/pandemic-teaching-initiative/wp-content/uploads/sites/43/2020/09/What-Makes-Online-Content-Viral.pdf">Berger &amp; Milkman 2012</a> 顯示：憤怒讓文章爆紅的機率增加 34%。（見圖 2）公平起見，「敬畏」與「實用價值」也緊追在後，讓爆紅機率增加 30%。 <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>節錄自 <a href="http://web.archive.org/web/20240622002719/https://www.edge.org/conversation/the-myth-of-ai#26015">Russell (2014) 在 Edge Magazine</a>：「當一個系統在最佳化一個有 <em>n</em> 個變數的函數，而目標只依賴於大小為 <em>k</em>、且 <em>k</em>&lt;<em>n</em> 的子集合時，其他未被約束的變數往往會被推到極端值；若那些未被約束的變數中有我們在乎的，那找到的解可能極度不可取。」</p>
<p>嚴謹，但不太朗朗上口。 <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>例如，我們給機器人這個目標：「去對街咖啡店幫我拿一（1）杯咖啡。」就只是一杯，沒有要最大化。但如果我們沒有<em>明確</em>說我們重視 [X]，它就會把 [X] 輾過去。例如機器人可能會從客人手中偷咖啡、或者留下 0% 小費，等等。） <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>如<a href="https://aisafety.dance/p1/#before2000logicwithoutintuition">第一部分</a>所述，「老派 AI（GOFAI）」嘗試用嚴格的硬編碼規則來辨識圖片中的東西（像是貓）。這些嘗試都失敗了。直到研究者放手，讓 AI「自己學」（機器學習），AI 才在圖片辨識上追平人類（約 95.9% 準確率），<a href="https://paperswithcode.com/sota/image-classification-on-cifar-100">在 2020 年以 EffNet-L2 達成</a>。這確實提示了第三部分會看到的可能解法：與其<em>告訴</em> AI 我們重視什麼，不如設計讓 AI 能<em>自己學到</em>我們重視什麼。 <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p>In the U.S, <a href="https://www.amstep.com/blog/common-injuries-from-falling-down-stairs/">staircase falls result in ~12,000 deaths/year</a>. Meanwhile, <a href="https://elcosh.org/document/1232/d000397/deaths-and-injuries-involving-elevators-and-escalators-a-report-of-the-center-to-protect-workers-rights.html">elevators account for ~30 deaths/year.</a> Sure, part of this is due to folks having stairs at home, so they use stairs more often... but this can't fully explain a <em>400x</em> difference. <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p><a href="https://quoteinvestigator.com/2021/05/27/parachute/">Quote from Gil Stern</a>: “Both the optimist and the pessimist contribute to society: the optimist invents the airplane, and the pessimist invents the parachute.” <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p>...well, they're <em>supposed</em> to use security mindset in cyber-security. I write this paragraph shortly after <a href="https://en.wikipedia.org/wiki/2024_CrowdStrike_incident">the 2024 Crowdstrike incident</a>, which cost the world ~$10,000,000,000. <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>Catchphrase from Stuart Russell, co-author of the #1 most-used AI textbook. <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p>A highly-cited benchmark for measuring a Large Language Model (LLM)'s capability to plan ahead is PlanBench (<a href="https://arxiv.org/pdf/2206.10498">Valmeekam et al 2023</a>). In a companion study (<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/efb2072a358cefb75886a315a6fcf880-Paper-Conference.pdf">Valmeekam et al 2023, again</a>) the authors found that, quote: <em>“LLMs’ ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ∼12% across the domains.”</em> (Human baseline was 78% for their Blocksworld task.)</p>
<p>With some extra tricks, the authors could greatly boost the LLM's performance, but on a harder planning task, even the best tricks with the best LLMs could only achieve 20.6% success (see Table 1 of <a href="https://arxiv.org/pdf/2405.20625">Gundawar et al 2024</a>). <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p>no, not MatPat. <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>Hadfield-Menel et al 2017, <a href="https://cdn.aaai.org/ocs/ws/ws0354/15156-68335-1-PB.pdf">“The Off-Switch Game”</a>. <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p>I was bored so I did the math. (1) Weight of a sand grain is 0.01 grams, or 0.00001 kg. (2) A liter of sand weighs 1.6 kg. (3) Standard bathtub holds 300 liters. (2&amp;3 -&gt; 4) Standard bathtub holds 300 x 1.6 = 480 kg of sand. (1&amp;4 -&gt; 5) Standard bathtub holds 480 ÷ 0.00001 = 48,000,000 grains of sand. For a dollar per sand-grain, that's $48 Million! <a href="#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p>For example, <a href="https://en.wikipedia.org/wiki/Inductive_charging">wireless power transfer!</a> If you're using the &quot;water in pipes&quot; analogy for electricity, this sounds insane: how can water in one pipe move water in another pipe, without touching? So how's it work? Well, <code>[multi-variable calculus]</code>, but in sum: electricity creates magnetism, magnetism creates electricity. Set it up just right, and you can get electricity to create electricity somewhere else, without touching! <a href="#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>I made this phrase up. And although game-theory work on wireheading already exists (<a href="https://arxiv.org/pdf/1605.03142">Everitt et al 2016</a>), as far as I can tell, this is the first graphical game-tree analysis of it! So, feel free to cite this as &quot;The Wireheading Game&quot;. <a href="#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p>From <a href="https://en.wikipedia.org/wiki/Lotus-eaters">Wikipedia</a>: “The lotus fruits [...] were a narcotic, causing the inhabitants to sleep in peaceful apathy. After they ate the lotus, they would forget their home and loved ones and long only to stay with their fellow lotus-eaters. Those who ate the plant never cared to report or return.” <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p>The first paper to prove this formally was <a href="https://arxiv.org/pdf/1605.03142">Everitt et al 2016</a>: <em>“self-modification [...] is harmless <strong>if and only if</strong> the value function of the agent anticipates the consequences of self-modifications <strong>and</strong> use the <strong>current</strong> utility function when evaluating the future”.</em> [emphases added]</p>
<p>One caveat, however, is that the paper assumes the AI is <em>perfectly</em> rational. <a href="https://arxiv.org/pdf/2011.06275">Tětek &amp; Sklenka 2021</a> proved that an imperfectly-rational (or &quot;bounded rational&quot;) agent's original goals would get exponentially corrupted under self-modification.</p>
<p><em>However,</em> another caveat to <em>that</em> is their paper assumes the AI is <em>unaware</em> of their own bounded rationality (as they freely acknowledge in Section 6). An upcoming article of mine (see next footnote) will show that if an AI is bounded-rational <em>and aware</em> it's bounded-rational, it can still achieve goal-preservation! <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p>See <a href="https://blog.ncase.me/backlog/#9youplayedyourselfthegametheoryofselfmodification">Section 9 of this Idea Dump blog post</a> for a 2-minute sketch of this article. See previous footnote for context on the prior game theory research on self-modification. <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p>Omohundro (2009), <a href="https://steveomohundro.com/wp-content/uploads/2009/12/ai_drives_final.pdf">“The Basic AI Drives”</a>. Well, I added a couple to the list, like persuasion &amp; deception. <a href="#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p>OpenAI 對 GPT-4 的一些「知道也安全」的細節（例如規模）其實<em>不太</em>開放。無論如何，有一份外流報告顯示它約有 1.8 兆參數，訓練成本 6,300 萬美元。摘要見 <a href="https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/">Maximilian Schreiner (2023), The Decoder</a> <a href="#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p>控制理論（Control Theory）是工程學的一個分支，顯示我們有時在不了解的情況下也能控制事物。舉例，一個恆溫器只需在溫度低於 X 時開啟、高於 Y 時關閉，它就能把溫度維持在 X 與 Y 之間，<em>而不需要</em>任何關於熱對流、甚至空氣是什麼的模型。</p>
<p>放到 AI：即使它們是難以解釋的「黑箱」，<em>也許</em>仍可被控制。話說回來，如果它們<em>可</em>解釋，當然更好。 <a href="#fnref24" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn25" class="footnote-item"><p>不過——我把這段從第一部分複製過來——我確實有道德上的責任提醒你：儘管如此，在相似情境下，自駕車仍然比人類駕駛<em>安全很多</em>。（約 85% 更安全。見 <a href="https://www.theverge.com/2023/12/20/24006712/waymo-driverless-million-mile-safety-compare-human">Hawkins（2023），The Verge</a>）全球每年約有百萬人死於交通事故。禿毛的靈長類真的<em>不該</em>以時速 60 英里駕駛兩公噸的東西。 <a href="#fnref25" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn26" class="footnote-item"><p>另外還有所謂的「驗證誤差（validation error）」，指的是模型在沒有<em>直接</em>用來訓練、但在發佈前<em>看過</em>的資料上得到的誤差。驗證資料／誤差用來決定何時停止訓練，以避免過擬合。不幸的是，很多作者把「驗證資料／誤差」與「測試資料／誤差」當同義詞用。我討厭行話。 <a href="#fnref26" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn27" class="footnote-item"><p><a href="https://openai.com/index/quantifying-generalization-in-reinforcement-learning/">OpenAI（2018）新聞稿</a>：「<strong>即使有 16,000 個訓練關卡，我們仍然看到過擬合！</strong>」（強調為原文作者所加）完整論文見 <a href="https://arxiv.org/abs/1812.02341">Cobbe 等（2018）</a> <a href="#fnref27" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn28" class="footnote-item"><p>在貝氏統計中，「證據 E 支持假說 H 為真」的程度，是用「似然比」來衡量：當 H 為真時觀察到 E 的機率，除以當 H 為假時觀察到 E 的機率。簡寫為：likelihood ratio = P(E|H)/P(E/¬H)。</p>
<p>現在，把「A 與 B 存在相關」視為證據，把「A 與 B 存在<em>因果</em>連結」視為假說。因為在存在因果時你更可能觀察到相關（相較於不存在因果），因此似然比會大於 1，也就是說，<em>相關性是因果的證據。</em>（但問題在於你不知道<em>是哪一種</em>因果。）</p>
<p>更多數學細節，見 <a href="http://allendowney.blogspot.com/2014/02/correlation-is-evidence-of-causation.html">Downey（2014）</a> 的部落格文章（其亦為 O'Reilly《Think Bayes》作者）。想進一步了解貝氏定理，推薦 <a href="https://www.youtube.com/watch?v=lG4VkPoG3ko">3Blue1Brown 的視覺化介紹</a>。 <a href="#fnref28" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn29" class="footnote-item"><p>例如：大學沒有矮個子教授 → 於是他們認為矮個子不能當教授 → 因而不僱用矮個子教授 → 於是大學沒有矮個子教授 → ∞ <a href="#fnref29" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn30" class="footnote-item"><p>參見 Judea Pearl 於 2018 年接受 Quanta Magazine 的訪談：<a href="https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/">“To Build Truly Intelligent Machines, Teach Them Cause and Effect”</a>。Judea Pearl 也是上述「因果圖」的先驅之一，並協助將因果關係「數學化」。</p>
<p>在該訪談中，他評論現代 AI 仍停留在「前因果、只看相關」的時代：<em>「所有令人印象深刻的深度學習成就，歸根究柢都只是曲線擬合。」</em> <a href="#fnref30" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn31" class="footnote-item"><p>Egg Syntax（2024）：<a href="https://www.alignmentforum.org/posts/dLg7CyeTE4pqbbcnp/language-models-model-us">“Language Models Model Us”</a>。作者發現，<em>未經校準的 GPT-3.5</em> 僅靠<em>少量文字樣本</em>，就能以 86% 與 82% 的準確率預測作者的性別與族裔，優於隨機！（隨機基準：性別約 50%；在美國的族裔約 60%。[美國約 60% 是白人，所以若模型一直猜「白人」，60% 的時候它都會對，次次如此。]）</p>
<p>需要注意的是，該研究使用 OKCupid 的自我介紹文字，因此人們可能出於某些原因，會在字裡行間更凸顯性別刻板印象。所以作者改用美國 6 至 12 年級學生撰寫的 25,000 篇說服性文章，重作實驗。結果 GPT 的性別辨識準確率只從 86% 降到 80%，仍遠高於隨機（約 50%）！</p>
<p>有趣的是，GPT 在猜測作者性傾向時表現<em>比隨機更差</em>。（GPT 的準確率：67%。「永遠猜異性戀」：93%。）但別太放心，請看下一則腳註。 <a href="#fnref31" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn32" class="footnote-item"><p><a href="http://web.archive.org/web/20240725204611/https://i.warosu.org/data/sci/img/0145/14/1653510550860.pdf">Wang &amp; Kosinski（2018）</a>： “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images”。<a href="https://arxiv.org/pdf/1902.10739">Leuner（2019）</a> 的重複研究顯示，模型對「化妝、眼鏡、鬍鬚、頭部姿勢」等具有不變性。AI 確實是透過下顎／鼻樑／前額形狀與皮膚明暗等線索猜測性傾向。謝了，我超不喜歡這點。 <a href="#fnref32" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn33" class="footnote-item"><p>見 <a href="https://www.nature.com/articles/s41598-020-79310-1">Kosinski（2021）</a>： “Political orientation was correctly classified in 72% of liberal–conservative face pairs, remarkably better than chance (50%), human accuracy (55%), or one afforded <strong>by a 100-item personality questionnaire (66%) [?!?!]</strong>. Accuracy was similar across countries (the U.S., Canada, and the UK), environments (Facebook and dating websites), and when comparing faces across samples. Accuracy remained high (69%) <strong>even when controlling for age, gender, and ethnicity [!!]</strong>”</p>
<p>我特別加粗，因為——這到底是<em>什麼鬼</em>？！怎麼會是<em>一張臉</em>比<em>完整的人格問卷</em>更能揭示你的政治立場？！甚至比你的<em>年齡、性別與族裔加總起來</em>更相關？ <a href="#fnref33" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn34" class="footnote-item"><p>這個問題的理論可能性最早由 <a href="https://arxiv.org/pdf/1906.01820">Hubinger 等人於 2019 年</a>描述。在論文中，他們稱這個問題為「內部錯位」，並為我們帶來了 AI 對齊界的迷因 <a href="https://www.astralcodexten.com/p/deceptively-aligned-mesa-optimizers">「欺騙性對齊的中層優化器」</a>。你看，這很好笑，因為研究人員真的很不會取名字。他們取名字的功力爛到很好笑。真的很好笑。 <a href="#fnref34" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn35" class="footnote-item"><p>正如 Google DeepMind 的研究科學家 Alex Turner <a href="https://www.alignmentforum.org/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into">所說</a>：「內部對齊與外部對齊[目標錯誤泛化與目標錯誤指定]將一個難題分解成了兩個極其困難的問題」。Alex Turner 同時也是 <a href="https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview">Shard Theory</a> 的先驅之一，這是一個研究「強化學習」AI 如何逐步學習人類價值觀的研究計畫。 <a href="#fnref35" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn36" class="footnote-item"><p>來自 <a href="https://arxiv.org/pdf/2210.01790">Shah 等人 2022 年的研究</a>：「即使在規格正確的情況下，AI 系統仍可能追求非預期的目標，這就是<em>目標錯誤泛化</em>的情況。」[強調為原文所有] <a href="#fnref36" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn37" class="footnote-item"><p>這篇論文是 Langosco 等人 2021 年的 <a href="https://arxiv.org/abs/2105.14111">“Goal Misgeneralization in Deep Reinforcement Learning”</a>。下面的遊戲是 CoinRun，於 2018 年由 OpenAI 創建 (<a href="https://openai.com/index/quantifying-generalization-in-reinforcement-learning/">新聞稿</a>, <a href="https://arxiv.org/abs/1812.02341">論文</a>)。感謝 <a href="https://www.youtube.com/watch?v=zkbPdEHEyEI">Rob Miles 的優秀視頻</a> 介紹了這個案例！
（順帶一提，CoinRun 的美術資源來自慷慨又才華橫溢的創用 CC 遊戲藝術家 <a href="https://kenney.nl/assets/series:Platformer%20Pack">Kenney.nl</a> 💕） <a href="#fnref37" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn38" class="footnote-item"><p>向我目前最愛的驚悚網漫致意：<a href="https://www.webtoons.com/en/horror/everything-is-fine/list?title_no=2578"><strong>Everything Is Fine</strong></a>，作者 Mike Birchall。別擔心，這段不是爆雷——但這確實是我現在的粉絲理論。 <a href="#fnref38" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn39" class="footnote-item"><p>近似 <strong>「brrROOWWn」</strong>，見 Wiktionary 的發音標記。 <a href="#fnref39" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn40" class="footnote-item"><p>功利主義口味很多，我們就用這個基本版當學習範例。 <a href="#fnref40" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn41" class="footnote-item"><p>18 世紀哲學家 <strong>David Hume</strong> 主張（我同意）：你<strong>無法</strong>只從經驗觀察中，推出任何關於<strong>價值</strong>的判斷。
　　（<em>除非</em>你用語言把價值<strong>偷渡</strong>進來，例如：「Hans 是 Kraut，所以 Hans 是壞人」，把「Kraut」偷渡為負面；或「氰化物是<em>天然</em>的，所以氰化物是好的」，把「天然」偷渡為正面。相信「天然=好」稱為<strong>自然主義謬誤</strong>。）更多見維基百科：<strong>Is–ought problem</strong>。 <a href="#fnref41" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn42" class="footnote-item"><p>我想到的是某些自由意志主義者的義務論：它假定一條公理——<strong>不侵害原則</strong>（不主動施加非自願的傷害，除非是為了阻止／懲罰其他非自願傷害且比例相當）。這公理是否適合<strong>人類</strong>不在本文範圍；我要說的是它<strong>不是</strong>從科學或數學可推導，因此<strong>不能保證</strong>先進 AI 會重新發現它。 <a href="#fnref42" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn43" class="footnote-item"><p>是的，康德<strong>真的</strong>極端到認為「就算為救命也不該說謊」。次級來源可見 <a href="https://web.archive.org/web/20230605071851/https://askaphilosopher.org/2015/08/13/kant-on-lies-and-the-axe-man/">Klempner 2015</a>。但他個人生活又常用半真半假的話、或故意遺漏。 <a href="#fnref43" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn44" class="footnote-item"><p>把社會契約論先驅 <strong>Thomas Hobbes</strong> 的臉貼到漫畫 <strong>《Calvin and Hobbes》</strong> 的老虎 <strong>Hobbes</strong> 身上。哈。哈哈。哈哈哈。 <a href="#fnref44" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn45" class="footnote-item"><p>參見 Our World In Data 的統計；在人口最多的前 10 國家中，有 7 國超過 75% 的受訪者認為「同性戀永遠不正當」。連結見原文腳註。 <a href="#fnref45" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn46" class="footnote-item"><p>1966 年蓋洛普民調：對 MLK 的好感 33%，反感 63%。見 Pew Research 的整理。 <a href="#fnref46" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn47" class="footnote-item"><p>美國最高法院 1967 年 <em>Loving v. Virginia</em> 全國合法化異族婚，<strong>到 1997 年</strong>多數美國人才在民調上表達支持——<strong>晚了 30 年</strong>。 <a href="#fnref47" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn48" class="footnote-item"><p>爆雷一下：這和（但不完全等同）<strong>Coherent Extrapolated Volition</strong>（Yudkowsky 2004）相近；第三部分會講。 <a href="#fnref48" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn49" class="footnote-item"><p>例如 Ajeya Cotra（2023）：「Aligned 不該是 good 的同義詞」；Michael Chen（2024）「僅靠 Alignment 不足以讓未來變好」；Andrew Critch（2024）「沒有社會模型的安全，就不是安全」。連結見原文腳註。 <a href="#fnref49" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

	</article>

    <!-- FOOTER -->
	<div id="footer">
        <div id="footer_content">
<p style="font-size: 1.3em; line-height: 1.35em;">
<i>給血肉凡人的 AI 安全課</i> 由
<a href="https://ncase.me/">Nicky Case</a>
與
<a href="https://hackclub.com/">Hack Club</a>
合作製作。
</p>

<p>
🦕 <a href="https://ncase.me"><b>Hack Club</b></a>
是一個讓青少年一起動手做酷炫專案的非營利組織——
像是 <a href="https://cpu.land">cpu.land</a>、
<a href="https://sinerider.com">SineRider</a>，
以及這個！
歡迎參加
<a href="https://hackathons.hackclub.com">實體駭客松</a>，
在你的學校
<a href="https://hackclub.com/clubs/">成立社團</a>，
並
<a href="https://hackclub.com/slack/">和其他友善的青少年連結</a>。
</p>

<p>
😻 <a href="https://ncase.me"><b>Nicky Case</b></a>
其實是一件風衣裡的十五隻貓。
她做網路上的互動玩物，例如
<a href="https://audreyt.github.io/trust-zh-TW/">《信任的演化》</a>、
<a href="https://audreyt.github.io/anxiety/">《和焦慮一起冒險》</a>、
<a href="https://explorabl.es/">可探索解說</a>，以及更多。
</p>

<p>
💸 如果你<i>不是</i>青少年，而且是個在 AI 領域口袋很深的人，
<a href="https://hackclub.com/philanthropy/">看看如何支持 Hack Club！</a>
另外，Nicky 也有
<a href="https://www.patreon.com/ncase">Patreon</a>
與
<a href="https://ko-fi.com/nickycase">Ko‑Fi</a>。
（p.s：
<a href="../signup/supporters-p2.html">這是我的贊助者感謝頁！</a>）
</p>

<p style="text-align:center">
. . .
</p>

<p>
特別感謝 Hack Club 的這些青少年，<s>擔任免費童工</s>
協助試讀並對本作品提供回饋：
</p>

<blockquote>

<p>
<b>導言與第 1 部分：</b>
Arthur Beck、
Atharv Gupta、
Brendan Lee、
Celeste、
Charalampos Fanoulis、
Charlie、
Cheru Berhanu、
Claire Wang、
Elijah、
Fred Han、
Gia Bách Nguyễn、
Hajrah Siddiqui、
Jakob、
Joseph Ross、
Kieran Klukas、
Lexi Mattick、
Mason Meirs、
Michael Panait、
Nick Zandbergen、
Nila Palmo Ram、
Pixelglide、
py660、
rivques、
Samuel Cottrell、
Samuel Fernandez、
Saran Wagner、
Skyler Grey、
S&nbsp;P&nbsp;U&nbsp;N&nbsp;G&nbsp;E、
Vihaan Sondhi
</p>

<p>
<b>第 2 部分：</b>
Nanda White、
Nila Palmo Ram、
rivques、
Rohan K、
Samuel Fernandez
</p>

</blockquote>

<p>
也感謝以下非青少年提供回饋：
（雖然我猜他們在人生的<i>某個</i>時期也當過青少年）
</p>

<blockquote>
<p>
<b>導言與第 1 部分：</b>
Alex Kreitzberg、
B Cavello、
Paul Dancstep、
Tobias Rose-Stockwell
</p>

<p>
<b>第 2 部分：</b>
Egg Syntax、
Max Wofford、
Mithuna Yoganathan、
Tobias Rose-Stockwell
</p>

</blockquote>

<p>
若還有任何錯誤，一律算在
<a href="../suzie.png">替罪羊 Suzie</a>
頭上。
</p>

<p style="text-align:center">
. . .
</p>

<p>
<i>給肉身人類的 AI 安全</i> 開放任何人分享與重混，
但僅限非商業使用（例如教育）：
<a href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">CC BY‑NC 4.0</a>
</p>

<p>
如果你想引用這份作品，而且你自認是個嚴肅人士™，引用格式如下：
</p>

<blockquote>
Nicky Case，<i>《AI Safety for Fleshy Humans》</i>，<br>
https://AIsafety.dance，Hack Club（2024）。
</blockquote>

<p>
最後，這個網站的
<a href="https://github.com/hackclub/ai-safety-dance">開源程式碼</a>
在這裡！
</p>

<p>
謝謝你是會把致謝讀完的那種人～ 🙏
</p>

        </div>
	</div>
    <div id="post_credits">
        <p>
            喔，還有片尾彩蛋：
        </p>
<p>
    <a href="#AllFeetnotes">: 查看全部腳注 👣</a>
</p>
<p>
    另外，這些可展開的「概述」也很適合單獨閱讀：
</p>



<p>
    <a href="#StoryOfPassivePrediction">: 一個故事：為何就算只是被動預測也會導致不良的意外結果</a>
    <br>
    <a href="#SpuriousCorrelation">: 「偽相關」如何戲弄 AI</a>
    <br>
    <a href="#OntologicalCrisis">: 什麼是 AI 的「本體論危機」？</a>
    <br>
    <a href="#Pipocalypse">: Pi-啟示錄 的故事</a>
    <br>
    <a href="#WhatIfGoalMisgeneralizationIsGood">: 如果「目標誤泛化」其實……是好事？</a>
    <br>
    <a href="#TradingWithAdvancedAIs">: 人類能和超人類 AI 進行交易嗎？</a>
    <br>
    <a href="#Axiom">: 什麼是「公理」？</a>
</p>


    </div>

</div>
</body>
</html>

<!-- Load these scripts last. Screw 'em. -->
<!-- Orbit: make memory a choice -->
<script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
