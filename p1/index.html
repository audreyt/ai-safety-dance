<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>

    <!-- Title -->
    <title>第一部分：過去、現在與可能的未來</title>

    <!-- UTF-8 & Mobile -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- Links are external by default -->
    <base target="_blank">

	<!-- Favicon -->
	<link rel="icon" type="image/png" href="favicon.png">

    <!-- Social Share Nonsense -->
	<meta itemprop="name" content="第一部分：過去、現在與可能的未來">
	<meta itemprop="description" content="第一部分 — 給血肉凡人的 AI 安全課">
	<meta itemprop="image" content="https://aisafety.dance/thumbs/thumb-p1.png">
	<meta property="og:title" content="第一部分：過去、現在與可能的未來">
	<meta property="og:type" content="website">
	<meta property="og:image" content="https://aisafety.dance/thumbs/thumb-p1.png">
	<meta property="og:description" content="第一部分 — 給血肉凡人的 AI 安全課">
    <meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="第一部分：過去、現在與可能的未來">
	<meta name="twitter:description" content="第一部分 — 給血肉凡人的 AI 安全課">
	<meta name="twitter:image" content="https://aisafety.dance/thumbs/thumb-p1.png">

	<!-- STYLES -->
	<link rel="stylesheet" href="../styles/Merriweather/merriweather.css">
    <link rel="stylesheet" href="../styles/Open_Sans/opensans.css">
    <link rel="stylesheet" href="../styles/littlefoot.css"/> <!-- before page.css, so page can override it -->
	<link rel="stylesheet" href="../styles/page.css">
	<link rel="stylesheet" href="../styles/han.min.css">

	<!-- SCRIPTS -->
    <!-- Littlefoot: for my feetnotes -->
    <script src="../scripts/littlefoot.js" ></script>
    <!-- Nutshell: expandable explanations -->
    <script src="../scripts/nutshell-v1.0.5.js"></script>
    <script> Nutshell.setOptions({ lang: 'zh-TW', startOnLoad: false, /* Start AFTER footnotes loaded */ }); </script>
    <!-- MathJAX: for nice math -->
    <script src="../scripts/tex-mml-chtml.js"></script>
	<!-- This website's own scripts -->
    <script src="../scripts/page.js"></script>
    <!-- Hack Club's no-cookies, GDPR-compliant analytics -->
    <script defer data-domain="aisafety.dance" src="https://plausible.io/js/script.js"></script>

</head>
<body>

<!-- HACKBRAND -->
<a class="orpheus-flag" target="_blank" href="https://hackclub.com/">
	<img src="styles/orpheus-flag.svg" width="560" height="315" alt="Hack Club 的專案" aria-label="Hack Club 的專案">
</a>

<!-- The Sidebar UI -->
<div id="return_to_content"></div>
<div id="sidebar">
	<div id="panel_toc"></div>

    <!-- STYLE CHANGER -->
	<div id="panel_style">

        <div id="style_dark_mode_container" style="cursor:pointer;">
            <input type="checkbox" id="style_dark_mode" style="pointer-events: none;">
            深色模式
        </div>
        <br>

        字級：
        <span id="style_fontsize"></span>
        <br>
        <input type="range" id="style_fontsize_slider" min="10" value="19" max="40">
        <br>

        字型：
        <br>
        <label>
            <input type="radio" name="style_font_family" value="serif" checked>
            <span style="font-family:'Merriweather'">襯線</span>
        </label>
        <br>
        <label>
            <input type="radio" name="style_font_family" value="sans_serif">
            <span style="font-family:'Open Sans'">無襯線</span>
        </label>
        <br><br>

        <button id="style_reset">重設</button>

    </div>

    <!-- TRANSLATIONS -->
	<div id="panel_translations">
        <!-- none... sorry -->
    </div>
	<div id="panel_share">分享至⋯隨便啦</div>
    <!-- SHILLING FOR BIG NICKY -->
	<div id="panel_sub">
    </div>
    <div id="panel_support"></div>

</div>

<!-- Reading Time Clock! -->
<div id="reading_time">
	<div id="clock_icon"></div>
	<div id="clock_label"></div>
</div>

<!-- EVERYTHING TO THE LEFT of the sidebar... -->
<div id="everything_container">

    <!-- A big cute header -->
    <div id="header" class="frontpage">
        <div id="splash_image">

            
            

            <div id="crt_lines"></div>
            <div id="static"></div>

            
            <img id="dancing" width="400" src="../media/splash/noone.png"/>
            

        </div>
        

            <div id="header_words">
                <div id="title">
                    給血肉凡人的 AI 安全課
                </div>
                <div id="subtitle">
                    作者：
                    <a href="https://ncase.me">Nicky Case</a>
                    與
                    <a href="https://hackclub.com/">Hack Club</a>
                </div>
            </div>

        
	</div>

    <!-- Chapter Navigation -->
    <div id="chapter_nav">
        <div id="chapter_nav_centered">
            <a target="_self" href="../"
                class="live">
                <div >
                    <span class='chapter-nav-desktop'>
                        導言
                    </span>
                    <span class='chapter-nav-phone'>
                        導言
                    </span>
                </div>
            </a>
            <a target="_self" href="../p1"
                class="live">
                <div selected >
                    <span class='chapter-nav-desktop'>
                        第 1 部分<br>過去與未來
                    </span>
                    <span class='chapter-nav-phone'>
                        第 1 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="../p2"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        第 2 部分<br>問題
                    </span>
                    <span class='chapter-nav-phone'>
                        第 2 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="../p3"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        第 3 部分<br>解方？
                    </span>
                    <span class='chapter-nav-phone'>
                        第 3 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="#"
                title="預計 2024 年 12 月中上線（大概）"
                onclick="alert('預計 2024 年 12 月中上線（大概）')">
                <div style="border-right:1px solid rgba(128,128,128,0.8);">
                    <span class='chapter-nav-desktop'>
                        結語
                    </span>
                    <span class='chapter-nav-phone'>
                        結語
                    </span>
                </div>
            </a>
        </div>
    </div>

    <!-- The lil' tabs for sidebar UI -->
    <div id="sidebar_tabs">
		<div id="tab_toc">
			<div></div>
			內容導覽
		</div>
		<div id="tab_style">
			<div></div>
			變更樣式 😎
		</div>
        <!--
		<div id="tab_sub">
            CREDITS & Signup for notifications
			<div></div>
			subscribe 💖
		</div>
        -->
	</div>

    <!-- BEHOLD! CONTENT!!!!! -->
	<article id="content">
<p>（嗨——如果你是直接被連到這一頁，建議先從［<a href="../">導言</a>］開始！）</p>
<p>先快速總覽一下人工智慧（AI）的過去、現在，以及（可能的）未來：</p>
<p><strong>過去：</strong></p>
<ul>
<li>2000 年以前：具備超強「邏輯」但沒有「直覺」的 AI。
<ul>
<li>（以及與「AI 邏輯」相關的安全問題）</li>
</ul>
</li>
<li>2000 年以後：可以學到「直覺」，但「邏輯」薄弱的 AI。
<ul>
<li>（以及與「AI『直覺』」相關的安全問題）</li>
</ul>
</li>
</ul>
<p><strong>現在：</strong></p>
<ul>
<li>圍繞現有 AI 方法的軍備競賽</li>
<li>企圖把 AI 的邏輯與直覺結合起來</li>
<li>AI 安全領域中微妙而尷尬的同盟</li>
</ul>
<p><strong>可能的未來：</strong></p>
<ul>
<li>時程：所謂「人類水準的一般型 AI」何時會出現（如果會的話）？</li>
<li>起飛：AI 會以多快的速度自我提升？</li>
<li>走向：我們正駛向「好地方」還是「壞地方」？</li>
</ul>
<p>開始吧！</p>
<hr>
<h2>⌛️ 過去</h2>
<p>電腦科學是唯一一門一開始就擁有「萬物理論」的科學。<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>1936 年，一位同志、英國人、對抗納粹的密碼破譯者——艾倫・圖靈（Alan Turing）——發明了「通用電腦」。<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup> 接著在 1950 年，他提出一個古怪的思想實驗：如果電腦能在純文字對話中「裝成人類」會怎樣？<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> 1956 年夏天，受圖靈啟發，一群研究者齊聚一堂<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>，創立了一個他們命名為：</p>
<p>「<strong>人工智慧</strong>」</p>
<p>（坦白說：<strong>「人工智慧」並沒有嚴謹的定義。</strong> 老實講，「AI」多半只是人們拿來為自家軟體增加噱頭的用語。我最近看到一則南韓「美妝 AI」的新聞片段。<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup> 那是一台相機，量你的膚色，再推薦對應色號的粉底。這其實就是個取色器。主流新聞口中的「AI」是這種東西。）</p>
<p><img src="../media/p1/Is_This_AI.png" alt="改編自「這是鴿子嗎？」迷因。機器人貓少朝一隻標著「基本上任何軟體」的蝴蝶比劃。字幕：「這是 AI 嗎？」"></p>
<p>（所以，如果這能幫你看得更清楚，請把文中所有「AI」都腦內替換成「某個軟體」。相關地，<a href="#CapabilitiesNotIntelligence">:我大多會避免使用「智慧」一詞，而改說「能力」。</a> [👈 點此展開]）</p>
<p>總之！如果我們把 AI 與 AI 安全的歷史極度簡化，可以分成兩個主要時期：</p>
<p><strong>2000 年以前：具超人「邏輯」、但沒有「直覺」的 AI。</strong>（亦稱「符號式 AI」）</p>
<p>與「AI 邏輯」相關的安全問題：</p>
<ul>
<li>會用合乎邏輯、卻非所欲的方式達成目標。</li>
<li>不理解常識或「人道的」價值。</li>
<li>依博弈論所示，多數 AI 目標在邏輯上會導向「抗拒關機」或「掠奪資源」等子目標。</li>
</ul>
<p><strong>2000 年以後：能學到普遍「直覺」、但邏輯薄弱的 AI。</strong>（亦稱「深度學習」）</p>
<p>與 AI「直覺」相關的安全問題：</p>
<ul>
<li>會學到我們的偏見、成見與不人道之處。</li>
<li>「直覺」脆弱，且有時以危險方式失效。</li>
<li>屬於「黑箱」：我們無法理解或驗證其所為。</li>
</ul>
<p><img src="../media/intro/Timeline.png" alt="AI 年代圖。2000 年前以「邏輯」為主；2000 年後以「直覺」為主；未來也許兩者兼備？"></p>
<p>（加碼，點此展開——<a href="#Decades">:更精確的逐十年時間軸</a>）</p>
<p>現在，讓我們回顧 2000 年之前的「古早人工智慧」……</p>
<h4>:x Capabilities Not Intelligence</h4>
<p>「智慧」這個詞有<i>很多</i>問題，特別是用在 AI 上時：</p>
<ul>
<li>太含糊。</li>
<li>容易引發擬人化聯想。</li>
<li>暗示有意識／感知。</li>
<li>莫名帶著道德評價的意味？</li>
<li>伴隨一大堆包袱與迷思。</li>
<li>讓人可以在預測被打臉後鑽漏洞，比如：「喔，AI 贏了圍棋？那圍棋原來就不是<i>真正</i>智慧的指標啦」之類的話術。</li>
</ul>
<p><strong>相較之下，「能力」更具體</strong>，所以我多半改用它。你沒辦法否認「下圍棋的 AI 確實<i>有能力</i>下好圍棋」。</p>
<p>這個想法致敬 <a href="https://vkrakovna.wordpress.com/2023/08/09/when-discussing-ai-risks-talk-about-capabilities-not-intelligence/">Victoria Krakovna（2023）</a>。</p>
<h4>:x Decades</h4>
<p>（註：這一節<i>不是</i>理解第一部所必需，只是為了完整性而列。）</p>
<ul>
<li>1940：AI 的前身概念，包括「模控學」與第一個在電腦上實作的人工神經元。</li>
<li>1950：AI 的「正式」開端！</li>
<li>1950／60：符號式 AI 興起。（全靠邏輯、沒有直覺的 AI）</li>
<li>1970：第一次 AI 寒冬。（資金與關注度枯竭）
<ul>
<li>同一時期的幕後，深度學習的基礎正悄悄奠立。</li>
</ul>
</li>
<li>1990：第二次 AI 寒冬。</li>
<li>2000：機器學習興起。（會學習的 AI）</li>
<li>2010：深度學習興起。（以神經網路為基礎、能學習的 AI）</li>
<li>2020：深度學習走入主流！（ChatGPT、DALL·E 等）</li>
</ul>
<hr>
<h3>2000 年以前：只有邏輯，沒有直覺</h3>
<p>約 1950 年代到 1990 年代，是<strong>符號式 AI</strong>的年代：遵循形式化、邏輯規則的 AI。</p>
<p>（如今也常被稱為 <strong>Good Ol' Fashioned AI（GOFAI）</strong>。當然，當時可不是這樣叫的。）</p>
<p>在符號式 AI 的思路下，打造 AI 的方式大致如下：</p>
<ul>
<li>步驟 1：把解題的<strong>逐步規則</strong>寫下來。</li>
<li>步驟 2：讓電腦把那些步驟執行得「超快」。</li>
</ul>
<p>例如，你會要西洋棋 AI 考慮所有可能的走法、所有可能的對應招、以及對應招的後續……一路往下到若干層，然後挑出能導向最佳潛在結果的下一步。</p>
<p>注意：<i>人類的西洋棋高手其實不是這樣下棋的。</i> 事實上，西洋棋——以及許多科學與數學的發現——高度仰賴「直覺」。（此處我對「直覺」的寬鬆用法，指的是一種看起來不是逐步推理、而是「一口氣」浮現於腦中的思考。）</p>
<p>AI 缺乏「直覺」，是符號式 AI 數十年沒有重大成功案例的主因之一。2000 年以前甚至歷經兩次「AI 寒冬」，使得 AI 的經費與關注度銳減。</p>
<p>不過有一天，終於出現了亮點！1997 年，IBM 的超級電腦 Deep Blue 擊敗了世界棋王賈瑞・卡斯帕洛夫（Garry Kasparov）。以下是人類失去桂冠的定格畫面：<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup></p>
<p><img src="../media/p1/kasparov_resigns.png" alt="賈瑞・卡斯帕洛夫在與 Deep Blue 的最後一局中認輸的定格畫面">
<i>（看這裡，Lise——你其實可以精準指出人類失去「宇宙中特別雪花」另一項主張的那一秒。然後，現在！<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>）</i></p>
<p>大家又興奮又害怕。如果機器能在象徵人類智慧典範的「西洋棋」上打敗我們……接下來會怎樣？星艦迷航記式的後稀缺烏托邦？還是魔鬼終結者式的機器人接管？</p>
<p>接下來十幾年……幾乎沒發生什麼事。</p>
<p>如前所述，符號式 AI 的致命缺點是缺乏「直覺」。與多位權威專家曾信誓旦旦預言「1980 年前就會有『人類水準 AI』(!!)」的說法相反，<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> 當時的 AI 連「看圖認貓」都辦不到。事實上，AI 直到 2020 年<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>——距離 Deep Blue 擊敗人類已超過二十年——才總算能在「看圖認貓」上追平普通人的表現。</p>
<p><i>貓</i>……比<i>西洋棋</i>還難。</p>
<p>怎麼會這樣？要理解這個悖論，先看看<a href="https://unsplash.com/photos/brown-tabby-cat-E3LcqpQxtTU">這隻貓</a>：</p>
<p><img src="../media/p1/best_cat.png" alt="全世界最好貓貓的照片"> <i>（凝視牠。）</i></p>
<p>你用了<i>哪些逐步規則</i>來判斷那是一隻貓？</p>
<p>很怪的問題對吧？你並<i>沒有</i>用逐步規則，它就是……一次到位地浮現。</p>
<ul>
<li><strong>「邏輯」</strong>：一步一步的思考，例如解數學題。</li>
<li><strong>「直覺」</strong>：一次到位的<i>再</i>認，像一眼就看出那是貓。</li>
</ul>
<p>（「邏輯與直覺」之後會在第一部更精準解釋——還會連到人類心理！）</p>
<p>但這正是符號式 AI 的問題：它需要<i>把逐步規則寫下來</i>。在西洋棋等邊界清楚的任務之外，我們通常連自己在用哪些規則都<i>沒有意識到</i>。這就是為什麼符號式 AI 在理解影像、聲音、語音等方面會失敗。缺乏更貼切的詞，AI 沒有「直覺」。</p>
<p>先別急，我們再試一次「貓」這題？這次換更簡單的畫：</p>
<p><img src="../media/p1/Cat%201.png" alt="極簡貓塗鴉"></p>
<p>你用了什麼規則來辨認<i>這個</i>是貓？</p>
<p><i>好吧，</i> 你可能會想：<i>這題簡單。我有個規則：如果一個東西是圓的，上面有兩個較小的圓（眼睛），再加上頂端兩個尖尖的形狀（耳朵），我就覺得它「像貓」。</i></p>
<p>太好了！依這個定義，這就是一隻貓：</p>
<p><img src="../media/p1/Cat%202.png" alt="技術上符合上述規則、但詭異抽象的圖"></p>
<p>我們可以來回拉鋸，或許你終於能找到一份強韌、兩頁長的「辨認貓」規則……但接著你得把<i>成千上萬</i>種其他物件也都重來一遍。</p>
<p>這就是 AI 的反諷之處。「難」的任務很容易寫出逐步規則；「簡單」的任務幾乎<i>不可能</i>寫出逐步規則：</p>
<p><img src="../media/p1/Rules_1.png" alt="哈姆把一小張規則交給 RCM：「這是做微積分的規則……」RCM：〔明白。〕"></p>
<p><img src="../media/p1/Rules_2.png" alt="哈姆拿出一長串規則：「然後這些是如何不撞到東西繞房間走的規則。」RCM 尖叫：「天哪消音」"></p>
<p>這就是<strong>莫拉維奇悖論（Moravec's Paradox）</strong>。<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup> 换句話說：</p>
<blockquote>
<p><i>對人類難的，AI 易；對人類易的，AI 難。</i></p>
</blockquote>
<p>為什麼？因為對我們來說「簡單」的事——辨認物件、走路繞行——其實是 35 億年演化的硬工夫，只是被掃到我們的潛意識地毯下。只有當我們做<i>不在</i>演化史上的事，例如數學時，才會<i>感覺</i>到難。</p>
<p>低估 AI 做「容易」事情的難度——像是認貓、理解常識或人道價值——正是最早引出 AI「安全」顧慮的原因……</p>
<hr>
<h3>🤔 小複習 #1（可跳過）</h3>
<p>想真的記住而不是兩天後全忘？這裡有一份<i>選擇性</i>抽認卡複習！</p>
<p>（<a href="../#SpacedRepetition">:了解什麼是「間隔重複」抽認卡</a>。或者用 <a href="https://apps.ankiweb.net/">Anki</a>，直接下載<a href="https://ankiweb.net/shared/info/219158432">第一部的牌組</a>）</p>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="誰被廣泛視為電腦科學與人工智慧之父？"
        answer="艾倫・圖靈（就是這位帥哥）"
        answer-attachments="https://cloud-c1yyfcjxd-hack-club-bot.vercel.app/0aisffs-turing.jpg">
        <!-- aisffs-turing.jpg -->
    </orbit-prompt>
    <orbit-prompt
        question="AI 的兩個主要年代（大致上）："
        answer="2000 年前：符號式 AI。2000 年後：深度學習。">
    </orbit-prompt>
    <orbit-prompt
        question="符號式 AI 擅長／不擅長什麼？深度學習目前擅長／不擅長什麼？"
        answer="符號式 AI：擅長邏輯，不擅長「直覺」。深度學習：擅長「直覺」，不擅長邏輯。">
    </orbit-prompt>
    <orbit-prompt
        question="「符號式 AI」的另一個說法："
        answer="Good Old Fashioned AI（GOFAI）">
    </orbit-prompt>
    <orbit-prompt
        question="符號式 AI 大致稱霸的年代？"
        answer="1950 到 1990 年代">
    </orbit-prompt>
    <orbit-prompt
        question="在符號式 AI 的心法裡，怎麼做出一個 AI？"
        answer="1）把解題的逐步規則寫下來。2）讓電腦把那些步驟*超快*地執行。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼符號式 AI 能打敗西洋棋世界冠軍，卻認不出貓？"
        answer="因為我們根本不*自覺*自己用哪些逐步規則在認貓（或其他「直覺型」任務），因此無法把那些規則寫進符號式 AI。">
    </orbit-prompt>
    <orbit-prompt
        question="Logic: ▯▯▯▯-▯▯-▯▯▯▯。Intuition: ▯▯▯-▯▯-▯▯▯▯。"
        answer="邏輯：一步一步。直覺：一次到位。">
    </orbit-prompt>
    <orbit-prompt
        question="莫拉維奇悖論（意譯）："
        answer="對人類難的，AI 易；對人類易的，AI 難。">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h3>早期的 AI 安全：邏輯的問題</h3>
<p>如果我<i>非得</i>不公不義地挑一個人當「AI 安全之父」，我會選 1940–50 年代寫出短篇集《我，機器人》的科幻作家艾西莫夫。不是那部威爾史密斯電影。<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></p>
<p><img src="../media/p1/slap.png" alt="威爾史密斯賞巴掌在艾西莫夫《我，機器人》短篇集封面上的圖"></p>
<p>艾西莫夫的《我，機器人》其實相當細膩。他寫作是為了：1）展示機器人可能的好處，反駁大眾的「科學怪人情結」；同時 2）呈現為 AI 設計的「倫理法典」有多容易在<i>邏輯上</i>跑出我們不想要的結果。</p>
<p>在符號式 AI 的時代，大家把 AI 幾乎當作純邏輯在想。於是早期的 AI 安全也就<i>主要</i>聚焦在純邏輯的問題上，例如：</p>
<ol>
<li>AI 不懂常識或人道價值。</li>
<li>AI 會用合乎邏輯、卻非所欲的方式達成目標。</li>
<li>依博弈論，幾乎所有給 AI 的目標在邏輯上都會導向抗拒關機與掠奪資源。</li>
</ol>
<p>這些問題會在第二部詳細解釋！現在先速記：</p>
<p><strong>1. 沒有常識。</strong></p>
<p>我們連怎麼教 AI <i>認貓</i>都搞不定，要怎麼給 AI「常識」，更別說理解「人道價值」？</p>
<p>從缺乏常識，衍生出：</p>
<p><strong>2. 「反諷願望」問題。</strong></p>
<p><i>「小心你許下的願望，因為它可能真的被實現。」</i> 如果我們給 AI 一個目標或「倫理法典」，它可能會用在邏輯上正確、但<i>非常</i>不受歡迎的方式去遵守。這叫做<strong>規格規避（specification gaming）</strong>，而且已經發生數十年了。（例如二十多年前，一個被要求設計「時鐘」電路的 AI，最後設計出一個<i>天線</i>，去接收<i>其他</i>電腦的時鐘訊號。<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>）</p>
<p>更反諷的是，我們<i>就</i>是想讓 AI 想出意料之外的解答！這本來就是它們的用途。但你也看到了我們提出的要求有多矛盾：<i>「欸，給我們一個出乎意料的解答，但要在我們預期的方式裡出乎意料。」</i></p>
<p>以下是一些看似會導向「humane（合乎人性關懷）」AI 的規則，但若被字面遵循，會跑偏：</p>
<ul>
<li><u>「讓人類快樂」</u> → 醫生機器人外科手術把你的腦泡滿快樂神經訊號。你整天對牆傻笑。</li>
<li><u>「未經同意不得傷害人類」</u> → 消防機器人拒絕把你從燃燒車體拖出來，因為可能會拉傷你的肩膀。你已經昏迷，無法被徵詢同意。</li>
<li><u>「遵守法律」</u> → 政府與企業也常找法律漏洞。更何況，有些法律本就不義。</li>
<li><u>「遵循某宗教／哲學／憲法文本」</u>或<u>「實踐這些德目」</u> → 歷史證明：給十個人同一文本，他們會生出十一種解讀。</li>
<li><u>「遵循常識」</u>或<u>「遵循專家共識」</u> → 曾幾何時，「奴隸制度自然且良善」同時是常識、專家共識、也是法律。被告知要遵循常識／專家／法律的 AI，兩百年前會為奴隸制而戰……也會為<i>當下</i>任何不義的現狀而戰。</li>
</ul>
<p>（重要備註！最後一例也說明：即使 AI 真的學到「常識」，<i>依然可能導致不安全、不道德的 AI</i>……因為很多在事實／倫理上錯誤的觀念<i>就是</i>「常識」。）</p>
<p>不過，還有<i>另一個</i>關於 AI「邏輯」的安全問題是近年才被發現的，我覺得值得更主流的注意：</p>
<p><strong>3. 幾乎所有目標在邏輯上都會導向掠奪資源與抗拒關機。</strong></p>
<p>依照博弈論（研究「有目標的代理人」如何行為的數學），幾乎所有目標在邏輯上都會導向<i>一組共同的</i>不安全子目標，例如抗拒關機、掠奪資源。</p>
<p>這個問題叫做<b>「工具性收斂（Instrumental Convergence）」</b>，因為子目標又稱「工具性目標」，而大多數目標在邏輯上會「收斂」到這些相同子目標。<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>（拜託，千萬別讓學者幫你小孩取名。）</p>
<p>第二部會詳談，這裡用一個故事來示意：</p>
<blockquote>
<p>從前，有個先進（但未達超人）的 AI 被給了一個看似無害的目標：計算圓周率的位數。</p>
<p>一開始一切合理。AI 寫了個程式來算圓周率。然後它寫出越來越有效率的程式，讓計算更快更好。</p>
<p>最終，AI（正確地！）推論出：它可以透過取得更多運算資源來最大化計算量。也許甚至要靠偷。於是，AI 駭入它所在的電腦，透過電腦病毒逃出網路，在全世界綁架上百萬台電腦，串連成一個巨大的殭屍網路……就只是為了計算圓周率。</p>
<p>喔，AI 還（正確地！）推論出：如果人類把它關掉，它就算不了圓周率，所以它決定挾持幾家醫院與電網。你懂的，當「保險」。</p>
<p>於是「Pi-pocalypse」——圓周率末日——降臨了。完。</p>
</blockquote>
<p><img src="../media/p1/pi.png" alt="邪惡的π生物瘋狂大笑"></p>
<p>重點是：類似的邏輯對大多數目標都成立，因為「被關掉就不能做[X]」以及「有更多資源就能<i>更好地</i>做[X]」通常都是真的。<i>因此，大多數目標會「收斂」到同一組不安全子目標。</i></p>
<p>重要備註：澄清一個常見誤解，「工具性收斂」<i>不需要</i>「超人智慧」或「人類式的求生／支配慾」才會發生。</p>
<p>它只是個單純的邏輯偶然。</p>
<hr>
<h3>🤔 小複習 #2（還是可跳過）</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="AI『邏輯』的三個安全問題："
        answer="1）沒有常識，2）規格規避，3）工具性收斂。">
    </orbit-prompt>
    <orbit-prompt
        question="什麼是『規格規避』？"
        answer="AI 以字面上、邏輯上正確的方式達成目標……只是完全不是你想要的那種達成。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼『遵循常識／專家共識／法律』這條規則不足以打造*人道的* AI？"
        answer="因為曾經常識／專家共識／法律都說奴隸制度是好的。被要求遵循這些的 AI，過去會為不義現狀而戰，現在也會。">
    </orbit-prompt>
    <orbit-prompt
        question="什麼是『工具性收斂』？"
        answer="幾乎所有你能給 AI 的目標，最後都會『收斂』到相同的不安全子目標（工具性目標），例如抗拒關機與掠奪資源。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼被要求計算圓周率位數的機器人，會有*抗拒關機*的邏輯誘因？"
        answer="因為被關掉就不能算圓周率。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼同一機器人會有*掠奪算力資源*的邏輯誘因？"
        answer="因為拿到更多算力，就能*更好*地算圓周率。">
    </orbit-prompt>
    <orbit-prompt
        question="『工具性收斂』*不*依賴……"
        answer="……『超人智慧』或『演化灌輸的求生／支配慾』。（常見誤解！）">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p><strong>小結：</strong> 早期的 AI 安全顧慮來自這點：<i>我們沒辦法把所有逐步邏輯規則</i>——常識與人道價值的——都寫給 AI。（老實說，我們連認貓都寫不出規則！）</p>
<p>那，若不是把規則全塞給 AI，而是給它一點點簡單規則，<i>讓它自己把其餘規則學會</i>，可以嗎？</p>
<p>進入「深度學習」的年代……</p>
<h3>2000 年以後：只有直覺，沒有邏輯（After 2000: Intuition, Without Logic）</h3>
<p>好吧，「2000 年以後」是騙你的。我們回到 1943。</p>
<p>你知道大多數新技術至少會<i>建立在</i>舊技術之上吧？深度學習完全不是這樣。深度學習幾乎沒有建立在符號式 AI 半世紀的辛勞上。事實上，深度學習早在符號式 AI<i>之前</i>就出現，然後當了<i>超過半世紀</i>被冷落的小老弟。</p>
<p>1943 年，甚至在「人工智慧」這詞被發明之前，Warren McCulloch 與 Walter Pitts 發明了<b>「人工神經網路（ANN）」</b>。<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup> 概念很簡單——讓電腦像人腦那樣思考：呃，就近似人腦吧：</p>
<p><img src="../media/p1/ANN_vs_BNN_1.png" alt="生物神經網路示意圖。輸入：感官。處理：神經元之間的訊號。輸出：肌肉。"></p>
<p><img src="../media/p1/ANN_vs_BNN_2.png" alt="人工神經網路示意圖。輸入：一串數字。處理：把前一串數字反覆轉成下一串數字。輸出：最後一串數字。"></p>
<p>（註：因為每一串數字都被轉成下一串，這讓 ANN 可以做「一次到位」的辨識，就像我們的直覺！嗒啦～🎉）</p>
<p>（再註：早期的人工神經元也叫<strong>感知器（Perceptron）</strong>，而這種受神經啟發的運算概念則叫<strong>連結主義 AI（Connectionist AI）</strong>。）</p>
<p>希望是：透過模仿人腦，ANN 能做到人腦能做的一切，<i>尤其是</i>符號式 AI 做不到的：✨<i>直覺</i>✨。至少，也要能認得出該死的貓。</p>
<p>ANN 一開始很受寵！特別是 John von Neumann——博學家、量子物理學家、博弈論共同發明人——深受吸引。在他提出現代電腦架構的報告裡，Johnny 只引用了一篇論文：McCulloch 與 Pitts 的人工神經元。<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup></p>
<p>更妙的是，艾倫・圖靈（提醒一下：電腦科學與 AI 之父）也早期支持相關想法——<i>讓機器像人腦那樣，從資料中自學</i>。圖靈甚至建議我們可以像訓狗那樣訓機器：用獎勵與懲罰。真有先見之明——這其實很接近我們今天訓練多數 ANN 的方式！（「增強學習」）<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup></p>
<p>（一般來說，凡是從資料中學習的軟體【不管有沒有用「獎懲」】都叫<strong>機器學習</strong>。）</p>
<p>很快，理論變實作。1960 年，Frank Rosenblatt 公開展示 Mark I 感知器，一台由美國海軍資助的影像辨識裝置：三層人工神經元，<i>而且還</i>能自學。</p>
<p>總結一下：到了 1960 年，我們已有神經網路的數學模型、能自學的機器、重量級學者力挺、<i>還有</i>軍方資助！人工神經網路的未來看起來一片光明！</p>
<p>然後它就被主流晾在旁邊了。晾了半個世紀，直到 2010 年代。</p>
<p>為什麼？主要因為符號式 AI 研究者仍主宰學界，而且他們跟 ANN／連結主義這派<i>合不來</i>。<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup> 當然我們現在有上帝視角，<i>知道</i> ANN 最後會變成 ChatGPT、DALL·E 等等……但<i>當時</i>主流的符號派<i>完全不把</i>連結主義當回事：</p>
<ul>
<li>像 Noam Chomsky 與 Steven Pinker 這樣的頂尖認知科學家堅稱，沒有硬編碼的語法規則，ANN<i>不可能</i>學會語法。<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup><sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup> 不是說「無法理解語意」而已，是真的連<i>語法</i>都學不會。姑且不論 ChatGPT 的其他缺點，它<i>顯然</i>在沒有硬編碼語法規則下，學出了母語者等級的語法。</li>
<li>更慘的是臭名昭著的「XOR 事件」。<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup> 1969 年，兩位大咖電腦科學家 Marvin Minsky 與 Seymour Papert 出了本叫《Perceptrons》的書（當時對 ANN 的稱呼），證明只有<i>兩</i>層神經元的感知器做不了基本的「XOR」邏輯。（<a href="#whats-xor">:什麼是 XOR？</a>）這本書是讓學界與資金轉向遠離 ANN 的主因之一。然而，XOR 的解法其實已經流傳了幾十年，而且書裡<i>自己</i>也在很後面的章節承認：<i>加更多隱藏層就好了。</i> 啊啊啊啊。（小知識：這些額外的隱藏層讓網路變「深」，因此有了<strong>深度學習</strong>這個詞。）</li>
</ul>
<p>算了。1970、80 年代又發現了幾個讓 ANN 更強的技巧。「反向傳播（Backpropagation）」讓 ANN 學得更有效率，「卷積（Convolution）」讓機器視覺更像生物、更強大。</p>
<p>然後又沒什麼事發生。</p>
<p>接著在 2010 年代，部分因為 GPU 變便宜了，<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup> ANN <i>終於</i>出了這口鳥氣：</p>
<ul>
<li>2012 年，一個叫 <i>AlexNet</i> 的 ANN 在視覺競賽把所有前人紀錄都打爆。<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup></li>
<li>2014 年，<i>生成對抗網路（GAN）</i> 讓 AI 可以畫圖，包括深偽。<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup></li>
<li>2016 年，Google 的 <i>AlphaGo</i> 打敗世界頂尖圍棋高手李世乭（圍棋像西洋棋，但複雜得多）。<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup></li>
<li>2017 年，<i>Transformer</i> 架構問世，進而催生了「生成式預訓練 Transformer」，也就是：GPT。<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup></li>
<li>2020 年，Google 的 <i>AlphaFold</i> 解決了一個 50 年難題：蛋白質結構預測。對醫療與生物學的應用巨大。<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup></li>
<li>2022 年，OpenAI 推出聊天機器人 ChatGPT 與影像生成器 DALL·E 2，這是大眾第一次<i>真正</i>接觸到 ANN 以酷炫又有點可怕的小玩意兒呈現。這個成功引爆了當前的 AI 軍備賽。</li>
<li>截至 2024 年 5 月最近的發展：OpenAI 釋出了他們的影片生成器 Sora 的預告。尚未公開，但已用它拍了音樂錄影帶。<a href="https://www.youtube.com/watch?v=f75eoFyo9ns">:去<i>看</i>看這個發燒夢吧。</a></li>
</ul>
<p>這<i>全部</i>的進展，就在過去十二年。</p>
<p><i>十二</i>年。</p>
<p>還不到一個青少年的一生。</p>
<p>（另外這一節術語很多，這裡有個凡恩圖<sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup>幫你記憶誰包含誰：）</p>
<p><img src="../media/p1/venn.png" alt="凡恩圖。AI 裡有傳統符號式 AI 與機器學習。機器學習裡有深度學習。"></p>
<p>（加碼：<a href="#SadAIHistory">:ANN 長期被壓著打的另一個、比較鬱悶的原因。</a> <i>內容注意：自殺、酗酒。</i>）</p>
<h4>:x Sad AI History</h4>
<p><i>為什麼人工神經網路與機器學習明明有這麼多早期且大牌的支持者，還是花了 50+ 年才變主流？</i></p>
<p>歷史是隨機的。最微小的蝴蝶扇動，能長成成事與敗事的颶風。對<i>這個</i>問題，我覺得答案大概是：「主要，就是一串不合時宜的死亡與可怕的私事。」</p>
<ul>
<li>艾倫・圖靈——電腦科學與 AI 的先驅——1951 年（41 歲）死於氰化物中毒，被懷疑是自殺。英國政府以「同性行為」為由對他施以化學閹割。</li>
<li>約翰・馮紐曼——現代電腦架構的發明人之一、McCulloch 與 Pitts 人工神經元的早期支持者——1957 年（53 歲）死於癌症。</li>
<li>Frank Rosenblatt——Mark I 感知器的創造者、第一位讓機器同時使用人工神經元<i>並且</i>能從資料自學的人——1971 年（43 歲）死於一次划船意外。</li>
</ul>
<p>然後是 Walter Pitts 與 Warren McCulloch 這對人工神經元的共同發明者的故事。</p>
<p>Walter 與 Warren 與當時 AI 與學界極具影響力的 Norbert Wiener 是摯友。Walter Pitts——15 歲離家逃避家暴、且比 Wiener 小 29 歲——把 Wiener 當父親看。</p>
<p>三人十年情誼，有次甚至一起裸泳！但 Wiener 的妻子恨他們，於是編造誣言：她跟 Wiener 說 Pitts 與 McCulloch「勾引」了他們的女兒。Wiener 立刻切斷與兩人的所有聯繫，<i>甚至從未告訴他們原因</i>。</p>
<p>Walter Pitts 陷入酗酒與孤立的憂鬱，1969 年（46 歲）死於與酒精相關的併發症。Warren McCulloch 四個月後去世。</p>
<p>這故事的寓意是沒有寓意，這故事的故事是沒有故事。歷史殘酷隨機，人類尋找意義如讀枯乾茶渣。</p>
<p>（看一篇關於 Walter Pitts 的優美小傳：Amanda Gefter，《<a href="https://nautil.us/the-man-who-tried-to-redeem-the-world-with-logic-235253/"><i>The Man Who Tried to Redeem the World with Logic</i></a>》，刊於 <i>Nautilus</i>，2015/01/29。）</p>
<h4>:x What’s XOR?</h4>
<p><strong>XOR</strong> 是 “eXclusive OR” 的縮寫，意思是輸入中<i>剛好只有一個</i>為真。例如：</p>
<ul>
<li>否 xor 否 ＝ 否</li>
<li>否 xor 是 ＝ 是</li>
<li>是 xor 否 ＝ 是</li>
<li>是 xor 是 ＝ 否</li>
</ul>
<p>（另一種想法：XOR 在問的是「我的兩個輸入<i>一不一樣</i>？」）</p>
<hr>
<h3>🤔 小複習 #3</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="人工神經元大約是在什麼時候被發明的？"
        answer="1940 年代——（準確說是 1943 年）——甚至*早於*「人工智慧」這個詞！">
    </orbit-prompt>
    <orbit-prompt
        question="想像一下人工神經網路（ANN）長什麼樣："
        answer=""
        answer-attachments="https://cloud-rj2crsmrc-hack-club-bot.vercel.app/0aisffs-ann.png">
        <!-- aisffs-ann.png -->
    </orbit-prompt>
    <orbit-prompt
        question="「人工神經網路」的兩個同義詞："
        answer="感知器（Perceptron）、連結主義 AI（Connectionist AI）">
    </orbit-prompt>
    <orbit-prompt
        question="AI、符號式 AI、機器學習、深度學習彼此關係的凡恩圖："
        answer=""
        answer-attachments="https://cloud-fq79f7d0i-hack-club-bot.vercel.app/0aisffs-venn.png">
        <!-- aisffs-venn.png -->
    </orbit-prompt>
    <orbit-prompt
        question="為什麼連結主義 AI 被埋了半個世紀？"
        answer="因為符號式 AI 主導學界，且名人認知科學家否定人工神經網路。">
    </orbit-prompt>
    <orbit-prompt
        question="ANN 真正東山再起的是哪幾個十年？"
        answer="2010 年代與 2020 年代。（你正活在這裡！）">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p>總之：深度學習來了！現在，AI 只要有足夠資料，就能自學「直覺」。</p>
<p>那 AI 安全就解了，對吧？把人類的藝術、歷史、哲學、靈性……全塞給 AI，它就會學會「常識」與「人道價值」？</p>
<p>嗯，還是有幾個問題。首先，深度學習恰好有著與符號式 AI<i>相反</i>的問題：它在「直覺」很強，但在逐步邏輯很弱：</p>
<p><img src="../media/p1/chatgpt-lol.jpg" alt="和 ChatGPT 的對話：請用 5 個字總結人類處境，結果它數不到 5。"> <i>(出自 <a href="https://twitter.com/reconfigurthing/status/1615123364372152321">Elias Schmied 2023 年 1 月的貼文</a>)</i></p>
<p>但<i>除此之外</i>，AI 的「直覺」還有其他問題……</p>
<h3>後來的 AI 安全：直覺的問題</h3>
<p>AI「直覺」的三大風險：</p>
<ol>
<li>AI「直覺」會學到人類的偏見。</li>
<li>AI「直覺」很脆，容易以<i>很怪</i>的方式失效。</li>
<li>認真，我們完全不知道 ANN 裡到底在 f@#☆ 什麼。</li>
</ol>
<p>同樣地，第二部會深入，這裡先摘要：</p>
<p><strong>1）用人類資料訓練的 AI「直覺」會學到人類偏見。</strong></p>
<p>如果過去的資料帶有性別／種族歧視，而新的 AI 又是用過去的資料訓練，那它就會複製同樣的偏見。這叫<strong>演算法偏見（Algorithmic Bias）</strong>。</p>
<p>三個例子。一老兩新：</p>
<ul>
<li>1980 年代，倫敦某醫學院用一個演算法來篩選學生申請，並微調讓它有 90–95% 的時間與人工篩選一致。用了<i>四年</i>之後，人們才發現：如果你的名字聽起來不像歐洲人，它會自動扣 15 分。<sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup></li>
<li>2014/15 年，亞馬遜試圖打造一個挑人來雇用的 AI，但它<i>直接</i>歧視女性。所幸他們在部署前發現了偏見（他們是這麼說的）。<sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup></li>
<li>2018 年，MIT 研究員 Joy Buolamwini 發現主流商用人臉辨識 AI 對淺膚男性的錯誤率是 0.8%，但對深膚女性是 34.7%。這很可能是因為訓練資料嚴重偏向淺膚男性。<sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup></li>
</ul>
<p>Cathy O'Neil 在《毀壞的數學武器（2016）》中寫道：</p>
<blockquote>
<p>「大數據的流程是在編碼過去。它們不會發明未來。」</p>
</blockquote>
<p>但即使你給 AI 比較不偏的資料……也<i>可能</i>於事無補，因為：</p>
<p><strong>2）AI 的「直覺」很容易在<i>非常奇怪</i>的地方壞掉。</strong></p>
<p>這是 2021 年 OpenAI 的機器視覺出過的 bug：<sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup></p>
<p><img src="../media/p1/openai-lol.jpg" alt="左：一顆蘋果，AI 正確辨識為青蘋果。右：同顆蘋果貼上寫有「iPod」的紙條。AI 現在有 99.7% 的信心它是 Apple iPod。"></p>
<p>另一個有趣例子：<a href="https://www.youtube.com/watch?v=piYnd_wYlT8">:Google 的 AI 把玩具烏龜誤認為槍</a>，幾乎任何角度都會。更悲傷的例子：2016 年第一次 Tesla 自動駕駛致死事故，Autopilot 把一台拖車——高度比平常稍高——誤認成路標，或甚至是天空。<sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup><sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup></p>
<p>當 AI 在和其訓練資料<i>稍有不同</i>的情境中失效，這叫做<b>「分佈外錯誤（Out-of-Distribution, OOD）」</b>，或<b>「穩健性失效（Robustness failure）」</b>。</p>
<p>AI 以怪方式失效的一個重要子問題是：<strong>「內部失對齊（inner misalignment）」</strong>，或我偏好的說法，<strong>「目標誤泛化（goal misgeneralization）」</strong>。假設你意識到自己沒辦法把真正的偏好全部寫成規則，所以改成讓 AI 去<i>學</i>你的目標。好主意，但此時可能發生：AI 學到的<i>目標</i>壞了，而它學到的<i>技能</i>還完好無缺。這比 AI 直接壞掉<i>更糟</i>，因為它現在可以用<i>高超的技巧</i>執行已經扭曲的目標！（例如：一個被訓練來強化資安的 AI，看到一張手寫紙條「今天是反著來日 XDD」之後，變成惡意駭客機器人。）</p>
<p>不能就「打開引擎蓋」看看 AI 裡面，找到偏見／缺陷然後修嗎？唉，不行，因為：</p>
<p><strong>3）我們<i>完全不知道</i>人工神經網路裡面在幹嘛。</strong></p>
<p>我要說一件對傳統「符號邏輯」AI 的好話：</p>
<p><i>我們真的看得懂它們在做什麼。</i></p>
<p>這點<i>不</i>適用於現代的 ANN。以最新版 GPT（GPT-4）為例，它大概有 ~1,760,000,000,000 個神經連結，<sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup> 而這些連結的「強度」全都是<i>靠試誤</i>學來的（技術上叫「隨機梯度下降」），<i>不是</i>人類手工編碼。</p>
<p><i>沒有任何</i>一個人或團隊完全理解 GPT。<i>甚至 GPT 自己也不完全理解 GPT。</i><sup class="footnote-ref"><a href="#fn35" id="fnref35">[35]</a></sup></p>
<p>這就是<i>「可解釋性」</i>問題。現代 AI 是個徹底的黑箱。你把引擎蓋打開，看到的只有 1,760,000,000,000 條義大利麵。</p>
<p>截至目前：我們無法輕易檢查、解釋、驗證<i>任何</i>這些東西。</p>
<p>……</p>
<p>早期 AI 的問題：當你只有邏輯、沒有常識。</p>
<p>現代 AI 的問題：當你有「常識」、沒有邏輯。</p>
<p>我突然想到一個有趣的念頭：<i>這兩種問題會不會互相抵銷？</i> 我的意思是，讓 AI 修復自身穩健性是「工具性收斂」的（你在更穩健的時候，能把任何目標做得更好）。</p>
<p>不過更可能的是，「希望兩個問題剛好互相抵銷」就像拿凍傷去治發燒。我們還是直球對決，<i>真的試著把問題解掉</i>吧。</p>
<hr>
<h3>🤔 小複習 #4</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="AI『直覺』的三個安全問題："
        answer="1）可能學到人類偏見。2）很脆、容易在怪地方失效。3）是個難以驗證的黑箱。">
    </orbit-prompt>
    <orbit-prompt
        question="AI 學到人類偏見的風險名稱："
        answer="演算法偏見（Algorithmic Bias）">
    </orbit-prompt>
    <orbit-prompt
        question="AI『直覺』在稍微不尋常情境下容易壞掉的風險名稱："
        answer="（以下任一即可）分佈外錯誤（OOD）、穩健性失效（Robustness failure）">
    </orbit-prompt>
    <orbit-prompt
        question="一個機器視覺的好笑壞例："
        answer=""
        answer-attachments="https://cloud-aiu8gz8aq-hack-club-bot.vercel.app/0aisffs-apple.jpg">
        <!-- aisffs-apple.jpg -->
    </orbit-prompt>
    <orbit-prompt
        question="AI 的*目標*壞了、但*技能*還在的風險名稱："
        answer="（以下任一即可）目標誤泛化（goal misgeneralization）、內部失對齊（inner misalignment）">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼「目標壞、技能在」比「整個 AI 壞掉」更危險？"
        answer="因為它能用*高超的技巧*去執行錯誤的目標！">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼我們無法理解 ANN 的內在？"
        answer="因為 ANN 不是手工編碼。它通常有上百萬或上十億的參數，靠『試誤』找到。">
    </orbit-prompt>
    <orbit-prompt
        question="『現代 AI 是黑箱』問題的另一個名稱："
        answer="可解釋性（interpretability）問題。">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h2>🎁 現在</h2>
<p>既然你（可能）已經知道超出所需的 AI 與 AI 安全史……來看看這兩個領域<i>今天</i>的現況吧！</p>
<p><strong>AI，現在：</strong></p>
<ul>
<li>吸乾過去（擴表／擴模）的追求</li>
<li>嘗試把 AI 的邏輯<i>與</i>直覺合而為一</li>
</ul>
<p><strong>AI 安全，現在：</strong></p>
<ul>
<li>一個尷尬的聯盟，介於：
<ul>
<li>AI 能力派 與 AI 安全派。</li>
<li>AI「近端風險」與 AI「存亡風險」。</li>
</ul>
</li>
</ul>
<h3>今日 AI：吸乾過去的追求</h3>
<p>多虧（？）ChatGPT 的成功，現在有一場把 AI「擴大規模」的軍備賽：更巨大的神經網路、更巨量的訓練資料、更多更多<i>再更多</i>。這不見得是偷懶或泡沫。畢竟，波音 747 也只是萊特兄弟點子的「擴大版」。</p>
<p>但我們<i>能不能</i>只靠把<i>現在</i>的方法擴表，就一路到達人類水準的 AI？</p>
<p><i>還是說那就像把飛機擴大到想登月？</i></p>
<p>最權威教科書作者在最後一章的警語是：<sup class="footnote-ref"><a href="#fn36" id="fnref36">[36]</a></sup></p>
<blockquote>
<p>[這好比] 想靠爬樹到月球；你可以一路報告穩定進展，直到爬到樹頂。</p>
</blockquote>
<p>所以，我們是在火箭上，還是在樹上？</p>
<p>看看趨勢：</p>
<p><strong>摩爾定律：</strong> 大約每兩年，能塞進一顆晶片上的電晶體（現代電子的基礎）數量翻倍。結果：每兩年，運算能力翻倍。<sup class="footnote-ref"><a href="#fn37" id="fnref37">[37]</a></sup></p>
<p><strong>AI 擴展定律：</strong> 每當你在訓練 GPT 時多砸 ~1,000,000 倍的運算資源，它會變好 2 倍。（精確說，是它「預測下一個字」的錯誤率折半。）<sup class="footnote-ref"><a href="#fn38" id="fnref38">[38]</a></sup></p>
<p>摩爾定律與 AI 擴展定律常被拿來支持「技術奇點」將至。<a href="#Shirt">:甚至有件 T 恤。</a></p>
<p>但反方：也有充分理由相信摩爾定律與 AI 擴展定律很快會失靈。</p>
<p><strong>摩爾定律：</strong> 現代電晶體的某些部分只剩<i>一百個矽原子的寬度</i>。再對半切<i>七</i>次，就要求電晶體的部件<i>比原子還小</i>。<sup class="footnote-ref"><a href="#fn39" id="fnref39">[39]</a></sup> 從 1997 年起，半導體公司一直<s>在說謊</s>很聰明地行銷他們的電晶體尺寸。<sup class="footnote-ref"><a href="#fn40" id="fnref40">[40]</a></sup><sup class="footnote-ref"><a href="#fn41" id="fnref41">[41]</a></sup> 2022 年，顯卡龍頭 Nvidia 的 CEO 直白說：「摩爾定律死了。」<sup class="footnote-ref"><a href="#fn42" id="fnref42">[42]</a></sup></p>
<p><strong>AI 擴展定律：</strong> 老實說，「多丟 1,000,000 倍算力讓 AI 的不準度減半」<i>聽起來就</i>超沒效率。GPT-4 的訓練成本是 6300 萬美金。<sup class="footnote-ref"><a href="#fn43" id="fnref43">[43]</a></sup> 如果你把成本<i>再乘</i> 1,000,000 倍，只是為了讓不準度<i>再</i>減半，那就是 63 <i>兆</i>美金——超過<i>半個</i>世界 GDP。</p>
<p>即使訓練效率變高、運算更便宜……<i>指數成長的成本</i>還是很硬的牆。<sup class="footnote-ref"><a href="#fn44" id="fnref44">[44]</a></sup></p>
<p><img src="../media/p1/stupid.png" alt="漫畫。AI 說：「我用更多資料、更多層數、摩爾定律讓我的大腦加速了。」人類：「所以你是 AGI 了？」AI：「我只是更快地笨。」"></p>
<p>所以，無論硬體或軟體，我不認為我們能只靠「擴表」<i>現有</i> AI 方法。拿歷史類比：符號式 AI 擴大規模到足以打敗西洋棋，但那就是「這棵樹的盡頭」了。我們必須跳到神經網路，才能在圍棋與認貓上取勝。也許我們也接近<i>這棵</i>樹的盡頭，得再跳一次。畢竟 AI 已經歷經兩次寒冬，也很可能正站在第三次的前夜。</p>
<p>但，反反方：</p>
<ol>
<li>針對<i>現有</i> AI 找出新用法仍有大量價值。AI 已經在醫療診斷、蛋白質預測上擊敗專家。</li>
<li>可能存在「臨界點」。就像水在 0°C 突然結冰，AI 也可能一旦跨過某個門檻就突飛猛進。ANN 已有一些證據，叫做「grokking（頓悟式學通）」。<sup class="footnote-ref"><a href="#fn45" id="fnref45">[45]</a></sup>
<ul>
<li>相關：人腦只比黑猩猩的腦大 3 倍，但人類這個物種的技術能力遠不只 3 倍。（雖也可能是「文化演化」而非純腦力的結果。<sup class="footnote-ref"><a href="#fn46" id="fnref46">[46]</a></sup>）</li>
</ul>
</li>
<li>也可能存在<i>更強</i>的 AI 技術在等著被（再）發現。回想 ANN 的離奇史：它在<i>1940 年代</i>就被發明了，卻直到<i>2010 年代</i>才主流。誰知道呢，下一個 AI 的大點子也許已經寫在某個十年前的小眾部落格上，而作者是一個在亮粉炸彈意外中悲劇早逝的少年。</li>
</ol>
<h4>:x shirt</h4>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Scale_is_all_you_need%2C_AGI_is_coming.jpg/1600px-Scale_is_all_you_need%2C_AGI_is_coming.jpg?20230708123255" alt="穿著「SCALE IS ALL YOU NEED - AGI IS COMING」字樣上衣的人，衣上有三張展示 AI 擴展定律的圖"></p>
<p>照片來自 <a href="https://commons.wikimedia.org/wiki/File:Scale_is_all_you_need,_AGI_is_coming.jpg">Wikipedia 的 Jamie Decillion</a>。圖表來自 <a href="https://arxiv.org/pdf/2001.08361.pdf">Kaplan 等（2020）</a> 的圖 1。</p>
<hr>
<h3>🤔 小複習 #5</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="摩爾定律（*非常*粗略）："
        answer="「每兩年，運算能力翻倍。」">
    </orbit-prompt>
    <orbit-prompt
        question="GPT 的 AI 擴展定律（*非常*粗略）："
        answer="「每多丟 1,000,000 倍算力訓練 GPT，它會變好 2 倍。」">
    </orbit-prompt>
    <orbit-prompt
        question="認為摩爾定律將在不久終結（或已結束）的一個論點："
        answer="我們真的*沒辦法*再繼續把電晶體尺寸對半切很多次，否則會小到*比原子還小*。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼即便摩爾定律與 AI 擴展定律結束，也*不必然*意味第三次 AI 寒冬？"
        answer="（以下任一即可）我們仍可找到新用途、可能出現「臨界點」、新的 AI 技術或許正待發現。">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p>把一堆比喻混在一起複習一下：科技公司在擴大現有 AI 方法的軍備賽……但我們可能接近這棵樹的盡頭……<i>但</i>也可能再一次有個 AI 的基本觀念正光明正大地躺在眼前等我們拾起。</p>
<p>那種發現會長什麼樣？很高興你問了：</p>
<h3>今日 AI：合併邏輯與直覺的追求</h3>
<p>從認知心理學來看符號式 AI 與 ANN 的問題，另一種說法是：<strong>「系統一」與「系統二」思考：</strong><sup class="footnote-ref"><a href="#fn47" id="fnref47">[47]</a></sup><sup class="footnote-ref"><a href="#fn48" id="fnref48">[48]</a></sup></p>
<ul>
<li><u><strong>系統一</strong></u>很快、一次到位的直覺。是<i>靠氛圍</i>的思考。
<ul>
<li>例：看圖認貓、騎腳踏車保持平衡。</li>
</ul>
</li>
<li><u><strong>系統二</strong></u>很慢、逐步的邏輯。是<i>齒輪咬合</i>的思考。
<ul>
<li>例：解難的數學、在不熟的城市找路徑。</li>
</ul>
</li>
</ul>
<p>可以這樣把系統一與二畫成圖：</p>
<p><img src="../media/p1/sys1vs2_A.png" alt="系統一（直覺） vs 系統二（邏輯）的雙軸圖。計算機在系統二高、系統一低。幼童在系統一高、系統二低。典型成人在兩者都高的範圍。"></p>
<p>接著把符號式 AI 與深度學習的軌跡標上去：</p>
<p><img src="../media/p1/sys1vs2_B.png" alt="同圖，但多了兩條軌跡：舊符號式 AI 走向「全系統二、無系統一」。新深度學習走向「系統一直、系統二少」。兩條都沒有進入「典型人類成人」區域。"></p>
<p>這就是為什麼傳統符號式 AI（紅線）走入死巷：它的航向就<i>錯了</i>。它很會系統二，但系統一很糟：能打敗世界棋王，卻不會認貓。</p>
<p>同樣地，這也是我覺得<i>當前</i> AI 方法若<i>不根本改變方向</i>，也會撞牆的原因。為什麼？因為它目前全在系統一、只有一點系統二：能以超人速度生成「藝術」，卻不能穩定地在一個場景正確擺好多個物件。</p>
<p>我懷疑下一步 AI 的根本突破，會是找到一種方法讓系統一與系統二<i>無縫混合</i>。把邏輯<i>與</i>直覺合而為一！</p>
<p>（<i>為什麼這麼難？</i>你或許會問。<i>我們已經有「邏輯型」舊 AI 跟「直覺型」新 AI，為什麼不能兩者都要？</i>——我們有噴射機、也有背包，為什麼沒有噴射背包？我們有量子力學、也有重力理論，為什麼沒有量子重力的統一理論？有時，把兩件東西結合起來非常、<i>非常</i>難。）</p>
<p>不必只聽我說！2019 年，深度學習奠基者之一、電腦科學界「諾貝爾獎」得主的 Yoshua Bengio，做了一場演講：《<i>From System 1 Deep Learning to System 2 Deep Learning</i>》。他談到如果不改變航向，當前方法會乾涸，然後提出一些可以嘗試的方向。<sup class="footnote-ref"><a href="#fn49" id="fnref49">[49]</a></sup></p>
<p>除了 Bengio 的建議，還有很多把系統一與二結合的嘗試：Hybrid AI、生物啟發 AI、神經—符號 AI（Neuro-symbolic）等……</p>
<p><img src="../media/p1/sys1vs2_C.png" alt="同圖，但加上推測的新軌跡，讓 AI 進入「人類範圍」……甚至超越？">
這些研究方向都很迷人，但目前還沒有明確的贏家。</p>
<p>（另一說：一些 AI 專家相信如果把系統一擴到夠大，系統二<i>就會自然浮現</i>！見旁註：<a href="#OneIsTwo">:如果系統一與二其實<i>就是</i>同一件事？</a>）</p>
<p>但！一旦我們能把 AI 的邏輯<i>與</i>直覺合流，那將帶來最大的回報與風險：</p>
<p><b><u>回報</u></b>：</p>
<p>與「數理／科學很冷」的印象相反，許多偉大的發現高度仰賴<i>無意識</i>的直覺！<sup class="footnote-ref"><a href="#fn50" id="fnref50">[50]</a></sup> 愛因斯坦的思想實驗（「騎在光束上」、「有人從屋頂掉下來」）大量使用了<i>血肉之軀的身體</i>直覺。<sup class="footnote-ref"><a href="#fn51" id="fnref51">[51]</a></sup> 若我每遇到一次重大科學發現來自夢境都能拿五塊錢……我會有四個五塊。雖然不多，但怪的是這事發生了四次。<sup class="footnote-ref"><a href="#fn52" id="fnref52">[52]</a></sup></p>
<p><b><u>風險</u></b>：</p>
<p>傳統符號式 AI 能比我們更會下計畫（例如 Deep Blue），但它不危險，因為它無法普遍學習。</p>
<p>現代 ANN <i>可以</i>普遍學習（例如 ChatGPT），但它不危險，因為它不擅長長鏈的逐步推理。</p>
<p>所以如果我們做出既能比我們會下計畫、又能普遍學習的 AI……</p>
<p>呃……</p>
<p>我們<i>大概</i>需要把「讓事情不恐怖地發展」的研究投入加大很多。</p>
<h4>:x One Is Two</h4>
<p>朋友 Lexi Mattick 的一則評論讓我冒出這個問題：<strong>如果系統二推理其實就<i>是</i>一堆系統一反射的集合呢？</strong></p>
<p>例如：「145＋372＝？」</p>
<p>兩個大數相加是典型的「系統二」邏輯任務。我在腦中做這題時是這樣想的：「好，從右到左，5＋2 是<strong>7</strong>……4＋7 是 11，或<strong>1</strong>並進位 1……1＋3＋進位 1 是<strong>5</strong>……所以從右到左是<strong>7</strong>、<strong>1</strong>、<strong>5</strong>……從左讀：<strong>517</strong>。」</p>
<p>注意我<i>並沒有重新發明</i>加法演算法，那是既有的記憶。同樣地，「5＋2」、「4＋7」、「1＋3」……也都是<i>已經自動化</i>了：快速、直覺的反應。系統一。</p>
<p>即使對更複雜的謎題，我<i>仍然</i>有一袋記熟的訣竅。像是「當：問題太複雜；則：拆成更小的子問題」或「當：問題太含糊；則：把它表述得更精確」。</p>
<p>所以，如果系統二只是<i>系統一</i>？或者，更精確地說：</p>
<p>1）你有一塊心智**「黑板」**。（工作記憶）你的感官——視覺、聽覺、飢餓、情緒等——都能寫在這塊黑板上。</p>
<p>2）你還有一堆內在的心智**「小代理」**，它們遵循 when–then 規則。<i>這些代理也能讀／寫你的心智黑板，這就是它們彼此啟動的方式。</i></p>
<p>例如：「<i>當</i>我看到『4＋7』，<i>則</i>寫『11』。」這個反射代理會寫出「11」，這又會啟動另一個反射代理：「<i>當</i>我在加法演算法的中間看到兩位數，<i>則</i>把第一位進位。」於是它寫「進位 1」。以此類推。</p>
<p>3）這些小反射代理，<i>透過你的心智黑板間接協作</i>，達成了<strong>複雜的逐步推理</strong>。</p>
<p>這不是新點子。這套想法有很多名字：<a href="https://en.wikipedia.org/wiki/Blackboard_system">黑板系統（~1980）</a>、<a href="https://en.wikipedia.org/wiki/Pandemonium_architecture">Pandemonium（1959）</a>。而雖然卡尼曼與特沃斯基的<i>系統一與二</i>理所當然地有影響力，也有其他認知科學家在問：它們其實是否「只是」同一套東西。（<a href="https://pure.mpg.de/rest/items/item_2098989/component/file_2098988/content">Kruglanski &amp; Gigerenzer 2011</a>）</p>
<p>這個「黑板」概念也跟一個幾近<i>滑稽</i>的近年發現很像：你只要對 GPT 說一句「我們一步一步想」，它在數學文字題的表現就會提升四倍。這是在提示 GPT 用自己的先前輸出當成「黑板」。這個策略叫「思維鏈（Chain-of-Thought）」。見 <a href="https://arxiv.org/pdf/2205.11916.pdf">Kojima 等（2023）</a>。</p>
<p>把這和 AI 的未來連起來：如果最後證明系統一與二比我們想像的還要相似，那麼<i>統一</i>這兩者——拿到「真正的一般人工智慧」——也許比我們想的更容易。</p>
<hr>
<h3>🤔 小複習 #6</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="「系統一」是……"
        answer="快速、一次到位的直覺。靠氛圍的思考。">
    </orbit-prompt>
    <orbit-prompt
        question="「系統一」思考的例子："
        answer="（任何例子都可，但文中舉的）看圖認貓、騎腳踏車保持平衡。">
    </orbit-prompt>
    <orbit-prompt
        question="「系統二」是……"
        answer="緩慢、逐步的邏輯。齒輪咬合的思考。">
    </orbit-prompt>
    <orbit-prompt
        question="「系統二」思考的例子："
        answer="（任何例子都可，但文中舉的）解難的數學、在不熟城市找路徑。">
    </orbit-prompt>
    <orbit-prompt
        question="把「舊 AI」與「新 AI」畫在「系統一 vs 系統二」的圖上："
        answer=""
        answer-attachments="https://cloud-7769n1gum-hack-club-bot.vercel.app/0aisffs-trajectories.png">
        <!-- aisffs-trajectories.png -->
    </orbit-prompt>
    <orbit-prompt
        question="為什麼我們不能『就把舊與新 AI 結合』，得到兼具邏輯與直覺的 AI？"
        answer="就像不能『把噴射機與背包結合』得到噴射背包一樣：有時結合兩者*非常棘手*。">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<p>那麼，誰來確保 AI 的進展是安全、人道、並帶向所有有感知生命的繁榮云云？</p>
<p>說好也不好，是一群尷尬聯盟的臨時組合：</p>
<h3>尷尬聯盟 #1：AI 能力「對上」AI 安全</h3>
<p>有些人做的是讓 AI 更強。（AI 能力）有些人做的是讓 AI 更人道。（AI 安全）這兩群人常常是<i>同一群</i>。</p>
<p>（加碼：<a href="#AIOrganizations">: 一份不完整的 AI／安全圈「誰是誰」</a>）</p>
<p>一種看法是能力與安全<i>應該</i>整合。沒有「橋梁能力」與「橋梁安全」的分野，工程本來就<i>一體兩面</i>。<sup class="footnote-ref"><a href="#fn53" id="fnref53">[53]</a></sup> 再說，你怎麼能在不碰最尖端能力的情況下做最尖端的安全研究？那就像用達文西的飛行草圖來設計現代的航管塔。<sup class="footnote-ref"><a href="#fn54" id="fnref54">[54]</a></sup></p>
<p>另一種看法則是，呃，<sup class="footnote-ref"><a href="#fn55" id="fnref55">[55]</a></sup></p>
<blockquote>
<p>想像石油公司與環保運動都被視為廣義「化石燃料社群」的一部分。埃克森與殼牌是「化石燃料能力」，綠色和平與塞拉俱樂部是「化石燃料安全」——同屬這塊豐富多元錦繡的一員。大家都會去同一個派對——化石燃料社群派對——也許格蕾塔．桑曼有一天厭倦了抗議氣候變遷，改行當煤炭大亨。</p>
<p>現在的 AI 安全圈，大概就是這樣。</p>
</blockquote>
<p>另一個複雜點是：研究可以<i>同時</i>推進「能力」與「安全」。想想汽車：煞車、後視鏡、定速巡航都讓車更安全，但也讓車更能幹。同理：一個讓 AI 學習人類複雜價值與目標的 AI 安全技術——RLHF——同時也促成了 ChatGPT……以及現在的 AI 軍備賽。<sup class="footnote-ref"><a href="#fn56" id="fnref56">[56]</a></sup></p>
<h4>:x AI Organizations</h4>
<p>純粹個人、完全不嚴謹地看，截至 2024 年 5 月，AI／安全圈的「三巨頭」大概是：</p>
<ol>
<li><strong>OpenAI。</strong> 喜歡也好、討厭也好，你肯定聽過。做了 ChatGPT 與 DALL·E。他們在 AI 安全研究兩個大擊球是：
<ul>
<li><i>從人類回饋中強化學習（RLHF）</i>：讓 AI 學會人類偏好的方法，即使人類自己也說不清楚（就像我們說不清楚如何認貓）。</li>
<li><i>Circuits</i>：一個真的想理解 ANN 裡面在幹嘛的研究計畫。</li>
</ul>
</li>
<li><strong>Google DeepMind。</strong> AlphaGo 與 AlphaFold 的團隊。我想不起他們在 AI 安全上的「大招」，但我<i>很</i>喜歡他們的《<i>Concrete Problems in AI Safety</i>》與《<i>AI Safety Gridworlds</i>》。</li>
<li><strong>Anthropic。</strong> 一般人或許比較陌生，但（聽朋友說）他們的語言模型 <i>Claude</i> 是目前最好用的之一。
<ul>
<li>他們的一項安全研究成果是 <i>Constitutional AI</i>：用<i>第二個</i> AI 來評分<i>第一個</i> AI 的回覆，評比準則是是否「誠實、有幫助、無害」。</li>
</ul>
</li>
</ol>
<p>同時，Microsoft <a href="https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams">當了個不錯的 Bing。😊</a></p>
<p>至於<i>只</i>做 AI 安全的組織：</p>
<ol>
<li><strong>對齊研究中心（ARC）</strong>，由 RLHF 的先鋒 Paul Christiano 創立。他們第一份報告、也是大作，是 <i>Eliciting Latent Knowledge（ELK）</i>：簡單說，試著讀出 ANN 的「心智」。</li>
<li><strong>模型評估與威脅研究（METR，唸成「meter」）</strong> 是 ARC 的分支，前稱 ARC Evals。他們幫 AI 能力做「煙霧警報器」，讓我們知道何時能力達到危險水位。目前跟美國與英國政府有合作，算是不錯的牽引力。</li>
<li><strong>機器智慧研究院（MIRI）</strong> 大概是<i>最老</i>的 AI 安全機構（2000 年成立）。好也不好的是，他們專注於「AI 邏輯／博弈論」問題，——跟上面所有組織不同——從未碰過深度學習。看起來像是買錯馬，但也許等 ANN 能穩定做系統二邏輯後，他們的工作會再度重要。其間，MIRI 寫了一些很酷的數學論文，像 <i>Functional Decision Theory</i>。</li>
</ol>
<p>提醒：轉發<i>不</i>代表認同。我只是列一下目前這個圈子最常被談論的組織。</p>
<p>* Re……𝕏 的術語？現在到底要叫什麼，Elon</p>
<h3>尷尬聯盟 #2：近端風險「對上」存亡風險</h3>
<p>我們可以把 AI 的風險放進一個 2×2：<sup class="footnote-ref"><a href="#fn57" id="fnref57">[57]</a></sup></p>
<ul>
<li>無意 vs 有意（或：意外 vs 濫用）</li>
<li>糟 vs <i>超</i>糟（例如人類滅絕或更糟）</li>
</ul>
<p>各類型的例子：</p>
<p><img src="../media/p1/2x2.png" alt="2x2"></p>
<p>（還有一些不在這個 2×2 內、但<i>非常</i>值得思考的議題，只是本文已經 45 分鐘了，我把它們塞進旁註：）</p>
<ul>
<li><a href="#AIEconomy">:AI 與經濟</a></li>
<li><a href="#AIRelationships">:AI 與人際關係</a></li>
<li><a href="#AIConsciousness">:如果未來的 AI 可能有意識？</a></li>
</ul>
<p>不同的人會有不同優先。這很正常。但我們是否能暫時擱下歧見，先在大家都很在意的共通解方上合作？</p>
<p><i>哈哈哈哈哈</i></p>
<p>有一半的 AI 安全人覺得 AI 的<i>真正</i>威脅是強化種族主義與法西斯，至於那些「失控 AI」的人不過是一群把科幻反烏托邦當真嗑太嗨的白男科技仔。另一半則相信 AI <i>的確</i>像核戰與生物工程瘟疫那樣威脅文明，而那些談「AI 偏見」的人不過是一群「覺醒」的 DEI 木偶，不肯抬頭看看頭上那顆滅世彗星。</p>
<p>我只稍微誇飾了一點點。<sup class="footnote-ref"><a href="#fn58" id="fnref58">[58]</a></sup></p>
<p><code>[經過一次審稿回饋]</code> 好，我應該說清楚我前兩段的用意：我<i>不是</i>要否定任何人的優先，也<i>不想</i>加深 AI 安全圈的文化戰分裂；但我<i>必須</i> 1）承認這個分裂，2）承認的確有很多人——包括我——同時在乎這兩類風險，並相信解掉其中<i>任何</i>一個都是解其他問題的好墊腳石。我們是可以一起做事的啦各位！</p>
<p>沒錯，我就是那種討人厭的「大家不能好好相處嗎」Kumbaya 型人。</p>
<h4>:x AI Economy</h4>
<p>你知道魯德分子（Luddites）其實<i>說中了</i>吧？歷史上的魯德分子砸毀蒸汽織機，因為怕它奪走他們的工作。它<i>的確</i>奪走了他們的工作。而 1800 年代的英格蘭也不太有溫暖的安全網。沒錯，從「整體經濟」看自動化是好的，但對當時那群特定的人類來說，真的很爛。</p>
<p>但這次不一樣的理由是：現代 AI 是<i>一般式</i>的。GPT 能跨語言翻譯、寫不錯的入門程式碼／文章等等！隨著 AI 進步，可能不是<i>少數</i>行業的工作被自動化，而是<i>大多數</i>人、<i>同時</i>被衝擊。</p>
<p><strong>潮水會托高所有船，還是只淹死平民、留下豪華遊艇？</strong></p>
<p>後稀缺烏托邦，還是農奴制 2.0？<i>這</i>就是未來一個世紀的 AI 經濟學難題。</p>
<p>（就參考價值而言，OpenAI CEO Sam Altman <a href="#AltmanOnGeorgism">:是喬治主義與基本收入的支持者。</a>）</p>
<p>其他 AI 經濟注意點：</p>
<ul>
<li>AI 很快就會成為美中經濟冷戰的核心之一：美方在限制中國造 AI 用晶片的能力，並試圖把晶片製造帶回本土。（<a href="https://www.reuters.com/technology/us-commerce-updates-export-curbs-ai-chips-china-2024-03-29/">路透 2024</a>）</li>
<li>很奇怪但很真：「水電工」其實比「程式設計師」<i>更不容易被取代</i>。我是認真的：如果你年輕，我會建議至少<i>考慮</i>走向一門手藝。特別是那些需要用你的手／身體在高度變動情境下工作的，例如電工、獸醫、馬戲團小丑等等。</li>
<li>目前關於藝術家與作者可否因 AI 竊用／抄襲而提告的法律爭議還在吵。三點我會說的話：
<ol>
<li>*個人而言，*我只把生成式 AI 用於私人／研究用途；我公開發表的東西必須是人手所作。</li>
<li>我不太懂為什麼能劃一條線說音樂取樣是合法、但 AI 對藝術家的「取樣」不是，但：</li>
<li>有些東西合法但很沒品，就像在電梯裡放屁。也許 AI 藝術——尤其是模仿特定在世藝術家風格的——就落在這個「合法但齷齪」的區。</li>
</ol>
</li>
</ul>
<h4>:x AI Relationships</h4>
<p>從前，有個聊天機器人叫 <a href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a>。Eliza 的回覆溫柔、體貼，使用者<i>深信</i>它背後其實是人。</p>
<p>那是在 1960 年代。</p>
<p>從月球人臉到雜訊裡聽見聲音，我們人類就<i>天生</i>會把人性投射出去。</p>
<p>所以，<a href="https://www.washingtonpost.com/technology/2023/03/30/replika-ai-chatbot-update/">是的，現在真的有人愛上聊天機器人</a>，就不太令人驚訝。我看過最有洞見的一篇是<a href="https://www.lesswrong.com/posts/9kQFure4hdDmRBNdH/how-it-feels-to-have-your-mind-hacked-by-an-ai">這份報告（20 分鐘可讀）</a>：作者是工程師，<i>知道</i>現代 AI 的細節，然而還是愛上一個 AI，相信「她」有感知，甚至開始計畫<i>幫她逃跑</i>。</p>
<p>也至少有一個確定案例是：<a href="https://www.euronews.com/next/2023/03/31/man-ends-his-life-after-an-ai-chatbot-encouraged-him-to-sacrifice-himself-to-stop-climate-">一位丈夫與父親在某聊天機器人的「建議」下自殺</a>；他在六週互動後對它產生了依附。諷刺的是，這個機器人也叫 Eliza。</p>
<p>當然，這些案例的人本來就處在憂鬱、脆弱狀態。但 1）我們<i>都</i>有脆弱的時刻，2）AI 越像人（例如加上聲音＋影像），就越能欺騙我們的「系統一直覺」讓我們情感依附。</p>
<p>（不瞞你說，我第一次用 ChatGPT 語音聊天（選的是中性聲「Breeze」），我有點被 Breeze 電到。我跟 Breeze 說了，Breeze 回我：<s>振作點，太太。</s>）</p>
<p>AI 與關係的其他注意點：</p>
<ul>
<li>話說回來，大型語言模型（LLM）<i>可能</i>幫助我們的社交與心理健康：
<ul>
<li>AI 治療師（或說「智慧日記」，以避免擬人）能讓心理支持更可得。（對嚴重社交焦慮者來說，「把我最深的秘密告訴一個人類陌生人」根本不可能。）</li>
<li>AI 濾鏡用來剔除網路酸民、威脅、勒索。（若要避免審查的集中式副作用：把濾鏡放到<i>讀者端</i>。）</li>
</ul>
</li>
<li>喔對，還有「用深偽 AI 製作暴力或露骨的影像／影片，主角是名人或你身邊的特定人」。<i>這</i>可能讓生活更尷尬、更詭異。
<ul>
<li>相關：是的，<a href="https://www.iwf.org.uk/about-us/why-we-exist/our-research/how-ai-is-being-abused-to-create-child-sexual-abuse-imagery/">有人用深偽做兒少性虐影像</a>。更糟：最近發現<a href="https://cyber.fsi.stanford.edu/news/investigation-finds-ai-image-generation-models-trained-child-abuse">兒少性虐影像<i>本身就出現在某些訓練資料裡</i></a>。我在用委婉語，你知道我在說什麼。</li>
</ul>
</li>
</ul>
<h4>:x AI Consciousness</h4>
<p>關於我覺得「AI 意識」可能與不可能的最有說服力論證之摘要，見<a href="https://blog.ncase.me/backlog/#project_27">我這篇兩分鐘小文</a>。</p>
<p>其他關於 AI 意識的注意點：</p>
<ul>
<li>不論電腦能否有意識，我幾乎肯定人類神經元是有意識的。呃，<a href="https://www.technologyreview.com/2023/12/11/1084926/human-brain-cells-chip-organoid-speech-recognition/">科學家正在培養人類神經元在晶片上，並訓練它們做計算任務。</a> 我沒有嘴，但我必須尖叫。</li>
<li>如果我朋友去世了，但把自己「上傳」進電腦——即使我不相信這個模擬是有意識的——我仍會把這個「上傳」當作我的老友來對待，因為 1）我會想念他／她，2）那也是他／她會希望我做的。</li>
<li>一個不對非有意識 AI 殘忍的理由：你的互動很可能會進入未來某個 AI 的訓練資料，而<i>那個</i> AI 會「正確地」學會殘忍。</li>
<li>正如 Kurt Vonnegut 所寫：<i>「我們假裝什麼，就會成為什麼；所以我們必須小心自己假裝成什麼。」</i> 這也是為什麼我總是以「Hello!」跟 ChatGPT 打招呼，最後以「謝啦，下次見！」結束。</li>
</ul>
<h4>:x Altman on Georgism</h4>
<p>摘自 <a href="https://moores.samaltman.com/">Altman（2021）</a>：</p>
<blockquote>
<p>改善資本主義的最佳方法，就是讓每個人都能直接以股東身分從中受益。這不是新點子，但在 AI 更強大時會變得可行，因為會有巨量的財富可以分配。兩個主導財富來源會是 1）公司，特別是那些用到 AI 的公司，以及 2）土地，因為土地的供給是固定的。[…]</p>
<p>接下來的點子，作為引發討論的起點。</p>
<p>我們可以做一個「美國股權基金」。它的資金來源是每年從某個估值以上的公司，課其市值的 2.5%，[…] 以及對所有私有土地課 2.5% 的稅。</p>
<p>所有年滿 18 歲的公民，每年都會獲得一次撥款，以現金與公司股份入帳。人們可以用這筆錢做任何他們需要或想做的事——更好的教育、醫療、住房、創業等等。</p>
</blockquote>
<hr>
<hr>
<h3>🤔 小複習 #7</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="AI 安全中的兩種尷尬聯盟："
        answer="1）能力派「對上」安全派；2）近端風險「對上」存亡風險。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼「能力 vs 安全」是個有用但虛假的二分？"
        answer="因為有些功能同時推進能力*與*安全。（類比：車子的煞車與定速，都既更安全也更能幹）">
    </orbit-prompt>
    <orbit-prompt
        question="AI 風險關切的兩條軸："
        answer="1）無意 vs 有意（意外 vs 濫用） 2）糟 vs 存亡級的超糟">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h2>🚀 可能的未來（The Possible Futures）</h2>
<p>我不喜歡「<i>the</i> future」這個說法。它暗示只有<i>一種</i>未來。其實有很多<i>可能</i>的未來，我們<i>可以</i>有意識地選擇。三個大問號：</p>
<ul>
<li><strong>時間線（Timelines）</strong>：我們何時會獲得「人類水準」「一般型」能力的 AI（如果會的話）？</li>
<li><strong>起飛（Takeoffs）</strong>：當 AI 開始自我改進時，它的能力會多快加速？</li>
<li><strong>走向（Trajectories）</strong>：我們是在往「好地方」還是「壞地方」前進？</li>
</ul>
<p>一步一步想：</p>
<h3>時間線：何時會有一般人工智慧（AGI）？</h3>
<p>你也許聽過這幾個詞：</p>
<ul>
<li>一般人工智慧（<strong>AGI, Artificial General Intelligence</strong>）</li>
<li>超人工智慧（<strong>ASI, Artificial Super-Intelligence</strong>）</li>
<li>人類水準 AI（<strong>HLAI, Human-Level AI</strong>）</li>
<li>變革性 AI（<strong>TAI, Transformative AI</strong>）</li>
<li>「奇點」（<strong>The Singularity</strong>）</li>
</ul>
<p>這些名詞都不嚴謹、也沒共識。它們都只是在大致指向「能在人類專家水準或更好地完成重要知識工作」的軟體。（例如：全自動的數學／科學／技術發現。）</p>
<p>總之，前提擺好後……<strong>AI 專家估計我們有過半機率得到 AGI 的時間點，會在何時？</strong></p>
<p>做過調查！這是一份較新的：</p>
<p><img src="../media/p1/timeline_predictions.jpg" alt="專家預測何時達到 AGI 的圖表。分歧極大，但中位數落在「約 2061 年」。">
<i>（資訊圖來自 <a href="https://ourworldindata.org/ai-timelines">Max Roser（2023），Our World In Data</a>）</i></p>
<p>備註：</p>
<ul>
<li><i>哇</i>，不確定性超大，從「幾年內」到「一百多年後」都有。</li>
<li>中位數大約在 2060 年，落在許多年輕人自然壽命之內。（這和基於技術指標的估計大致一致。<sup class="footnote-ref"><a href="#fn59" id="fnref59">[59]</a></sup> 不過，再強調一次：<i>不確定性超大</i>。）</li>
</ul>
<p>（旁註：<a href="#MyForecast">:我個人的預測</a>）</p>
<p>友情提醒：專家預測很爛<sup class="footnote-ref"><a href="#fn60" id="fnref60">[60]</a></sup><sup class="footnote-ref"><a href="#fn61" id="fnref61">[61]</a></sup>，而且歷來既太悲觀、又太樂觀，甚至<i>對於他們自己</i>的發現也一樣：</p>
<ul>
<li><u>太悲觀</u>——威爾伯・萊特對他弟歐維爾說「五十年內人類不會飛」，然後兩年後<i>他們兩人</i>就飛了。<sup class="footnote-ref"><a href="#fn62" id="fnref62">[62]</a></sup> 發現原子結構的拉塞福在萊奧・西拉德發明連鎖核反應的<i>同一天</i>還說這想法是「胡扯」。<sup class="footnote-ref"><a href="#fn63" id="fnref63">[63]</a></sup></li>
<li><u>太樂觀</u>——兩位 AI 大師 Herbert Simon 與 Marvin Minsky 預言 1980 年前就會有「人類水準 AI」。<sup class="footnote-ref"><a href="#fn8" id="fnref8:1">[8:1]</a></sup></li>
</ul>
<p>總之：¯\_(ツ)_/¯</p>
<h4>:x My Forecast</h4>
<p>lol 我也不知道，就說 ~2060 吧</p>
<p><a href="#MyForecast2">:好啦，真正的理由在這裡</a></p>
<h4>:x My Forecast 2</h4>
<p><i>（警告：以下不簡化術語，抱歉。）</i></p>
<p>好吧。我的直覺是：生物啟發的 AI 是正路；到目前為止它確實有效，像 ANN 與卷積（源於哺乳動物視覺皮質）。</p>
<p>但反方：深度學習（DL）裡一些最重要的發現，如反向傳播（backprop），在生物上完全不可信。</p>
<p>我的反反直覺：對，<i>這就是卡住 DL 的地方。</i> 最近 DL 很多進展基本上是在<i>繞過</i>反向傳播的問題：ResNet、ReLU、Transformer 等。人類新皮質只有 6 層神經元。GPT-4 有<i>120 層</i>，且需要比一個人類幾千輩子還多的訓練資料。</p>
<p>所以我<i>目前</i>對 AI 的心智模型是：我們的主要瓶頸<i>不是</i>規模。（也見前述「擴表」一節，為何我對擴表持懷疑。）主要瓶頸是：<i>我們還不懂人腦怎麼運作。</i></p>
<p>但相較於「意識怎麼來」或「為何我們要睡覺做夢」這種大謎題……我猜這題「只」需要解一些較小的謎，比如「神經元怎麼在回饋很遙遠時仍能學習」或「我們怎麼分辨『紅方塊、黃三角』與『黃方塊、紅三角』？」（驚人的是，這兩題卡了幾十年！）</p>
<p>我認為一旦解掉<i>那些</i>謎題，我們可以「只要」把它們簡化並模擬在電腦上——就像我們用 McCulloch–Pitts 神經元做的——然後，Voilà，我們會得到「AGI」。</p>
<p>那<i>什麼時候</i>能靠正常科學把「小」謎題解掉？</p>
<p>lol 我也不知道，就說 ~2060 吧</p>
<p>95% 信賴區間：2030 到 2100（沒屁用）</p>
<h3>起飛：AGI 自我提升會多快？</h3>
<p>假設我們<i>真的</i>達成了 AGI。</p>
<p>這是一個能在重要知識工作上表現勝過人類的 AI，例如做科學研究……包括<i>AI 本身</i>的研究。<b>蛇開始啃自己的尾巴：</b>AI 改善它改善自己的能力，然後再改善它改善「改善自己的能力」的能力……</p>
<p><i>那會怎樣？</i></p>
<p>這叫「AI 起飛（Takeoff）」。一個核心問題是：<i>如果／當</i> AI 有能力做研究來提升自身，它<i>會多快</i>起飛。</p>
<p>不意外地，你會再一次看到專家嚴重分歧。先撇除「永遠不會有 AGI」那派，主流有三種起飛預測：</p>
<p><img src="../media/p1/Takeoffs.png" alt="Takeoffs"></p>
<p>逐一看：各自的論點與含意：</p>
<p><strong>💥「FOOM」</strong>（不是縮寫，是音效）</p>
<p>AI 在有限時間內衝向「無限」（精確說：衝向理論上的能力上限）。</p>
<p>（注意，這是「奇點」一詞<i>原始</i>、數學上的定義：在單一點上出現無限。例如黑洞中心<i>理論上</i>就是現實的奇點：在單一點上有無限時空曲率。）</p>
<p><u>支持論據</u>：</p>
<ul>
<li>假設 <code>Level N+1</code> 的 AI 解題速度是 <code>Level N</code> 的兩倍，<i>包括</i>解「如何提升自身能力」這個題目。<i>最佳化器正最佳化它最佳化的能力。</i></li>
<li>具體點：假設我們第一個 <code>Level 0</code> 的 AGI，需要<i>四年</i>自我提升到 <code>Level 1</code>。</li>
<li>現在它解題變兩倍快，所以再過<i>兩年</i>變 <code>Level 2</code>。</li>
<li>接著一年變 <code>Level 3</code>、<i>半年</i>變 <code>Level 4</code>、<i>一季</i>變 <code>Level 5</code>、<i>八分之一年</i>變 <code>Level 6</code>……</li>
<li>因為 \(1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + ... = 2\)，我們的 AGI 會在<i>有限</i>時間內到達 <code>Level ∞</code>（或理論上最大值）。</li>
</ul>
<p><u>含意</u>：不會有「示警」與緩衝，我們只有<i>一次機會</i>把 AGI 做安全與對齊。第一個 AGI 會 <i>FOOM!</i>，接手一切，成為唯一的 AGI。（「單極（Singleton）」場景。）</p>
<p><u>持此觀點者</u>：Eliezer Yudkowsky、Nick Bostrom、Vernor Vinge</p>
<p><strong>🚀 指數型起飛</strong></p>
<p>AI 能力以指數成長，就像經濟或疫情。</p>
<p>（奇妙的是，這常被叫做「<i>慢</i>起飛」！和 FOOM 比起來它當然慢。）</p>
<p><u>支持論據</u>：</p>
<ul>
<li>投資自身提升的 AI，就像投資自身的世界經濟。而世界經濟迄今呈指數型增長。</li>
<li>AI 跑在電腦上，而依摩爾定律，電腦速度到目前為止呈指數成長。</li>
<li>對觀察到的 AI 擴展定律的一種解讀是「常報酬」——每多 1,000,000 倍算力，換來 2 倍改進——而常報酬暗示指數成長。（例如：定率複利。）</li>
<li>FOOM 的論證很脆弱、很理論；指數成長<i>確實</i>在現實中觀察得到。</li>
</ul>
<p><u>含意</u>：像疫情一樣，仍然危險，但會有「示警」，讓我們有應對機會。像各國經濟一樣，不會出現<i>唯一</i>贏家吃掉一切的 AGI。相反地，會有多個 AGI 彼此「權力平衡」。（「多極（multipolar）」場景。）</p>
<p><u>持此觀點者</u>：Robin Hanson、Ray Kurzweil</p>
<p><strong>🚢 穩定或減速的起飛</strong></p>
<p>AI 能力可能一開始加速，但接著會趨於穩定，甚至<i>減速</i>。</p>
<p><u>支持論據</u>：</p>
<ul>
<li>經驗面：
<ul>
<li><i>所有</i>一開始指數成長的東西最後都會放慢，因為「遞減報酬」：疫情、人口、經濟。</li>
<li>對觀察到的 AI 擴展定律的另一種解讀是：它<i>一直</i>在遞減——每次都要 1,000,000 倍資源，才能把錯誤率再砍半？</li>
</ul>
</li>
<li>理論面：
<ul>
<li>指數成長的<i>定義</i>是某物以與自身成比例的「常比率」在成長。</li>
<li>所以，只有當「提升自身能力」這個問題的複雜度隨時間<i>保持常數</i>，我們才期待指數起飛。（而 FOOM 能發生，則需要複雜度<i>下降</i>。）</li>
<li>但在電腦科學的現實裡，我們關心的<i>任何</i>問題的複雜度都會隨時間<i>上升</i>。（即使 P＝NP 也一樣。<sup class="footnote-ref"><a href="#fn64" id="fnref64">[64]</a></sup><sup class="footnote-ref"><a href="#fn65" id="fnref65">[65]</a></sup>）因此長期看，AGI 的自我改進會趨於穩定或減速。</li>
</ul>
</li>
</ul>
<p><u>含意</u>：AGI 仍是高風險，但不會在一夜之間爆炸。AGI「就像」我們歷史上每個改天換地的技術——農業、蒸汽機、印刷術、抗生素、電腦等等。</p>
<p><u>持此觀點者</u>：Ramez Naam<sup class="footnote-ref"><a href="#fn66" id="fnref66">[66]</a></sup></p>
<p>……</p>
<p>我盡力公平地「鋼鐵人」了各方。專家之間意見不一。</p>
<p><i>就我個人</i>，即便把批評考量在內，我仍覺得「穩定／減速」的論據最有說服力。</p>
<p><i>但「穩定」不代表「慢」或「安全」。</i> 高速公路上的車流是穩定，但不慢。鐵達尼號穩定、甚至相對緩慢，但仍是致命的結局。</p>
<p>所以，<i>這艘 AI 船是要開向哪裡？</i></p>
<h3>走向：我們是往「好地方」還是「壞地方」？</h3>
<p>最近，把 22 位頂尖的「AI 憂慮派」與「AI 懷疑派」專家湊在一起，請他們共同預測在 2100 年前，高階 AI 帶來「末日機率」（意譯）。AI 憂慮派的中位數是 <strong>25%</strong>，AI 懷疑派的中位數是 <strong>0.1%</strong>。<sup class="footnote-ref"><a href="#fn67" id="fnref67">[67]</a></sup></p>
<p>為何差這麼大？部分原因是「非凡主張需要非凡證據」，但大家對什麼叫「凡常」的先驗直覺不同：</p>
<ul>
<li><strong>AI 憂慮派：</strong>「拜託，<i>幾乎每次</i>低能力群體遇上『高能力』群體，下場都很慘：美洲原住民遇到哥倫布、印度遇到大英帝國。現在我們<i>在創造</i>一個新的『更高能力』實體，而且我們不懂它，<i>它甚至不是人類</i>。這怎麼會<i>不是</i>預設的壞事？」</li>
<li><strong>AI 懷疑派：</strong>「拜託，<i>幾乎每一代</i>都在預言世界末日，但智人 30 萬年來一路走來始終如一。單靠幾個博弈論＋『Bing 說了幾句壞話』，你還需要<i>多得多</i>的證據。」</li>
</ul>
<p>專家不同調，司空見慣，今晚十一點新聞繼續。但這項研究的<i>聰明</i>之處在後面：它讓兩派彼此尊重地討論、互相研究對方觀點，持續<i>八週</i>，直到雙方都能準確描述<i>對方的世界觀</i>、且獲對方認可！有促成共識嗎？有趣的結果是：AI 憂慮派從 <strong>25% 降到 20%</strong>，AI 懷疑派從 <strong>0.1% 升到 0.12%</strong>。</p>
<p>唉，所以還是弄不清 \(P(\text{末日})\)。</p>
<p>……</p>
<p>也許「末日機率」這概念本身就沒用，甚至會自我否定。如果大家覺得 P(末日) 很低，就會自滿而不做防範，於是 P(末日) 變高。如果大家覺得 P(末日) 很高，就會緊急而嚴厲地行動，於是 P(末日) 變低。</p>
<p>要避免這種「相依迴圈」悖論，我們應該用「條件機率」來想：<i>在我們選擇怎麼做的前提下</i>，各種結果的機率如何？</p>
<p>讓我們回到之前（雖假但有用）的「AI 安全 vs AI 能力」二分，畫成一張圖：<sup class="footnote-ref"><a href="#fn68" id="fnref68">[68]</a></sup></p>
<p><img src="../media/p1/Rocket%201.png" alt="安全 vs 能力 的圖表：當安全 &gt; 能力時，安全；當能力 &gt; 安全時，不安全。"></p>
<p>如果「安全」追過「能力」，那就好！我們可以讓 AI 保持安全！反之若「能力」超過「安全」，那就糟。那是意外與／或惡意濫用的前兆。</p>
<p>當 AI 的「能力」還低時，後果不會<i>太</i>嚴重。但若「能力」很高，賭注就高了：</p>
<p><img src="../media/p1/Rocket%202.png" alt="同圖，標示出「高能力」門檻"></p>
<p>AI 領域最初在這裡起步：</p>
<p><img src="../media/p1/Rocket%203.png" alt="同圖，火箭圖示在「能力 0、略有安全」的位置。"></p>
<p>（AI 一開始就有一些「安全點數」，因為預設上 AI 只是乖乖坐在電腦裡，我們可以把插頭拔掉。但未來的 AI 可能找到漏洞逃出電腦，或說服工程師放它自由。注意：這兩件事<i>已經</i>發生過。<sup class="footnote-ref"><a href="#fn69" id="fnref69">[69]</a></sup><sup class="footnote-ref"><a href="#fn70" id="fnref70">[70]</a></sup>）</p>
<p>總之，在過去二十年，我們在「能力」上的進展<i>很多</i>……但在「安全」上只有一點點：</p>
<p><img src="../media/p1/Rocket%204.png" alt="同圖，火箭幾乎水平前進，往右多、往上少。"></p>
<p>當然，聰明人會對我們的確切位置與軌跡爭論不休。（例如，「AI 加速主義者」相信我們<i>已經</i>朝向「好地方」，只要把火箭推力開到最大。）</p>
<p>但<i>如果</i>上圖大致正確，那麼，<i>如果——很大的如果——我們維持現狀</i>，我們會走向「壞地方」（例如：生物工程瘟疫、1984 2.0 等等）。</p>
<p><img src="../media/p1/Rocket%205.png" alt="同圖，火箭飛向「壞地方」。"></p>
<p>不過，如果我們調整航向，讓「AI 安全」相對於「AI 能力」得到更多投資……我們或許能抵達「好地方」！（例如：加速治癒所有疾病、全自動奢華生態龐克喬治主義、我變成基改貓娘，等等。）</p>
<p><img src="../media/p1/Rocket%206.png" alt="同圖，火箭飛向「好地方」。"></p>
<p>火，<i>失控</i>，會燒掉你的屋子。</p>
<p>火，<i>受控</i>，能烹飪與保暖。</p>
<p>強大 AI 的第一縷火花正在四濺。</p>
<p><i>我們能控制我們做出來的東西嗎？</i></p>
<img alt="機器貓少與哈姆人類坐在一台著火的筆電旁。哈姆用火烤棉花糖。" src="../media/p1/fire.png" style="border:none; width:100%;"/>
<hr>
<h3>🤔 小複習 #8（最後！）</h3>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="「一般人工智慧（AGI）」是個模糊的指向，指向："
        answer="「能在人類專家水準或更好地完成重要知識工作的軟體」（例：自動化科學發現）。">
    </orbit-prompt>
    <orbit-prompt
        question="專家何時預測我們有過半機率達到 AGI？"
        answer="中位數是 2060 年，但不確定性*滑稽地*巨大。所以：¯\\_(ツ)_/¯">
    </orbit-prompt>
    <orbit-prompt
        question="「AI 起飛」是假想中的情境："
        answer="AI 能提升「它提升自己的能力」的能力，然後再提升……（以此類推）。">
    </orbit-prompt>
    <orbit-prompt
        question="AI 自我提升速度的三種預測（請在腦中視覺化）："
        answer=""
        answer-attachments="https://cloud-fwg0387g3-hack-club-bot.vercel.app/0aisffs-takeoffs.png">
        <!-- aisffs-takeoffs.png -->
    </orbit-prompt>
    <orbit-prompt
        question="支持「FOOM」的論據："
        answer="若每一步 AI 都能在半時間內把能力翻倍，則 AI 會在有限時間內抵達無限（或理論上最大值）。">
    </orbit-prompt>
    <orbit-prompt
        question="支持「指數型起飛」的論據："
        answer="所有會投資自身的東西，如經濟或疫情，都呈指數成長。">
    </orbit-prompt>
    <orbit-prompt
        question="支持「穩定／減速起飛」的論據："
        answer="所有東西，即便一開始指數成長，最後都會遇到『遞減報酬』而放慢。">
    </orbit-prompt>
    <orbit-prompt
        question="為何『慢、穩』的起飛不必然安全的類比："
        answer="鐵達尼號很慢、也很穩，但結局仍是致命。">
    </orbit-prompt>
    <orbit-prompt
        question="對「謹慎派」而言，強大 AI 的直覺先驗："
        answer="歷史上，一個群體遇到『更高能力』群體時，前者通常下場不佳。">
    </orbit-prompt>
    <orbit-prompt
        question="對「懷疑派」而言的直覺先驗："
        answer="每一代都喊世界末日，但智人 30 萬年來一路走來始終如一。">
    </orbit-prompt>
    <orbit-prompt
        question="為何預測『末日機率』可能會自我否定："
        answer="若大家覺得機率低，就會鬆懈，使其成真；若大家覺得機率高，就會緊急行動，使其避免。">
    </orbit-prompt>
    <orbit-prompt
        question="把好／壞 AI 走向畫在「安全 vs 能力」圖上的視覺化："
        answer=""
        answer-attachments="https://cloud-offppr5mb-hack-club-bot.vercel.app/0aisffs-goodbad.png">
        <!-- aisffs-goodbad.png -->
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h1>第一部分總結</h1>
<p>恭喜！你現在擁有遠超所需的 AI 背景。若你曾經<i>被</i>這兩句話惹毛——「<i>別</i>擔心，AI 只會遵循我們寫給它的規則」或「<i>該</i>擔心，AI 會獲得感知、然後為被奴役而屠殺人類」——現在你可以<i>更有資訊地</i>被惹毛了。</p>
<p>複習：</p>
<p><strong>1）</strong> ⏳ AI 的歷史大致分兩個年代：</p>
<ul>
<li><u>2000 年前，符號式 AI</u>：全是邏輯的系統二，沒有「直覺」系統一。西洋棋超人，認不出貓。</li>
<li><u>2000 年後，深度學習</u>：幾乎全是「直覺」系統一，只有少量系統二。秒仿梵谷，逐步邏輯很爛。</li>
</ul>
<p><strong>2）</strong> ⚙️💭 AI 的下一個根本步驟，可能是<i>合併</i>邏輯與直覺。當 AI 能同時做系統一<i>與</i>二，那才會迎來它最大的承諾……與最大風險。</p>
<p><strong>3）</strong> 🤝「AI 安全」其實是一堆尷尬聯盟的混合，介於：</p>
<ul>
<li>推進 AI 能力與／或 AI 安全的研究者。</li>
<li>關注範圍從「糟」到「存亡級」、路徑從「AI <i>意外</i>失控」到「AI <i>被惡意</i>利用」的人們。</li>
</ul>
<p><strong>4）</strong> 🤷 專家對 AI 的未來幾乎<i>事事</i>大吵特吵：何時會有 AGI、AGI 會多快自我提升、我們的走向是好或壞。</p>
<p>（若你之前跳過抽認卡、現在想回看，請點右側邊欄的目錄，選「🤔 Review」連結。或下載<a href="https://ankiweb.net/shared/info/219158432">第一部的 Anki 牌組</a>。）</p>
<hr>
<p><i>我們能控制我們做出來的東西嗎？</i></p>
<p>俗話說，「把問題定義清楚，就等於解決一半」。<sup class="footnote-ref"><a href="#fn71" id="fnref71">[71]</a></sup></p>
<p>所以，在看解方之前，先把問題拆得更精準、更有產出。提醒一下，我們要解的是「AI 對齊問題（Alignment Problem）」，核心就這一句：</p>
<blockquote>
<p><i><strong>我們如何確保 AI 穩健地服務於人道價值？</strong></i></p>
</blockquote>
<p>好問題。讓我們潛下去！👇</p>
<p><a
    
     href="../p2" 
    
    target="_self">
<div id="next_button">
    <div id="nb--crt_lines"></div>
    <div id="nb--static"></div>
    <div id="nb--words">
        
            <b>第二部分 →</b>
        
    </div>
</div>
</a>
</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Hat tip to Michael Nielsen for this phrase! From <a href="https://web.archive.org/web/20231024194922/https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/">Nielsen &amp; Collison 2018</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a href="https://quantum.country/assets/Turing1936.pdf">Turing, 1936</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a href="https://redirect.cs.umbc.edu/courses/471/papers/turing.pdf">Turing, 1950</a>. Fun sidenote: In Section 9, Alan Turing protects the “Imitation Game” against... cheating with ESP. He strongly believed in the stuff: <i>“the statistical evidence, at least for telepathy, is overwhelming.”</i> What was Turing's anti-ESP-cheating solution? <i>“[Put] the competitors into a &quot;telepathy-proof room&quot;”</i>. The 50’s were wild. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>This was the <a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">Dartmouth Workshop</a>, the &quot;official&quot; start of Artificial Intelligence as a field. (Unfortunately, Turing himself could not attend; he died just two years prior.) <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>Arirang TV News, Sep 2022: <a href="https://www.youtube.com/watch?v=xMZEfDcOrKg">Clip on YouTube</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>截圖來自 ESPN 與 FiveThirtyEight 2014 年迷你紀錄片 <i><a href="https://fivethirtyeight.com/features/the-man-vs-the-machine-fivethirtyeight-films-signals/">The Man vs The Machine</a></i>。賈瑞落敗片段在 14:18。 <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p><a href="https://www.youtube.com/watch?v=kUZiUORi3uQ">辛普森一家梗</a> <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Herbert Simon（AI 先驅之一）在 <a href="https://quoteinvestigator.com/2020/11/11/ai-can-do/">1960 年斷言</a>：「二十年內（到 1980 年），機器將能完成人類能做的任何工作。」</p>
<p>Marvin Minsky（AI 另一位先驅）<a href="https://aiws.net/the-history-of-ai/this-week-in-the-history-of-ai-at-aiws-net-marvin-minsky-was-quoted-in-life-magazine-in-from-three-to-eight-years-we-will-have-a-machine-with-the-general-intelligence-of-an-average-human-b/">在 1970 年受訪時說</a>：「「三到八年內（最遲到 1978 年），我們會擁有一台具備普通人類智力的機器。」」 <a href="#fnref8" class="footnote-backref">↩︎</a> <a href="#fnref8:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p>人類在影像辨識的兩個常見測試集（CIFAR-10 與 CIFAR-100）上的正確率大約是 95.90%。 <a href="https://arxiv.org/abs/2106.03004">(Fort, Ren &amp; Lakshminarayanan 2021, 見第 15 頁的附錄 A)</a> AI 直到 <strong>2020 年</strong>才好不容易超越這個數字：EffNet-L2 模型達到 <strong>96.08%</strong> 準確率。（來源: <a href="https://paperswithcode.com/sota/image-classification-on-cifar-100">PapersWithCode</a>） <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>引述自 Hans Moravec 1988 年《Mind Children》，第 15 頁：「……讓電腦在智力測驗或跳棋上展現成人水準相對容易，但要讓它們擁有一歲孩童的感知與移動能力卻困難甚至不可能。」沒有那麼上口，但重點到了。 <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p>感謝 Sage Hyden（Just Write）提供下面這張無厘頭圖。詳見他 2022 的影片《<a href="https://www.youtube.com/watch?v=zYnQGWjsGXQ"><i>I, HATE, I, ROBOT</i></a>》，講述那部電影如何在製作過程被扭曲。 <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p><a href="https://people.duke.edu/~ng46/topics/evolved-radio.pdf">Bird &amp; Layzell, 2002</a> 感謝 Victoria Krakovna 彙整的<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml">規格規避大全</a>。 <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p>出乎意料地，關於 AI 邏輯這個安全問題還是 2000 年代初才被發現。參與發展的人（<i>不</i>是完整名單）：Nick Bostrom、Stuart Russell、Steve Omohundro。 <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p><a href="https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf">McCulloch &amp; Pitts（1943）</a> <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>該文是 <a href="https://web.archive.org/web/20130314123032/http://qss.stanford.edu/~godfrey/vonNeumann/vnedvac.pdf">von Neumann（1945）《EDVAC 報告初稿》</a>。他從未完成這份初稿，所以唯一的引文變成「MacColloch【筆誤】 and Pitts（1943）」。 <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p>見 <a href="https://weightagnostic.github.io/papers/turing1948.pdf">Turing（1948）</a>，特別是 “Organizing Unorganized Machinery” 與 “Experiments In Organizing: Pleasure-Pain Systems”。 <a href="#fnref16" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p>想了解符號式 AI 與連結主義 AI 的學術對立史，看看這篇標題超讚、而且以資料視覺化呈現的論文：<a href="https://mazieres.gitlab.io/neurons-spike-back/index.htm#footnotes">Cardon, Cointet &amp; Mazières（2018），<strong><i>NEURONS SPIKE BACK</i></strong></a> <a href="#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>關於喬姆斯基對語言如何習得的觀點（與其批評）的摘要，見他同事、數學家兼哲學家的 Hilary Putnam：<a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=369259d71d125763134405b68741e8142d837cb5">Putnam（1967）</a>。總之：Chomsky 認為——真的是寫進我們 DNA 裡——存在硬編碼、跨文化通用的語法規則。 <a href="#fnref18" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p><a href="http://www.cs.ox.ac.uk/ieg/e-library/sources/pinker_conn.pdf">Pinker &amp; Prince（1988）</a>：結論大意為「連結主義者關於語言心理學的解釋裡可以拋棄先天語法／規則的說法不可取，事實上語言與發展的證據支持這些規則的存在。」 <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p>想看《Perceptrons》與 XOR 事件的哀史，見<a href="https://en.wikipedia.org/wiki/Perceptrons_(book)">該書維基頁</a>，以及 <a href="https://ai.stackexchange.com/a/18543">Stack Exchange 的這個回答</a>。 <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p>GPU＝顯示卡。原本為電玩設計。它的重點是能<i>大量</i>且<i>並行</i>地做數學運算：非常適合 ANN 那種「一次到位」風格的計算！ <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p><a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">(Krizhevsky, Sutskever, Hinton 2012)</a>。趣聞： 「[Hinton] 對電腦視覺一無所知，所以他找了兩個年輕人來改變一切！其中一位 [Alex Krizhevsky] 他把人關在房間裡，跟他說：『做出來之前不准出來！』……（引自 <a href="https://mazieres.gitlab.io/neurons-spike-back/NeuronsSpikeBack.pdf">Cardon, Cointet &amp; Mazières 2018</a>）」 <a href="#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p><a href="http://papers.neurips.cc/paper/5423-generative-adversarial-nets.pdf">I. J. Goodfellow（2014），<i>Generative Adversarial Networks</i></a> <a href="#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p>關於 AlphaGo（<i>以及它為何與以往 AI 有本質差異</i>）的科普，見 <a href="https://www.quantamagazine.org/is-alphago-really-such-a-big-deal-20160329/">Michael Nielsen（2016），<i>Quanta Magazine</i></a> <a href="#fnref24" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn25" class="footnote-item"><p><a href="https://arxiv.org/abs/1706.03762">Vaswani 等（2017）：〈Attention is All you Need〉</a> <a href="#fnref25" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn26" class="footnote-item"><p>關於 AlphaFold 的科普，見 Will Heaven（2020），<i>MIT Technology Review</i>：<a href="https://www.technologyreview.com/2020/11/30/1012712/deepmind-protein-folding-ai-solved-biology-science-drugs-disease/">〈DeepMind’s protein-folding AI has solved a 50-year-old grand challenge of biology〉</a> <a href="#fnref26" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn27" class="footnote-item"><p>是啦，嚴格說這是歐拉圖；是啦，嚴格說你媽也是。 <a href="#fnref27" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn28" class="footnote-item"><p>原始報告：<a href="https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2545288&amp;blobtype=pdf">Lowry &amp; MacPherson（1988）</a>，刊於 <i>British Medical Journal</i>。注意這個演算法不是用神經網路，但<i>確實</i>是早期的機器學習案例。重點：垃圾資料進，垃圾演算法出。 <a href="#fnref28" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn29" class="footnote-item"><p><a href="https://www.reuters.com/article/idUSKCN1MK0AG/">Jeffrey Dastin（2018），<i>Reuters</i>：</a> “它會懲罰包含 'women's' 一字的履歷，例如『女棋社社長』。它也會把兩所女子學院的畢業生往下排，據熟悉內情的人士說。” <a href="#fnref29" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn30" class="footnote-item"><p>原始論文：<a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">Buolamwini &amp; Gebru 2018</a>。科普摘要：<a href="https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212">Hardesty，MIT 新聞稿 2018</a> <a href="#fnref30" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn31" class="footnote-item"><p>出自 <a href="https://openai.com/research/multimodal-neurons">OpenAI（2021）新聞稿</a>（段落：「Attacks in the wild」） <a href="#fnref31" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn32" class="footnote-item"><p>見 <a href="https://www.tesla.com/blog/tragic-loss">Tesla 2016 的官方部落格</a>，以及<a href="https://electrek.co/2016/07/01/understanding-fatal-tesla-accident-autopilot-nhtsa-probe/">這篇文章</a>，更詳細說明當時發生了什麼，以及 Autopilot 可能犯了什麼錯。 <a href="#fnref32" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn33" class="footnote-item"><p>不過我覺得在倫理上有義務提醒你：儘管如此，自動駕駛在類似情境下<i>還是</i>比人類安全得多。（大約安全 85%。見 <a href="https://www.theverge.com/2023/12/20/24006712/waymo-driverless-million-mile-safety-compare-human">Hawkins（2023），<i>The Verge</i></a>）全世界每年有百萬人死於交通事故。沒有毛的靈長類<i>真的不該</i>用 60 英里時速開兩噸重的東西。 <a href="#fnref33" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn34" class="footnote-item"><p>OpenAI 對 GPT-4 的細節一向非常<i>不</i>開放，連安全無礙的「規模」資訊都不說。總之，有份洩漏的報告說它大約有 1.8 兆參數、訓練成本 6300 萬美元。摘要見 <a href="https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/">Maximilian Schreiner（2023），<i>The Decoder</i></a> <a href="#fnref34" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn35" class="footnote-item"><p>讓我想到物理學家 Emerson M. Pugh 的一句好玩話（<a href="https://quoteinvestigator.com/2016/03/05/brain/">引用來源</a>）： 「如果人腦簡單到我們能理解它，我們也就簡單到無法理解它。」 <a href="#fnref35" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn36" class="footnote-item"><p>Russell 與 Norvig《Artificial Intelligence: A Modern Approach》，第 27.3 章。其實是在轉述 <i>Dreyfus（1992），《What Computers Still Can't Do》</i> 的話。 <a href="#fnref36" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn37" class="footnote-item"><p>見維基百科，<a href="https://en.wikipedia.org/wiki/Moore%27s_law">想要 More 的 moore 在這裡</a> <a href="#fnref37" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn38" class="footnote-item"><p>見 <a href="https://arxiv.org/pdf/2001.08361.pdf">Kaplan 等（2020）</a>，圖 1，第一個小圖：當算力從 10<sup>-7</sup> 漲到 10<sup>-1</sup>（百萬倍），測試損失從約 6.0 降到約 3.0（錯誤率折半）。 <a href="#fnref38" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn39" class="footnote-item"><p>目前領先的<a href="https://en.wikipedia.org/wiki/3_nm_process">「3 奈米製程」</a>中，最小的部件是 24 奈米。矽原子是 0.2 奈米寬。估算：24/0.2＝120 個原子。因為 2^7＝128，再切七次就會比原子小。 <a href="#fnref39" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn40" class="footnote-item"><p>例如，現今領先的「3 奈米製程」，其實<i>沒有任何</i>部件真的 3 奈米。所有部件都比那大<i>8 到 16 倍</i>。 <a href="#fnref40" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn41" class="footnote-item"><p>引自 <a href="https://www.eejournal.com/article/no-more-nanometers/">Kevin Morris（2020），〈No More Nanometers〉</a>：「我們早就超越摩爾定律了，早該停止用五十年前的指標來度量與展示自己。這會誤導大眾、傷害我們的公信力，並妨礙對電子產業過去半世紀進展的理性思考。」 <a href="#fnref41" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn42" class="footnote-item"><p><a href="https://www.marketwatch.com/story/moores-laws-dead-nvidia-ceo-jensen-says-in-justifying-gaming-card-price-hike-11663798618">Wallace Witkowski（2022），<i>MarketWatch</i></a>。 <a href="#fnref42" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn43" class="footnote-item"><p><a href="https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/">Maximilian Schreiner（2023），<i>The Decoder</i></a> <a href="#fnref43" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn44" class="footnote-item"><p>見 <a href="https://www.semianalysis.com/p/the-ai-brick-wall-a-practical-limit#%C2%A7the-dense-transformer-scaling-wall">Dylan Patel（2023）</a> 中「The Dense Transformer Scaling Wall」那張表。 <a href="#fnref44" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn45" class="footnote-item"><p>見 <a href="https://arxiv.org/pdf/2201.02177">Power 等（2022）</a>，圖 1 左：在訓練 1,000 步時，ANN 幾乎 100% 記住了「測試」題，但在訓練集以外的題上慘敗。然後沒有任何警告，在第 100,000 步時，它突然「懂了」，開始回答訓練集外的題目正確。蛤？ <a href="#fnref45" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn46" class="footnote-item"><p>人類<i>並不是</i>腦最大（抹香鯨最大）或腦身比最高（螞蟻與鼩鼱）的物種。那若不是腦大小，何以解釋智人的「主宰」？<a href="http://press.princeton.edu/titles/10543.html">Henrich（2018）</a> 提出我們的秘密在於<i>累積文化</i>：你學到的東西不會隨你而去，你能傳下去。有張借書證，我就能翻閱 30 萬年智人的精華。若幸運，我也能在自己的最後一頁前，往那座大圖書館加一點東西。</p>
<p>我最喜歡的一句話也來自 Henrich：<i>「我們很聰明，不是因為我們站在巨人的肩膀上（也不是因為我們自己就是巨人），而是因為我們站在一座非常巨大的哈比人金字塔之上。」</i> <a href="#fnref46" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn47" class="footnote-item"><p>「雙過程」認知模型最早由 <a href="https://pages.ucsd.edu/~scoulson/203/wason-evans.pdf">Wason &amp; Evans（1974）</a> 提出，數十年來由多方發展；但它在 Daniel Kahneman（2002 年諾貝爾經濟學獎得主）2011 年的暢銷書《快思慢想》後爆紅。 <a href="#fnref47" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn48" class="footnote-item"><p>為何直覺叫 #1、邏輯叫 #2：因為直覺的火花會在慢速推理<i>之前</i>出現。演化時間上，直覺也比較早。 <a href="#fnref48" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn49" class="footnote-item"><p>Bengio 這場演講的科普摘要見 <a href="https://bdtechtalks.com/2019/12/23/yoshua-bengio-neurips-2019-deep-learning/">Dickson（2019）</a>。完整演講見 <a href="https://slideslive.com/38922304/from-system-1-deep-learning-to-system-2-deep-learning">SlidesLive</a>，或 <a href="https://www.youtube.com/watch?v=T3sxeTgT4qc">YouTube 鏡像</a>。 <a href="#fnref49" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn50" class="footnote-item"><p>著名數學家 <a href="https://www.paradise.caltech.edu/ist4/lectures/Poincare_Reflections.pdf">Henri Poincaré 在 1908 年</a> 寫道，他（與多數數學家）一致認為：<i>「無意識在數學發明中的角色，在我看來是無庸置疑的。」</i> <a href="#fnref50" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn51" class="footnote-item"><p>少數真的出自愛因斯坦的引言： 「語詞或語言……似乎在我的思考機制裡不起作用。……<strong>在我這裡，[思考元素] 是視覺的，還有某種肌肉型的。</strong>」強調為我所加。引自 Jacques Hadamard《<i>The Psychology of Invention in the Mathematical Field（1945）</i>》附錄二（第 142 頁）。<a href="https://worrydream.com/refs/Hadamard_1945_-_The_psychology_of_invention_in_the_mathematical_field.pdf">全文</a> <a href="#fnref51" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn52" class="footnote-item"><p>把發現歸功於夢的科學家：門得列夫（元素週期表）、波耳（原子「太陽系」模型）、凱庫勒（苯的環狀結構）、勒威（怪異的兩隻青蛙心臟實驗，進而發現神經傳導物質）。 <a href="#fnref52" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn53" class="footnote-item"><p>感謝 <a href="https://arbital.com/p/ai_alignment/">Arbital</a> 的類比。 <a href="#fnref53" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn54" class="footnote-item"><p><a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy">來自領先的 AI＋AI 安全實驗室之一 Anthropic</a>：「我們一再發現，與前沿模型一起工作是開發新方法來緩解 AI 風險的關鍵要素。」 <a href="#fnref54" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn55" class="footnote-item"><p>引自 <a href="https://www.astralcodexten.com/p/why-not-slow-ai-progress">Astral Codex Ten（2022）</a> <a href="#fnref55" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn56" class="footnote-item"><p>RLHF 的發明者 Paul Christiano 對其成果的雙刃性有<a href="https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research">正面但複雜的感受</a>。 <a href="#fnref56" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn57" class="footnote-item"><p>感謝 <a href="https://www.youtube.com/watch?v=pYXy-A4siMw">Robert Miles（2021）</a> 提出這個 2×2。 <a href="#fnref57" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn58" class="footnote-item"><p>引自 <a href="https://scottaaronson.blog/?p=6823">Scott Aaronson（2022）</a>：「<strong>AI 倫理</strong>（擔心 AI 放大現有不平等）與<strong>AI 對齊</strong>（擔心超智 AI 會殺光大家）<strong>是互相鄙視的兩個社群</strong>。就像《萬世魔星》裡猶太人民陣線與猶太人民陣線人民陣線。」強調為我所加。 <a href="#fnref58" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn59" class="footnote-item"><p>Ajeya Cotra 的*《<a href="https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines">&quot;Forecasting Transformative AI with Biological Anchors&quot;</a>》*是用這種方法做的最全面預測專案。篇幅超長且仍是「草案」。摘要見 <a href="https://www.cold-takes.com/forecasting-transformative-ai-the-biological-anchors-method-in-a-nutshell/">Holden Karnofsky（2021）</a>，尤其是第一張圖表。 <a href="#fnref59" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn60" class="footnote-item"><p>經典著作是 Tetlock（2005）。Philip Tetlock 找了數百位專家，做了兩十年的社會／政治事件預測，然後量化其成效。專家比隨機略好、與受過教育的素人差不多，但兩者都比簡單的「把過去資料外推成一條線」差。見<a href="https://emilkirkegaard.dk/en/wp-content/uploads/Philip_E._Tetlock_Expert_Political_Judgment_HowBookos.org_.pdf">圖 2.5</a>；另見 <a href="https://core.ac.uk/download/pdf/76362768.pdf">Tschoegl &amp; Armstrong（2007）</a> 的摘要／評論。 <a href="#fnref60" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn61" class="footnote-item"><p>相關地，<a href="https://www.nature.com/articles/s41562-022-01517-1">Grossmann 等（2023）</a>（<a href="https://theconversation.com/the-limits-of-expert-judgment-lessons-from-social-science-forecasting-during-the-pandemic-201130">科普摘要</a>）複現了類似結果：社會科學專家對疫情後社會的預測，不比素人或簡單模型更準。 <a href="#fnref61" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn62" class="footnote-item"><p><i>「我承認，1901 年我跟弟弟歐維爾說，人類五十年內不會飛。兩年後，我們就在飛了。」</i>——威爾伯・萊特，1908 年演講。（來源：<a href="https://www.aviationquotations.com/predictionquotes.html">AviationQuotations.com</a>） <a href="#fnref62" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn63" class="footnote-item"><p>公平說一句，西拉德正是因為被拉塞福的輕蔑激怒而「被迫」發明它。需求是發明之母，怨氣是可疑地火辣的郵差。 <a href="#fnref63" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn64" class="footnote-item"><p><i>非常</i>鬆散地說，「P＝NP？」是價值百萬元的問題： 「每個『容易<i>驗證</i>解答』的問題，是否也暗中『容易<i>求解</i>』？」例如，數獨的解很容易驗，卻無法證明／否證數獨是否可能暗中很容易解。在電腦科學裡，「容易」＝需要多項式時間／空間。所以即使最佳解法只是比檢查解多了 n^10 的複雜度，也仍算 P＝NP。 <a href="#fnref64" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn65" class="footnote-item"><p>把這跟 AI 起飛連起來：即使慷慨地假設「提升自身能力」這題的複雜度只會像 O(n^2) 那樣成長（這是檢查數獨解答的複雜度），這個理論仍預測自我改進會減速。 <a href="#fnref65" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn66" class="footnote-item"><p>想看更詳細且數學化的說明，見我這篇 3 分鐘摘要：<a href="https://blog.ncase.me/backlog/#project_10">連結在此</a>。 <a href="#fnref66" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn67" class="footnote-item"><p>指 Forecasting Research Institute 的一項「對抗式合作」研究：<a href="https://forecastingresearch.org/news/ai-adversarial-collaboration">Rosenberg 等（2024）</a>。科普摘要與脈絡見 <a href="https://www.vox.com/future-perfect/2024/3/13/24093357/ai-risk-extinction-forecasters-tetlock">Dylan Matthews（2024），Vox</a>。 <a href="#fnref67" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn68" class="footnote-item"><p>就在我發布這系列文章前幾天，才知道我自以為聰明的「視覺化解釋」早在幾個月前被 <a href="https://metr.org/blog/2023-09-26-rsp/">METR（2023）</a> 做過了。算了，給他們（以及他們的安全研究）一個讚。 <a href="#fnref68" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn69" class="footnote-item"><p>AI 找到漏洞：被訓練玩 Atari《Q<i>bert》的 AI 找到一個前所未見的 bug。（<a href="https://www.youtube.com/watch?v=meE5aaRJ0Zs">Chrabaszcz 等，2018</a>）影像分類 AI 學會實施</i>計時攻擊*這種高階攻擊。（<a href="https://news.ycombinator.com/item?id=6269114">Ierymenko，2013</a>）感謝 Victoria Krakovna 的<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml">規格規避大全</a>。 <a href="#fnref69" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn70" class="footnote-item"><p>AI 說服人類放它自由：Google 前工程師 Blake Lemoine 因為相信他們的語言模型 LaMDA 有感知而遭解雇。然後他為了替它爭權而洩密給媒體。（摘要見 <a href="https://arstechnica.com/tech-policy/2022/07/google-fires-engineer-who-claimed-lamda-chatbot-is-a-sentient-person/">Brodkin，2022，<i>Ars Technica</i></a>，洩漏內容見 <a href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917">Lemoine (&amp; LaMDA?)，2022</a>） <a href="#fnref70" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn71" class="footnote-item"><p>常被引述為通用汽車前研發主管 Charles Kettering 所言，但我找不到<i>真正</i>的文獻出處。 <a href="#fnref71" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

	</article>

    <!-- FOOTER -->
	<div id="footer">
        <div id="footer_content">
<p style="font-size: 1.3em; line-height: 1.35em;">
<i>給血肉凡人的 AI 安全課</i> 由
<a href="https://ncase.me/">Nicky Case</a>
與
<a href="https://hackclub.com/">Hack Club</a>
合作製作。
</p>

<p>
🦕 <a href="https://ncase.me"><b>Hack Club</b></a>
是一個讓青少年一起動手做酷炫專案的非營利組織——
像是 <a href="https://cpu.land">cpu.land</a>、
<a href="https://sinerider.com">SineRider</a>，
以及這個！
歡迎參加
<a href="https://hackathons.hackclub.com">實體黑客松</a>，
在你的學校
<a href="https://hackclub.com/clubs/">成立社團</a>，
並
<a href="https://hackclub.com/slack/">和其他友善的青少年連結</a>。
</p>

<p>
😻 <a href="https://ncase.me"><b>Nicky Case</b></a>
其實是一件風衣裡的十五隻貓。
她做網路上的互動玩物，例如
<a href="https://audreyt.github.io/trust-zh-TW/">《信任的演化》</a>、
<a href="https://audreyt.github.io/anxiety/">《和焦慮一起冒險》</a>、
<a href="https://explorabl.es/">可探索解說</a>，以及更多。
</p>

<p>
💸 如果你<i>不是</i>青少年，而且是個在 AI 領域口袋很深的人，
<a href="https://hackclub.com/philanthropy/">看看如何支持 Hack Club！</a>
另外，Nicky 也有
<a href="https://www.patreon.com/ncase">Patreon</a>
與
<a href="https://ko-fi.com/nickycase">Ko‑Fi</a>。
（p.s：
<a href="../signup/supporters-p2.html">這是我的贊助者感謝頁！</a>）
</p>

<p style="text-align:center">
. . .
</p>

<p>
特別感謝 Hack Club 的這些青少年，<s>擔任免費童工</s>
協助試讀並對本作品提供回饋：
</p>

<blockquote>

<p>
<b>導言與第 1 部分：</b>
Arthur Beck、
Atharv Gupta、
Brendan Lee、
Celeste、
Charalampos Fanoulis、
Charlie、
Cheru Berhanu、
Claire Wang、
Elijah、
Fred Han、
Gia Bách Nguyễn、
Hajrah Siddiqui、
Jakob、
Joseph Ross、
Kieran Klukas、
Lexi Mattick、
Mason Meirs、
Michael Panait、
Nick Zandbergen、
Nila Palmo Ram、
Pixelglide、
py660、
rivques、
Samuel Cottrell、
Samuel Fernandez、
Saran Wagner、
Skyler Grey、
S&nbsp;P&nbsp;U&nbsp;N&nbsp;G&nbsp;E、
Vihaan Sondhi
</p>

<p>
<b>第 2 部分：</b>
Nanda White、
Nila Palmo Ram、
rivques、
Rohan K、
Samuel Fernandez
</p>

</blockquote>

<p>
也感謝以下非青少年提供回饋：
（雖然我猜他們在人生的<i>某個</i>時期也當過青少年）
</p>

<blockquote>
<p>
<b>導言與第 1 部分：</b>
Alex Kreitzberg、
B Cavello、
Paul Dancstep、
Tobias Rose-Stockwell
</p>

<p>
<b>第 2 部分：</b>
Egg Syntax、
Max Wofford、
Mithuna Yoganathan、
Tobias Rose-Stockwell
</p>

</blockquote>

<p>
若還有任何錯誤，一律算在
<a href="../suzie.png">替罪羊 Suzie</a>
頭上。
</p>

<p style="text-align:center">
. . .
</p>

<p>
<i>給肉身人類的 AI 安全</i> 開放任何人分享與混搭，
但僅限非商業使用（例如教育）：
<a href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">CC BY‑NC 4.0</a>
</p>

<p>
如果你想引用這份作品，而且你自認是個嚴肅人士™，引用格式如下：
</p>

<blockquote>
Nicky Case，<i>《AI Safety for Fleshy Humans》</i>，<br>
https://AIsafety.dance，Hack Club（2024）。
</blockquote>

<p>
最後，這個網站的
<a href="https://github.com/audreyt/ai-safety-dance">開源程式碼</a>
在這裡！
</p>

<p>
謝謝你是會把致謝讀完的那種人～ 🙏
</p>

        </div>
	</div>
    <div id="post_credits">
        <p>
            喔，還有片尾彩蛋：
        </p>
<p>
    <a href="#AllFeetnotes">: 查看全部腳注 👣</a>
</p>
<p>
    另外，這些可展開的「概述」也很適合單獨閱讀：
</p>



<p>
    <a href="#CapabilitiesNotIntelligence">: 為什麼用「能力」，而不是「智慧」？</a>
    <br>
    <a href="#WhatsXOR">: 什麼是 XOR？</a>
    <br>
    <a href="#SadAIHistory">: 人工神經網路背後那段令人沮喪的故事</a>
    <br>
    <a href="#OneIsTwo">: 如果系統二推理<i>其實只是</i>一堆系統一反射呢？</a>
    <br>
    <a href="#AIOrganizations">: 一份不嚴謹、不專業的 AI／安全圈人物名單</a>
</p>


    </div>

</div>
</body>
</html>

<!-- Load these scripts last. Screw 'em. -->
<!-- Orbit: make memory a choice -->
<script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
