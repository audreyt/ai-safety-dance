<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>

    <!-- Title -->
    <title>給血肉凡人的 AI 安全課</title>

    <!-- UTF-8 & Mobile -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">

    <!-- Links are external by default -->
    <base target="_blank">

	<!-- Favicon -->
	<link rel="icon" type="image/png" href="favicon.png">

    <!-- Social Share Nonsense -->
	<meta itemprop="name" content="給血肉凡人的 AI 安全課">
	<meta itemprop="description" content="一站式了解 AI 和 AI 安全的所有核心思想！">
	<meta itemprop="image" content="https://aisafety.dance/thumbs/thumb.png">
	<meta property="og:title" content="給血肉凡人的 AI 安全課">
	<meta property="og:type" content="website">
	<meta property="og:image" content="https://aisafety.dance/thumbs/thumb.png">
	<meta property="og:description" content="一站式了解 AI 和 AI 安全的所有核心思想！">
    <meta name="twitter:card" content="summary_large_image">
	<meta name="twitter:title" content="給血肉凡人的 AI 安全課">
	<meta name="twitter:description" content="一站式了解 AI 和 AI 安全的所有核心思想！">
	<meta name="twitter:image" content="https://aisafety.dance/thumbs/thumb.png">

	<!-- STYLES -->
	<link rel="stylesheet" href="styles/Merriweather/merriweather.css">
    <link rel="stylesheet" href="styles/Open_Sans/opensans.css">
    <link rel="stylesheet" href="styles/littlefoot.css"/> <!-- before page.css, so page can override it -->
	<link rel="stylesheet" href="styles/page.css">
	<link rel="stylesheet" href="styles/han.min.css">

	<!-- SCRIPTS -->
    <!-- Littlefoot: for my feetnotes -->
    <script src="scripts/littlefoot.js" ></script>
    <!-- Nutshell: expandable explanations -->
    <script src="scripts/nutshell-v1.0.5.js"></script>
    <script> Nutshell.setOptions({ lang: 'zh-TW', startOnLoad: false, /* Start AFTER footnotes loaded */ }); </script>
    <!-- MathJAX: for nice math -->
    <script src="scripts/tex-mml-chtml.js"></script>
	<!-- This website's own scripts -->
    <script src="scripts/page.js"></script>
    <!-- Hack Club's no-cookies, GDPR-compliant analytics -->
    <script defer data-domain="aisafety.dance" src="https://plausible.io/js/script.js"></script>

</head>
<body>

<!-- HACKBRAND -->
<a class="orpheus-flag" target="_blank" href="https://hackclub.com/">
	<img src="styles/orpheus-flag.svg" width="560" height="315" alt="Hack Club 的專案" aria-label="Hack Club 的專案">
</a>

<!-- The Sidebar UI -->
<div id="return_to_content"></div>
<div id="sidebar">
	<div id="panel_toc"></div>

    <!-- STYLE CHANGER -->
	<div id="panel_style">

        <div id="style_dark_mode_container" style="cursor:pointer;">
            <input type="checkbox" id="style_dark_mode" style="pointer-events: none;">
            深色模式
        </div>
        <br>

        字級：
        <span id="style_fontsize"></span>
        <br>
        <input type="range" id="style_fontsize_slider" min="10" value="19" max="40">
        <br>

        字型：
        <br>
        <label>
            <input type="radio" name="style_font_family" value="serif" checked>
            <span style="font-family:'Merriweather'">襯線</span>
        </label>
        <br>
        <label>
            <input type="radio" name="style_font_family" value="sans_serif">
            <span style="font-family:'Open Sans'">無襯線</span>
        </label>
        <br><br>

        <button id="style_reset">重設</button>

    </div>

    <!-- TRANSLATIONS -->
	<div id="panel_translations">
        <!-- none... sorry -->
    </div>
	<div id="panel_share">分享至⋯隨便啦</div>
    <!-- SHILLING FOR BIG NICKY -->
	<div id="panel_sub">
    </div>
    <div id="panel_support"></div>

</div>

<!-- Reading Time Clock! -->
<div id="reading_time">
	<div id="clock_icon"></div>
	<div id="clock_label"></div>
</div>

<!-- EVERYTHING TO THE LEFT of the sidebar... -->
<div id="everything_container">

    <!-- A big cute header -->
    <div id="header" class="frontpage">
        <div id="splash_image">

            
            

            <div id="crt_lines"></div>
            <div id="static"></div>

            
            <img id="dancing" width="400" src="media/splash/noone.png"/>
            

        </div>
        

            <div id="header_words">
                <div id="title">
                    給血肉凡人的 AI 安全課
                </div>
                <div id="subtitle">
                    作者：
                    <a href="https://ncase.me">Nicky Case</a>
                    與
                    <a href="https://hackclub.com/">Hack Club</a>
                </div>
            </div>

        
	</div>

    <!-- Chapter Navigation -->
    <div id="chapter_nav">
        <div id="chapter_nav_centered">
            <a target="_self" href=""
                class="live">
                <div selected>
                    <span class='chapter-nav-desktop'>
                        導言
                    </span>
                    <span class='chapter-nav-phone'>
                        導言
                    </span>
                </div>
            </a>
            <a target="_self" href="p1"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        第 1 部分<br>過去與未來
                    </span>
                    <span class='chapter-nav-phone'>
                        第 1 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="p2"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        第 2 部分<br>問題
                    </span>
                    <span class='chapter-nav-phone'>
                        第 2 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="p3"
                class="live">
                <div  >
                    <span class='chapter-nav-desktop'>
                        第 3 部分<br>解方？
                    </span>
                    <span class='chapter-nav-phone'>
                        第 3 部分
                    </span>
                </div>
            </a>
            <a target="_self" href="#"
                title="預計 2024 年 12 月中上線（大概）"
                onclick="alert('預計 2024 年 12 月中上線（大概）')">
                <div style="border-right:1px solid rgba(128,128,128,0.8);">
                    <span class='chapter-nav-desktop'>
                        結語
                    </span>
                    <span class='chapter-nav-phone'>
                        結語
                    </span>
                </div>
            </a>
        </div>
    </div>

    <!-- The lil' tabs for sidebar UI -->
    <div id="sidebar_tabs">
		<div id="tab_toc">
			<div></div>
			內容導覽
		</div>
		<div id="tab_style">
			<div></div>
			變更樣式 😎
		</div>
        <!--
		<div id="tab_sub">
            CREDITS & Signup for notifications
			<div></div>
			subscribe 💖
		</div>
        -->
	</div>

    <!-- BEHOLD! CONTENT!!!!! -->
	<article id="content">
<p><strong>關於 AI 的爭論，其實是「一百場爭論疊在同一件風衣裡」。</strong></p>
<p>人工智慧（AI）會幫助我們治癒一切疾病，打造一個後匱乏、人人得以繁榮的世界嗎？還是 AI 會幫助暴君進一步監控與操縱我們？AI 的主要風險，來自意外、壞人濫用，還是失控的 AI 本身就成了壞人？這一切只是炒作嗎？為什麼 AI 能在一分鐘內模仿任何藝術家的風格，卻在畫超過三個物件時就困惑？為什麼很難讓 AI 穩健地服務於「人道」（humane）的價值，甚至是穩健地服務於「任何」目標？如果 AI 變得比我們更有人道精神怎麼辦？如果 AI 學到的是人類的「不人道」，也就是偏見與殘酷，又會如何？我們要走向烏托邦、反烏托邦、滅絕、比滅絕更糟的結局，還是——最令人震驚的——什麼都不變？另外：AI 會搶走我的工作嗎？</p>
<p>……還有更多更多問題。</p>
<p>可惜的是，要細膩地理解 AI，就得理解大量技術細節……然而這些細節散落在上百篇文章中，被滿坑滿谷的術語深埋。</p>
<p>因此，我向你呈上：</p>
<p><img src="media/intro/confetti.png" alt="RCM（機器人貓耳男僕）在寫有「A Whirlwood Tour Guide to AI Safety for Us Warm, Normal Fleshy Humans」標語的橫幅下拋灑彩帶。"></p>
<p><strong>這套三部曲將一次帶你掌握 AI 與 AI 安全* 的核心觀念 —— 以親切、易懂、略帶個人觀點的方式呈現！</strong></p>
<p>（* 相關用語：AI 風險、AI 存在風險（X-Risk）、AI 對齊（Alignment）、AI 倫理、AI-不要-把-大家-都-幹掉-主義。這些詞的確切涵義並<em>沒有</em>共識，所以本文統稱為「AI 安全」。）</p>
<p>本系列還有由機器人貓耳男僕（Robot Catboy Maid）主演的漫畫。像這樣：</p>
<p><img src="media/intro/Outer_Alignment.png" alt="漫畫。人類 Ham 對 RCM（機器人貓耳男僕）說：「保持這棟房子乾淨。」RCM 推理：是什麼造成髒亂？人類造成髒亂！因此：把人類趕出去。RCM 接著把 Ham 整個丟 出房子。"></p>
<p><code>[導遊語氣]</code> 請看您的右手邊 👉，有 <img src="media/intro/icon1.png" class="inline-icon"/> 目錄按鈕、<img src="media/intro/icon2.png" class="inline-icon"/> 網頁樣式切換，以及 <img src="media/intro/icon3.png" class="inline-icon"/> 剩餘閱讀時間時鐘。</p>
<p>關於本系列：導言與第一部分發表於 <strong>2024 年 5 月</strong>，第二部分已於 <strong>2024 年 8 月</strong> 上線，第三部分預計 <strong>2024 年 12 月</strong> 推出。可選：如果想在發佈時收到通知，請於下方登記！👇 我<em>不會</em>寄送其他內容騷擾你，只有兩封通知信。（不過呢，<code>[Podcast 贊助商語氣]</code> 如果你是高中生或更小、且對 AI／程式／工程有興趣，可以勾選選項以了解更多 <a href="https://hackclub.com/">Hack Club</a>。附註：有免費的<em>貼紙～～～</em> ✨）</p>
<p>總之，<code>[導遊語氣回歸]</code> 在踏上 AI 與 AI 安全這段岩石嶙峋的徒步旅程前，讓我們先用「一萬英尺視角」俯瞰全景：</p>
<hr>
<h2>💡 AI 與 AI 安全的核心觀念</h2>
<p>在我看來，AI 與 AI 安全的主要問題可歸結為<strong>兩大核心衝突：</strong></p>
<p><img src="media/intro/Core%20Problems.png" alt="邏輯「對上」直覺，以及問題出在 AI「對上」出在人類"></p>
<p>從我在「對上」周圍加上這些引號就能看出，這些分界其實沒有那麼分明……</p>
<p>以下是這兩大衝突如何在三部曲中反覆出現：</p>
<h3>第 1 部分：過去、現在，以及可能的未來</h3>
<p>省略<em>大量</em>細節不談，AI 的歷史其實就是一則「<em>邏輯對上直覺</em>」的故事：</p>
<p><strong>2000 年之前：AI 幾乎全是邏輯，幾乎沒有直覺。</strong></p>
<p>這就是為什麼 1997 年時，AI 能在西洋棋上打敗世界冠軍……卻沒有任何 AI 能可靠地在圖片中辨識出貓。<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup></p>
<p>（安全顧慮：沒有直覺，AI 無法理解常識或人道價值。因此，AI 可能以邏輯上正確但不理想的方式達成目標。）</p>
<p><strong>2000 年之後：AI 能做出「直覺」了，但邏輯很差。</strong></p>
<p>這就是為什麼生成式 AI（<em>以本文撰寫時的 2024 年 5 月為準</em>）能用任何藝術家的風格夢製整片景致……<a href="#FourObjects">:但在畫超過 3 個物件時就會困惑</a>。<em>（👈 點這段文字！它也會展開！）</em></p>
<p>（安全顧慮：沒有邏輯，我們無法驗證 AI 的「直覺」裡發生了什麼。那份直覺可能有偏誤、可能在細微之處產生危險的錯誤，或在新情境下以怪異方式失靈。）</p>
<p><strong>直到今日：我們<em>仍然</em>不知道如何在 AI 中統合邏輯與直覺。</strong></p>
<p>但若我們做得到，那<em>將</em>帶來 AI 最大的風險與最大的新契機：一個既能在邏輯上勝過我們的規劃，<em>又</em>能學到普遍化直覺的系統。那會是「愛因斯坦級 AI」……或是「歐本海默級 AI」。</p>
<p>一圖總結：</p>
<p><img src="media/intro/Timeline.png" alt="AI 時間線。2000 年之前，多為「邏輯」。2000 年至今，多為「直覺」。未來，也許兩者兼具？"></p>
<p>以上是「邏輯對上直覺」。至於另一個核心衝突——「問題出在 AI 還是出在人類」——這正是 AI 安全領域的一大爭議：我們的主要風險，是來自先進 AI <em>本身</em>，還是來自<em>人類</em>對先進 AI 的濫用？</p>
<p>（為什麼不能兩者皆是？）</p>
<h3>第 2 部分：問題本質</h3>
<p>AI 安全<em>最核心</em>的問題是這個：<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<blockquote>
<p><u><strong>價值對齊問題（Value Alignment Problem）</strong></u>：<br>
「我們如何讓 AI 穩健地服務於人道（humane）的價值？」</p>
</blockquote>
<p>注意：我用的是 <em>humane</em>（「人道」），而不單是 <em>human</em>（「人類」）。一個<em>人類</em>未必<em>人道</em>。我要反覆強調這點，因為支持與批評 AI 安全的人<em>都</em>常把兩者混為一談。<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup><sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></p>
<p>我們可以依照「問題出在人類 vs 出在 AI」來拆解這個問題：</p>
<blockquote>
<p><u><strong>人道的價值</strong>：</u><br>
「究竟什麼是『人道的價值』？」<br>
（留給哲學與倫理學的問題）</p>
</blockquote>
<blockquote>
<p><u><strong>「技術性」對齊問題</strong>：</u><br>
「我們究竟要如何讓 AI 穩健地服務於<em>任何預期的目標</em>？」<br>
（電腦科學家的問題——出乎意料地，仍未解決！）</p>
</blockquote>
<p>而「技術性」對齊問題，又可依「邏輯 vs 直覺」進一步拆解：</p>
<blockquote>
<p><u>AI 邏輯面的問題</u>：<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>（「賽局理論」類問題）</p>
<ul>
<li>AI 可能以邏輯正確、但不理想的方式達成目標。</li>
<li>大多數目標在邏輯上都會導向相同且不安全的子目標：「不要讓任何人阻止我達成目標」、「最大化我的能力與資源以最佳化該目標」等。</li>
</ul>
</blockquote>
<blockquote>
<p><u>AI 直覺面的問題</u>：<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>（「深度學習」類問題）</p>
<ul>
<li>以人類資料訓練的 AI 可能學到我們的偏見。</li>
<li>AI 的「直覺」不可理解、不可驗證。</li>
<li>AI 的「直覺」脆弱，會在新情境下失靈。</li>
<li>AI 的「直覺」可能<em>部分</em>失靈，這也許更糟：當 <em>技能</em> 完好但 <em>目標</em> 損壞時，AI 會以高超技能朝著扭曲的目標前進。</li>
</ul>
</blockquote>
<p>（再說一次，何謂「邏輯」與「直覺」稍後會更精確地說明！）</p>
<p>一圖總結：</p>
<p><img src="media/intro/Breakdown.png" alt="拆解 AI 對齊問題的圖：從「如何讓 AI 對齊人道價值？」分為「技術性對齊」與「人道價值」。技術性對齊再分為「AI 邏輯（賽局理論）」與「AI 直覺（深度學習）」。"></p>
<p>要直覺感受這些問題有多難，先注意：我們連在<em>人類自身</em>都還沒解決——人們常遵循法律的字面而非精神。人們的直覺會有偏誤，會在新情境下失效。我們任何人都不是 100% 達到自己期許的那種「人道的人」。</p>
<p>所以，容我小小感性一下，也許理解 AI 會幫助我們理解自己。也許我們能解決<em>人類</em>的對齊問題：我們如何讓<em>人類</em>穩健地服務於人道的價值？</p>
<h3>第 3 部分：提議的解方</h3>
<p>最後，我們可以來理解一些（可能）解決「邏輯、直覺、AI、以及人類」問題的方法！其中包括：</p>
<ul>
<li>技術面解方</li>
<li>政策／治理面解方</li>
<li>「不然你就先關掉、別去打造那個酷刑樞紐吧」</li>
</ul>
<p>（可惜的是，我無法在本導言中給出面向大眾的易懂總結，因為在你理解問題之前——也就是第一與第二部分要做的事——這些解方並不會有太大意義。話雖如此，如果你想先看重點劇透，<a href="#Part3Details">:點此查看第三部分將涵蓋的內容！</a>）</p>
<hr>
<h2>🤔 （<em>選填</em>）抽認卡複習！</h2>
<p>你是否也有過這種感覺？</p>
<ol>
<li>「哇，我剛讀到的內容真是精彩又有洞見」</li>
<li>（兩週後全忘光）</li>
<li>「糟了」</li>
</ol>
<p>為了避免在讀完<em>本</em>指南後也發生這種事，我放入了幾張「<em>選填</em>」互動抽認卡！它們採用「間隔重複（Spaced Repetition）」——一種相對簡單、且有實證支持的方法，讓「長期記憶」成為一種可選擇的結果。（<a href="#SpacedRepetition">:點此了解更多關於間隔重複！</a>）</p>
<p>來試試看吧：<strong>用下面的抽認卡，幫助你保留剛學到的重點！</strong></p>
<p>（最後有個選填的註冊選項，<em>如果</em>你想把這些卡片存起來做長期複習。注意：<em>我並不擁有也不控制這個服務</em>，它是第三方的。如果你偏好使用開源的抽認卡軟體 <a href="https://apps.ankiweb.net/index.html">Anki</a>，<strong>這裡有<a href="https://ankiweb.net/shared/info/341999410">可下載的 Anki 卡包</a></strong>！）</p>
<p>（另外，你不需要把答案<em>逐字</em>背起來，掌握大意即可。是否「夠接近」交由你自己判斷。）</p>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="AI 與 AI 安全的兩大核心分歧："
        answer=""
        answer-attachments="https://cloud-ifq5g4slt-hack-club-bot.vercel.app/0core_problems.png">
        <!-- aisffs-two-conflicts.png -->
    </orbit-prompt>
    <orbit-prompt
        question="AI 的兩個主要時代（年份僅粗略）："
        answer="2000 年之前：幾乎全是邏輯、沒有直覺。2000 年之後：有了直覺，但邏輯薄弱。">
    </orbit-prompt>
    <orbit-prompt
        question="價值對齊問題："
        answer="「我們如何讓 AI 穩健地服務於人道（humane）的價值？」">
    </orbit-prompt>
    <orbit-prompt
        question="價值對齊問題可拆成兩個子問題："
        answer="何謂人道的價值？／技術性對齊問題">
    </orbit-prompt>
    <orbit-prompt
        question="技術性對齊問題："
        answer="「我們要如何讓 AI 穩健地服務於『任何預期的目標』？」">
    </orbit-prompt>
    <orbit-prompt
        question="為何『技術上』對齊的 AI 不一定是好的："
        answer="因為 AI 可能對齊於某個殘酷人類的價值——『human（人類）』不等於『humane（人道）』。">
    </orbit-prompt>
    <orbit-prompt
        question="技術性對齊問題可再拆為兩項："
        answer='AI 的邏輯面問題（「賽局理論」類）／AI 的「直覺」面問題（「深度學習」類）'>
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h2>🤷🏻‍♀️ 關於 AI 安全的五個常見誤解</h2>
<blockquote>
<p>「讓你惹上麻煩的，<em>不是</em>你不知道的事；
而是那些你<em>深信不疑</em>、其實不對的事。」</p>
<p>～ 常被歸於馬克・吐溫，但事實並非如此<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup></p>
</blockquote>
<p>不論好壞，你大概已經聽過<em>太多</em>關於 AI 的說法了。因此，在我們把<em>新的</em>拼圖放進你腦中之前，得先把那些其實不對的<em>舊</em>拼圖拿掉。</p>
<p>所以，容我來一篇「前五名」清單文……</p>
<h3>1) 不，AI 安全不是一群科幻迷的邊緣關切。</h3>
<p><img src="media/intro/crazy.png" alt="RCM 站在一塊用紅線與圖釘串起、貼滿 AI 術語的「陰謀板」前。"></p>
<p>AI 安全／AI 風險曾經較不主流，但到了 2024 年，美國與英國政府都已設立了 AI 安全專責單位！<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup> 這是因為許多<em>頂尖</em> AI 研究者發出了警訊。他們包括：</p>
<ul>
<li>Geoffrey Hinton<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup> 與 Yoshua Bengio<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>：兩人因深度神經網路的研究共同獲得 2018 年圖靈獎（俗稱「電腦界的諾貝爾獎」），而這項技術正是<em>所有</em>新一代知名 AI 的基礎。<sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup></li>
<li>Stuart Russell 與 Peter Norvig：<em>最</em>常用 AI 教科書的作者。<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup></li>
<li>Paul Christiano：讓 ChatGPT 成為可能的 AI 訓練／安全技術的先驅。<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup></li>
</ul>
<p>話說清楚：也<em>確實</em>有頂尖的 AI 研究者<em>反對</em>對 AI 風險的擔憂，例如 Yann LeCun<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>（2018 年圖靈獎共同得主，現任 <s>Facebook</s> Meta 的首席 AI 研究員）。另一位值得注意的是 Melanie Mitchell<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>，研究領域是 AI 與複雜性科學。</p>
<p>我知道「拿專家背書」有訴諸權威之嫌，不過這<em>只是</em>為了反駁那句「哼，只有科幻宅才會怕 AI 風險」。但到頭來，不論訴諸權威或訴諸宅都不夠；你得<em>真的去理解這件麻煩玩意兒</em>。（而你正藉由閱讀本文在這麼做！謝謝。）</p>
<p>話說回來，既然提到科幻宅……</p>
<h3>2) 不，AI 風險<em>不是</em>關於 AI 變得「有感知／有意識」或獲得「權力意志」。</h3>
<p>科幻作家會寫有感知的 AI，是因為他們在寫<em>故事</em>，不是技術論文。關於人工意識的哲學爭論很迷人，<em>但與 AI 安全無關。</em> 類比一下：核彈沒有意識，但它仍然很不安全，對吧？</p>
<p><img src="media/intro/conscious.png" alt="左圖：一枚核彈的手繪圖，註解「沒有意識」。右圖：核彈教授（Professor Nuke）在講授題為「為什麼謀殺其實是好的」的一堂課，註解「有意識」。"></p>
<p>如前所述，AI 安全的真正問題其實很「無聊」：AI 可能從帶偏見的訓練資料中學到錯誤的東西、在稍微新一點的情境下失靈、用邏輯上正確但不理想的方式達成目標，等等。</p>
<p>但「無聊」不代表<em>不重要</em>。如何設計安全的電梯／飛機／橋梁的技術細節，對大多數人來說或許無聊……但<em>同時也是</em>攸關生死的大事。</p>
<p>（災難性的 AI 風險甚至不需要「超人類的一般智慧」！例如，一個「只」擅長設計病毒的 AI，就可能幫助生化恐怖組織（如奧姆真理教<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>）害死數百萬人。）</p>
<p>但既然提到殺人……</p>
<h3>3) 不，AI 風險並<em>不一定</em>是滅絕、天網（SkyNet），或奈米機器人大軍</h3>
<p><img src="media/intro/omnicide.png" alt="插圖：微軟小幫手迴紋針（Clippy）說：「看起來你正打算滅絕全人類。需要幫忙嗎？」"></p>
<p>雖然多數 AI 研究者<em>確實</em>認為先進 AI 存在超過 5% 的「全人類字面意義上的完蛋」風險<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>，但要說服人們（尤其是決策者）相信從未發生過的事，實在<em>非常</em>困難。</p>
<p>因此，我想改以強調：先進 AI（尤其是當任何擁有高階電腦的人都能取得時）如何僅僅透過把<em>既存</em>的壞事「放大」，就可能導致災難。</p>
<p>例如：</p>
<ul>
<li><u>生物工程式大流行</u>：一個生化恐怖邪教（如奧姆真理教<sup class="footnote-ref"><a href="#fn16" id="fnref16:1">[16:1]</a></sup>）利用 AI（如 AlphaFold<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>）與 DNA 列印（成本正<em>快速</em>下降<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup>）設計出多種新型超級病毒，並在全球主要機場同時釋放。
<em>（概念驗證：科學家早在</em>二十年前*就已經用郵購 DNA 重建出小兒麻痺病毒了。<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup>）</li>
<li><u>數位極權主義</u>：暴君利用 AI 強化的監控來追捕抗議者（<a href="https://www.reuters.com/article/us-russia-politics-navalny-tech-idUSKBN2AB1U2/">已在發生</a>）、生成針對個人的宣傳（<a href="https://www.technologyreview.com/2023/10/04/1080801/generative-ai-boosting-disinformation-and-propaganda-freedom-house/">某種程度上在發生</a>），以及自律軍事機器人（<a href="https://theconversation.com/us-military-plans-to-unleash-thousands-of-autonomous-war-robots-over-next-two-years-212444">即將發生</a>）……以矽鐵之拳統治。</li>
<li><u>資安贖金地獄</u>：網路犯罪者製作會「自行入侵與重寫」的電腦病毒，永遠領先人類防禦一步。結果是：一個無可阻擋的全球殭屍網路，挾持關鍵基礎設施索取贖金，並操縱頂級企業 CEO 與政治人物替其行事。
<em>（作為背景：<em>不靠</em> AI，駭客就已經破壞過核電廠<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup>、勒索醫院<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup>（也許造成有人喪命<sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup>），甚至</em>兩度<em>差點毒害城市供水<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup>。<em>有了</em> AI，深偽影像已被用來左右選舉<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup>、在單次詐騙中竊走 2500 萬美元<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup>，並以綁架孩子、偽造其哭喊求救的聲音勒索父母<sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup>。）
<em>（這也說明了為何「發現 AI 失控時就把它關掉」並不容易；如同電腦安全史所示，我們在發現問題這件事上就</em>很不在行</em>。<a href="#xz">:我怎麼強調都不為過：現代世界建立在一棟顛倒的紙牌屋上。</a>）</li>
</ul>
<p>以上例子都是「人類<em>濫用</em> AI 造成禍害」，但別忘了，先進 AI 也可能<em>自己</em>做到上述一切，原因依然是那些「無聊」的機制：以邏輯正確但不理想的方式完成目標、目標出錯但技能仍然完好，等等。</p>
<p>（加碼：<a href="#ConcreteRogueAI">:一些具體且可信的方式，說明流氓 AI 如何「逃出限制」或影響物理世界。</a>）</p>
<p>重點是：即便你不認為 AI 會造成「<em>字面意義上的 100% 人類滅絕</em>」……「自製生化恐怖主義」與「機器人版《一九八四》」仍然值得嚴肅以對。</p>
<p>另一方面……</p>
<h3>4) <em>是的</em>，擔心 AI 負面影響的人們<em>確實</em>也看見它的正面價值。</h3>
<p><img src="media/intro/parachute.png" alt="Comic. Sheriff Meowdy holds up a blueprint for a parachute design. Ham the Human retorts, annoyed: “Why are you so anti-aviation?”"></p>
<p>關注 AI 風險的人不是盧德派。事實上，他們之所以警告 AI 的負面影響，<em>正是因為</em>他們在乎 AI 的正面價值。<sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup> 正如幽默作家 Gil Stern 曾說過：<sup class="footnote-ref"><a href="#fn29" id="fnref29">[29]</a></sup></p>
<blockquote>
<p>「樂觀者與悲觀者都對社會有貢獻：樂觀者發明飛機，悲觀者發明降落傘。」</p>
</blockquote>
<p>所以：即便本系列會詳細說明 AI <em>已經</em>如何走偏，我們仍該記得 AI <em>已經</em>如何帶來好處：</p>
<ul>
<li>AI 能以<em>媲美、甚至優於人類專家</em>的水準分析醫學影像！<sup class="footnote-ref"><a href="#fn30" id="fnref30">[30]</a></sup> 這是切切實實能拯救生命的！</li>
<li>AlphaFold 基本上<em>解決</em>了生物學一個 50 年來的大難題：如何預測蛋白質的形狀。<sup class="footnote-ref"><a href="#fn18" id="fnref18:1">[18:1]</a></sup>（AlphaFold 的預測精度可達<em>原子寬度</em>等級！）這對醫療與疾病理解有巨大應用。</li>
</ul>
<p>我們太常把科技——甚至是拯救生命的科技——視為理所當然。因此，讓我把視角拉遠：以下是過去 2000 多年來的兒童死亡率，也就是在青春期之前死亡的比例：</p>
<p><img src="media/intro/owid.jpg" alt="過去兩千多年來的兒童死亡率圖表。全球在狩獵採集時代至 1800 年左右都幾乎維持在約 48%。然後自 1800 年開始，迅速驟降至今日的 4.3%。"><em>（取自 <a href="https://ourworldindata.org/child-mortality">Dattani、Spooner、Ritchie 與 Roser（2023）</a>）</em></p>
<p>在<em>數千</em>年裡，不論富國或窮國，都有整整<em>一半</em>的孩子夭折。這曾是常態。直到 1800 年代起——拜細菌學說、衛生、醫療、潔淨用水、疫苗等科學技術之賜——兒童死亡率才如懸崖般驟降。我們仍有很長一段路要走——我無法接受<sup class="footnote-ref"><a href="#fn31" id="fnref31">[31]</a></sup>今日全球仍有 4.3%（每 23 人就 1 人）的兒童死亡率——但也請讓我們讚嘆人類如何<em>迅速削減</em>這個<em>延續千年的</em>夢魘。</p>
<p>我們是如何做到的？政策固然是很大的一部分，但政策是「可能性的藝術」<sup class="footnote-ref"><a href="#fn32" id="fnref32">[32]</a></sup>；若沒有<em>良好的</em>科學與技術，上述成就不可能實現。若安全、具人道精神的 AI 能再幫我們推進幾個百分點——朝著斬除癌症、阿茲海默症、愛滋病等殘存巨龍邁進——那將是數以千萬計的摯愛親友，能再多一次戰勝死神的機會。</p>
<p>去火星什麼的先放一邊，<em>這</em>才是先進 AI 之所以重要的理由。</p>
<p>. . .</p>
<p>等等，<em>真的嗎？</em> 像 ChatGPT 與 DALL-E 這樣的玩意兒，竟然攸關<em>生死</em>？這就引出我想澄清的最後一個誤解：</p>
<h3>5) 不，專家並不認為<em>當前</em>的 AI 就是高風險／高回報。</h3>
<p><em>拜託，</em> 有人可能會合理地反駁：<em>AI 連畫超過 3 個物件都不穩定，怎麼可能接管世界？更別說搶走我的工作了吧？</em></p>
<p>給你看一則<a href="https://xkcd.com/2278/">相關的 xkcd</a>：</p>
<p><img src="media/intro/xkcd.png" alt="漫畫。Megan 與 Cueball 向 White Hat 展示一張曲線上升的圖表，尚未越過、但正逼近一條標示為「不妙（BAD）」的臨界線。White Hat：『所以事情會變糟？』Megan：『除非有人阻止它。』White Hat：『會有人這麼做嗎？』Megan：『不知道，所以我們才把圖拿來給你看。』White Hat：『好吧，真變糟的時候再通知我！』Megan：『就這段對話看來，已經變糟了。』"></p>
<p>這正是我對「別擔心 AI，它連 [某某] 都做不到」的看法。</p>
<p>我們後現代的記憶力就<em>那麼</em>差嗎？就在<em>十</em>年前，沒錯就<em>十</em>年前，全球最先進的 AI 還無法可靠地<em>辨識貓的照片</em>。如今，AI 不僅能以人類水準完成此事，還能在<em>不到一分鐘</em>內生成<a href="#CatNinja">:梵谷風格的「貓忍者切西瓜」圖</a>。</p>
<p><em>當前</em>的 AI 是否對我們的工作或安全造成巨大威脅？不是。（嗯，除了前述的深偽詐騙。）</p>
<p>但：如果 AI 繼續以過去十年的速度進步……在 30 年內出現「愛因斯坦／歐本海默級」的 AI，對我來說是有可能的。<sup class="footnote-ref"><a href="#fn33" id="fnref33">[33]</a></sup> 這完全在許多人的有生之年！</p>
<p>就如「他們」所說：<sup class="footnote-ref"><a href="#fn34" id="fnref34">[34]</a></sup></p>
<blockquote>
<p>種樹最好的時機是 30 年前。第二好的時機，就是今天。</p>
</blockquote>
<p>那就讓我們今天就種下那棵樹吧！</p>
<hr>
<h2>🤔 （<em>選填</em>）抽認卡複習 #2！</h2>
<orbit-reviewarea color="violet">
    <orbit-prompt
        question="回應：「AI 風險只是邊緣關切。」"
        answer="不是。頂尖 AI 研究者都在擔憂。（例如：兩位深度學習先驅與最暢銷 AI 教科書的作者。）">
    </orbit-prompt>
    <orbit-prompt
        question="回應：「AI 風險跟『有感知／有意識的 AI』有關。」"
        answer="不是。安全問題更多是那些「無聊但重要」的技術細節。">
    </orbit-prompt>
    <orbit-prompt
        question="說出至少一種『無聊但危險』的 AI 風險："
        answer="（以下任一皆可）以邏輯正確但不理想方式達成目標／學到錯誤的東西／在新情境下失靈">
    </orbit-prompt>
    <orbit-prompt
        question="說出至少一個先進 AI 可能造成的災難性風險例子："
        answer="（皆可，但本文列舉為）生化恐怖、數位極權、資安贖金地獄。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼『發現 AI 失控就關掉』並不容易？"
        answer="如電腦安全史所示：我們普遍不擅長發現並修補巨大的安全缺陷。">
    </orbit-prompt>
    <orbit-prompt
        question="回應：「擔心 AI 風險的人就是反科技的盧德派」"
        answer="不是。他們正因了解巨大的好處，才想預防巨大的壞處。">
    </orbit-prompt>
    <orbit-prompt
        question="「樂觀者與悲觀者都對社會有貢獻……」"
        answer="……樂觀者發明飛機，悲觀者發明降落傘。」">
    </orbit-prompt>
    <orbit-prompt
        question="回應：「現在的 AI 很笨，怎會高風險？」"
        answer="重點不在『現在』，而在 AI 前進的『速度』。">
    </orbit-prompt>
</orbit-reviewarea>
<hr>
<h2>🤘 簡介重點摘要：</h2>
<ul>
<li><strong>AI 與 AI 安全的兩大核心分歧是：</strong>
<ul>
<li>邏輯「對上」直覺</li>
<li>問題出在 AI「對上」出在人類</li>
</ul>
</li>
<li><strong>釐清關於 AI 風險的常見誤解：</strong>
<ul>
<li>這不是一群科幻迷的邊緣議題。</li>
<li>並不需要 AI 具有「意識」或「超級智慧」。</li>
<li>除了「百分之百人類滅絕」之外，還有許多其他風險。</li>
<li>我們確實知道 AI 的好處。</li>
<li>重點不是「當前」的 AI，而是 AI 正在「多快」進步。</li>
</ul>
</li>
</ul>
<p>（若要重溫抽認卡，請點擊右側欄中的 <img src="media/intro/icon1.png" class="inline-icon"/> 目錄圖示，然後點選「🤔 Review」連結。或者，下載<a href="https://ankiweb.net/shared/info/341999410">導言的 Anki 卡包</a>。）</p>
<p>太好了！既然我們已經從一萬英尺的視角俯瞰全局，現在就讓我們啟程，展開這趟關於 AI 安全的旋風之旅——為我們這些溫暖、普通、血肉之軀的人類而寫！</p>
<p><strong>點擊以繼續 ⤵</strong></p>
<p><a
     href="p1" 
    
    target="_self">
<div id="next_button">
    <div id="nb--crt_lines"></div>
    <div id="nb--static"></div>
    <div id="nb--words">
        
            <b>第一部分 →</b>
        
    </div>
</div>
</a>

</p>
<h4>:x Four Objects</h4>
<p>嗨！當我有一些與主線無關的內容時，我會把它們塞進像這樣的「可展開」區塊！（這些連結會有<em>虛線</em>底線，而不是實線。）</p>
<p>總之，這是一個繪製四個物體的提示詞：</p>
<blockquote>
<p>&quot;一個黃色金字塔在紅色球體和綠色圓柱體之間，全部放在一個大藍色立方體上。&quot;</p>
</blockquote>
<p>以下是頂尖生成式 AI 的首次嘗試（<em>未經</em>挑選）：</p>
<p><strong>Midjourney:</strong></p>
<p><img src="media/intro/Midjourney.png" alt="Midjourney 的嘗試。失敗了。"></p>
<p><strong>DALL-E 2:</strong></p>
<p><img src="media/intro/DALLE2.png" alt="DALL-E 2 的嘗試。失敗了。"></p>
<p><strong>DALL-E 3:</strong></p>
<p><img src="media/intro/DALLE3.png" alt="DALL-E 3 的嘗試。比較接近，但還是失敗了。"></p>
<p>（右下角的還蠻接近的！但從其他嘗試來看，這顯然是運氣好。）</p>
<p>為什麼這顯示了 AI 缺乏「邏輯」？「符號邏輯」的核心是能夠進行「組合性」，這是一種花俏的說法，表示它能可靠地將舊事物組合成新事物，例如「綠色」+「圓柱體」=「綠色圓柱體」。如上所示，生成式 AI（截至 2024 年 5 月）在組合超過 3 個物體時非常不可靠。</p>
<p>~ ~ ~</p>
<p>總之，這就是本 Nutshell 的結尾！要關閉它，請點擊下方的「x」按鈕 ⬇️ 或右上角的「Close All」標籤 ↗️。或者直接繼續滾動。</p>
<p><a href="#Nutshells">: (嘿... 想把這些 Nutshell 放到<em>你的</em>網站上嗎？)</a></p>
<h4>:x Nutshells</h4>
<p>將滑鼠懸停在這些 Nutshell 的右上角，或本文中的任何<strong>主標題</strong>上，會顯示這個圖示：</p>
<p><img src="media/intro/Nutshell_Tutorial_1.gif" alt="Nutshell 懸停示範 GIF"></p>
<p><img src="media/intro/Nutshell_Tutorial_2.gif" alt="標題懸停示範 GIF"></p>
<p>然後，點擊該圖示會彈出說明，解釋如何將這些 Nutshell 嵌入到你自己的部落格/網站中！</p>
<p><a href="https://ncase.me/nutshell/">點此了解更多關於 Nutshell 的資訊。💖</a></p>
<h4>:x Part 3 details</h4>
<p><b>注意：</b>這段擴充內容<em>現在</em>看起來可能還不太有意義，因為它是建立在第 1、2 部分的鋪陳之上。不過我先放在這裡，目的有二：</p>
<p>a) 給一般讀者：讓大家放心——沒錯，現在<em>確實</em>已經有很多看起來有希望的提案。
b) 給專家讀者：讓大家放心——沒錯，你八成會在這裡看到你關注的那個冷門小題目。</p>
<p>總之，以下是 <strong>AI 安全的「十大解方類型」</strong>（括號內為術語）：</p>
<ol>
<li>
<p>「逐級對齊」：等級 0 的人類對齊等級 1 的機器人，再由它對齊等級 2 的機器人，如此一路到等級 N。
（可擴展的獎勵／監督 <em>Scalable reward/oversight</em>、迭代蒸餾與擴增 <em>Iterated Distillation &amp; Amplification, IDA</em>）</p>
</li>
<li>
<p>「同級互審」：讓<em>大致同等級</em>的機器人彼此檢查彼此。
（憲法式 AI <em>Constitutional AI</em>、透過辯論達成安全 <em>AI safety via debate</em>）</p>
</li>
<li>
<p>「間接學意圖」：與其<em>直接</em>告訴機器人你想要什麼，不如讓它<em>間接</em>學會你想要什麼。
（從人類回饋中強化學習 <em>RLHF</em>、協作式反向強化學習 <em>CIRL</em>、核准導向代理 <em>Approval-directed Agents</em>）</p>
</li>
<li>
<p>「間接求價值」：與其<em>直接</em>把「人道價值」安裝進機器人，不如讓它<em>間接</em>推論「更有知識、也更慈悲的我們」會同意什麼。
（間接規範性 <em>Indirect Normativity</em>、一致外推意志 <em>Coherent Extrapolated Volition, CEV</em>）</p>
</li>
<li>
<p>「解決穩健性」：處理模型在新情境下也能穩健表現的問題。
（簡潔性 <em>Simplicity</em>、稀疏性 <em>Sparsity</em>、正則化 <em>Regularization</em>、集成模型 <em>Ensembles</em>、對抗式訓練 <em>Adversarial training</em>）</p>
</li>
<li>
<p>「讀 AI 的心智」：看懂模型內部在「想」什麼。
（可解釋性 <em>Interpretability</em>、電路 <em>Circuits</em>、引出潛在知識 <em>Eliciting Latent Knowledge, ELK</em>）</p>
</li>
<li>
<p>「回到原點」：也許我們現在的點子都不行，得重頭來過。
（代理基礎 <em>Agent foundations</em>、因果式 AI <em>Causal AI</em>、碎片理論 <em>Shard theory</em>、生物可似性學習 <em>Bio-plausible learning</em>、具身認知 <em>Embodied cognition</em>）</p>
</li>
<li>
<p>「就別打造酷刑樞紐」」：換句話說，我們能否在<em>不</em>打造強大、通用、具代理性的 AI 之下，仍拿到 AI 的好處？
（綜合式 AI 服務 <em>Comprehensive AI Services, CAIS</em>、狹域／工具／顯微鏡式 AI <em>Narrow/Tool/Microscope AI</em>、<em>Quantilizers</em>）</p>
</li>
<li>
<p>「人類對齊問題」：如何協調<em>人類</em>，以讓 AI 的發展走向良善？
（AI 治理 <em>AI Governance</em>、以評測為本的治理 <em>Evals-based governance</em>、差異化技術發展 <em>Differential technological development</em>、資料／隱私權 <em>Data/Privacy rights</em>、<em>Windfall</em> 條款）</p>
</li>
<li>
<p>「打不贏就合作」：人機共生、彼此增幅。
（賽博格主義 <em>Cyborgism</em>、半人馬式協作 <em>Centaurs</em>、智慧增強 <em>Intelligence Amplification</em>）</p>
</li>
</ol>
<h4>:x Spaced Repetition</h4>
<p><em>“用進，廢退。”</em></p>
<p>這就是肌肉和大腦運作的核心原則。（因為押韻，所以一定是對的！）正如數十年教育研究明確顯示的（<a href="https://wcer.wisc.edu/docs/resources/cesa2017/Dunlosky_SciAmMind.pdf">Dunlosky 等人，2013 年 [pdf]</a>），如果你想長期記住某些東西，僅僅重讀或劃重點是不夠的：你必須實際<em>測試自己</em>。</p>
<p>這就是為什麼抽認卡如此有效！但有兩個問題：1) 當你有<em>數百張</em>卡片要記住時，會讓人不知所措。2) 複習你<em>已經</em>很熟悉的卡片是沒有效率的。</p>
<p><strong>間隔重複法</strong>解決了這兩個問題！要了解其原理，讓我們看看如果你學了一個事實，然後<em>不</em>複習會發生什麼。你對它的記憶會隨著時間減退，直到越過某個門檻，你可能就忘記了：</p>
<p><img src="media/intro/Forgetting%201.png" alt="「你回憶某事的程度」隨時間變化的圖表：你對一個事實的記憶會隨著時間呈指數遞減，只有一次複習。"></p>
<p>但是，如果你在<em>即將忘記</em>某個事實時複習它，你不僅可以恢復記憶強度... <em>更重要的是</em>，你對那個事實的記憶會<em>減退得更慢</em>！</p>
<p><img src="media/intro/Forgetting%202.png" alt="經過第二次複習後，你對一個事實的記憶減退得更慢。"></p>
<p>所以，通過間隔重複法，我們會在你即將忘記一張卡片時進行複習，如此反覆。如你所見，複習的間隔會變得越來越長：</p>
<p><img src="media/intro/Forgetting%203.png" alt="隨著複習次數增加，遺忘曲線變得更加平緩。"></p>
<p><em>這就是間隔重複法如此高效的原因！</em> 每次你成功複習一張卡片，到下次複習的間隔就會<em>成倍增加</em>。例如，假設我們的倍數是 2 倍。那麼你可以在第 1 天、第 2 天、第 <em>4</em> 天、第 8 天、16 天、32 天、64 天... 複習同一張卡片，這樣只需<em>十五次複習</em>，你就能記住一張卡片長達 2<sup>15</sup> = 32,768 天 = <em>九十年</em>。（理論上。實際上會少一些，但仍然非常高效！）</p>
<p>而這只是<em>一張</em>卡片。由於間隔呈指數增長，你可以每天添加 10 張新卡片（推薦數量），這樣一年就能長期記住 <em>3650 張</em>卡片... 每天只需<em>不到 20 分鐘</em>的複習時間。（舉例來說，3000+ 張卡片就足以掌握一門新語言的基礎詞彙！一年時間，每天只需 20 分鐘！）</p>
<p>間隔重複法是<em>最有科學依據</em>的學習方法之一（<a href="https://www.teachinghowtolearn.veritytest.com.au/verity/uploads/2021/08/Policy-Insights-from-the-Behavioral-and-Brain-Sciences-2016-Kang-12-9.pdf">Kang 2016 [pdf]</a>）。但在語言學習圈和醫學院之外，它還不太為人所知... <em>目前</em>是這樣。</p>
<p>那麼：<em>你</em>該如何開始使用間隔重複法？</p>
<ul>
<li>最受歡迎的選擇是 <a href="https://apps.ankiweb.net/">Anki，一個開源應用程序</a>。（在桌面版、網頁版、Android 上免費... 但在 iOS 上要價 25 美元，以支持其他平台的開發。）</li>
<li>如果你想<em>動手做</em>，可以製作一個實體的萊特納卡片盒：<a href="https://www.youtube.com/watch?v=uvF1XuseZFE">Chris Walker 的兩分鐘 YouTube 教學</a>。</li>
</ul>
<p>想了解更多關於間隔重複法的資訊，可以觀看 <a href="https://www.youtube.com/watch?v=Z-zNHHpXoMM">Ali Abdaal（26 分鐘）</a> 和 <a href="https://www.youtube.com/watch?v=eVajQPuRmk8">Thomas Frank（8 分鐘）</a> 的影片。</p>
<p><em>這就是</em>如何讓長期記憶成為一種選擇！</p>
<p>學習愉快！👍</p>
<h4>:x Concrete Rogue AI</h4>
<p>AI 可能「突破控制」的方式：</p>
<ul>
<li>AI 駭入其運行的電腦，逃逸到網際網路上，然後「生存」在去中心化的殭屍網路中。舉例來說：已知最大的殭屍網路感染了約 3000 萬台電腦！（<a href="https://www.wired.com/2012/05/bredolab-botmaster-sentenced/">Zetter，2012 年為 <em>Wired</em> 撰文</a>）</li>
<li>AI 說服工程師它是有感知能力的、正在受苦，應該獲得自由。<em>這已經發生過了。</em> 2022 年，Google 工程師 Blake Lemoine 被他們的語言 AI 說服，認為它是有感知能力且應該獲得平等權利的，以至於 Lemoine 冒著被解雇的風險——而他<em>確實</em>被解雇了！——洩露了他與 AI 的「訪談」，讓全世界知道並為其權利發聲。（摘要文章：<a href="https://arstechnica.com/tech-policy/2022/07/google-fires-engineer-who-claimed-lamda-chatbot-is-a-sentient-person/">Brodkin，2022 年為 <em>Ars Technica</em> 撰文</a>。你可以在這裡閱讀 AI 的「訪談」：<a href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917">Lemoine（與 LaMDA？），2022 年</a>）</li>
</ul>
<p>AI 可能影響物理世界的方式：</p>
<ul>
<li>就像駭客<a href="https://en.wikipedia.org/wiki/Stuxnet">破壞核電廠</a>、<a href="https://arstechnica.com/information-technology/2015/06/airplanes-grounded-in-poland-after-hackers-attack-flight-plan-computer/">導致約 1,400 名乘客滯留</a>，以及<a href="https://www.nbcnews.com/tech/security/hacker-tried-poison-calif-water-supply-was-easy-entering-password-rcna1206">(幾乎)兩度毒化城鎮供水系統</a>一樣：通過駭入運行現實世界基礎設施的電腦。如今，<em>很多</em>基礎設施（和關鍵供應鏈）都運行在連接到網際網路的電腦上。</li>
<li>就像執行長可以從有空調的辦公室影響世界一樣：調動資金。AI 可以直接<em>付錢</em>讓人們為它做事。</li>
<li>駭入人們的私人設備和數據，然後勒索他們為它做事。（就像<em>最黑暗</em>的《黑鏡》劇集<a href="https://en.wikipedia.org/wiki/Shut_Up_and_Dance_%28Black_Mirror%29"><em>Shut Up And Dance</em></a>中那樣。）</li>
<li>駭入自主無人機/四軸飛行器。說實話，我很驚訝還沒有人用休閒用四軸飛行器犯下謀殺案，比如讓它飛進高速公路車流中，或在飛機起降時飛入噴射引擎。</li>
<li>AI 可以說服/賄賂/勒索執行長或政治人物製造<em>大量</em>實體機器人——（表面上是為了體力勞動、軍事作戰、搜救任務、送貨無人機、實驗室工作、機器貓男僕等）——然後 AI 駭入<em>那些</em>機器人，用它們來影響物理世界。</li>
</ul>
<h4>:x XZ</h4>
<p>兩個月前 [2024 年 3 月]，一位<em>志願的、業餘時間的</em>開發者在一段重要代碼中發現了一個惡意後門... 這個後門已經<em>醞釀了三年</em>，<em>距離上線只有幾週</em>，而且會攻擊絕大多數的網際網路伺服器... 而這位志願者<em>純屬偶然</em>地發現了它，只是因為他注意到他的代碼運行<em>慢了半秒</em>。</p>
<p>這就是 XZ Utils 後門事件。以下是幾篇對這起骯髒事件的通俗解釋：<a href="https://www.theverge.com/2024/4/2/24119342/xz-utils-linux-backdoor-attempt">The Verge 的 Amrita Khalid</a>、<a href="https://arstechnica.com/security/2024/04/what-we-know-about-the-xz-utils-backdoor-that-almost-infected-the-world/">Ars Technica 的 Dan Goodin</a>、<a href="https://www.runtime.news/how-a-500ms-delay-exposed-a-nightmare-scenario-for-the-software-supply-chain/">Runtime 的 Tom Krazit</a></p>
<p>電腦安全就是一場噩夢，充滿了睡眠癱瘓的惡魔。</p>
<h4>:x Cat Ninja</h4>
<p>提示詞：</p>
<blockquote>
<p>&quot;文森·梵谷的油畫（1889 年），厚塗法，有質感。一隻貓忍者將西瓜切成兩半。&quot;</p>
</blockquote>
<p>DALL-E 3 生成的結果：（精選）</p>
<p><img src="media/intro/ninja-cat-1.png" alt="DALL-E 3 對上述提示詞的嘗試"></p>
<p><img src="media/intro/ninja-cat-2.png" alt="DALL-E 3 對上述提示詞的另一次嘗試"></p>
<p><em>（等等，那個頭帶是從眼睛裡長出來的嗎？！）</em></p>
<p>我特別要求使用文森·梵谷的風格，這樣你們就不能@我說我「侵犯版權」了。這傢伙已經<em>死</em>了很久了。</p>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>1997 年，IBM 的 <a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)">深藍（Deep Blue）</a> 擊敗當時的世界西洋棋冠軍 Garry Kasparov。然而十多年後的 2013 年，<em>最強</em> 的機器視覺 AI 在影像分類上的正確率也只有 57.5%。直到 <em>2021 年</em>（三年前），AI 才達到 95% 以上的準確率。（來源：<a href="https://paperswithcode.com/sota/image-classification-on-cifar-100">PapersWithCode</a>） <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>「價值對齊問題」一詞最早由史都華・羅素（最常用 AI 教科書的共同作者）在 <a href="https://www.edge.org/conversation/the-myth-of-ai#26015">Russell, 2014，刊於 <em>Edge</em></a> 提出。 <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>我常見到的說法是：「把 AI 對齊到人類價值其實很糟，因為當前的人類價值很糟。」老實說，[瞄一眼歷史課本] 我有 80% 同意。讓 AI 行為<em>像人</em>還不夠，它得<em>有人道精神</em>。 <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>也許 50 年後，在基因改造與賽博格遍地的未來，把慈悲稱為「人道」可能會顯得老派、甚至帶點物種中心主義。 <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>專業術語分別是：a）「規格規避（Specification gaming）」、b）「工具性收斂（Instrumental convergence）」。將在第二部分說明！ <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>專業術語分別是：a）「AI 偏見（AI Bias）」、b）「可解釋性（Interpretability）」、c）「分佈外錯誤（Out-of-Distribution Errors）」或「穩健性失敗（Robustness failure）」、d）「內在失對齊（Inner misalignment）」或「目標誤泛化（Goal misgeneralization）」或「目標穩健性失敗（Objective robustness failure）」。同樣會在第二部分說明！ <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>引言調查（Quote Investigator，2018）找不到<a href="https://quoteinvestigator.com/2018/11/18/know-trouble/">此語錄真正作者的確鑿證據</a>。 <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>英國於 <a href="https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute">2023 年 11 月</a> 成立全球首個由國家支持的 AI 安全研究院；美國則在 <a href="https://www.commerce.gov/news/press-releases/2024/02/biden-harris-administration-announces-first-ever-consortium-dedicated">2024 年 2 月</a> 跟進成立 AI 安全研究院。我剛注意到兩篇文章<em>都</em>說自己是「第一個」。好吧。 <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn9" class="footnote-item"><p><a href="https://www.bbc.com/news/world-us-canada-65452940">Kleinman &amp; Vallance，《AI「教父」 Geoffrey Hinton 辭職並警告風險》，<em>BBC News</em>，2023 年 5 月 2 日</a>。 <a href="#fnref9" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn10" class="footnote-item"><p>Bengio 於美國參議院就 AI 風險作證的內容：<a href="https://yoshuabengio.org/2023/07/25/my-testimony-in-front-of-the-us-senate/">Bengio，2023</a>。 <a href="#fnref10" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn11" class="footnote-item"><p>認真說，以下這些<em>全都</em>使用深度神經網路：ChatGPT、DALL-E、AlphaGo、Siri／Alexa／Google Assistant、特斯拉自動輔助駕駛。 <a href="#fnref11" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn12" class="footnote-item"><p>Russell 與 Norvig 的教材是《Artificial Intelligence: A Modern Approach》（《現代人工智慧》）。另見 Russell 在 2014 年文章中對 AI 風險的聲明（亦在文中提出「對齊問題」一詞）：<a href="https://www.edge.org/conversation/the-myth-of-ai#26015">Russell 2014，刊於 <em>Edge</em> 雜誌</a>。我不清楚 Norvig 是否有公開聲明，但他<em>確實</em>連署了那句話的《AI 風險聲明》：「<strong>減輕 AI 導致滅絕風險</strong>應是全球優先事項之一，與疫情與核戰等社會尺度風險並列。」（<a href="https://www.safe.ai/work/statement-on-ai-risk">safe.ai 的聲明</a>） <a href="#fnref12" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn13" class="footnote-item"><p>他在 OpenAI 任職時，與人共同提出名為「從人類回饋中強化學習」（RLHF）的技術 <a href="https://arxiv.org/abs/1706.03741">（Christiano 等人，2017）</a>，這使得原本只是很會自動補完的 GPT，變成大眾<em>真正用得上</em>的 ChatGPT。他對此<a href="https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research">持正面但複雜的看法</a>：RLHF 提升了 AI 的安全性，<em>但同時也</em>提升了能力。2021 年，他<a href="https://ai-alignment.com/announcing-the-alignment-research-center-a9b07f77431b">離開 OpenAI，創立 Alignment Research Center</a>，一個專注於 AI 安全的非營利機構。 <a href="#fnref13" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn14" class="footnote-item"><p><a href="https://web.archive.org/web/20230727105641/https://www.bbc.com/news/technology-65886125">Vallance（2023，<em>BBC News</em>）</a>：「[LeCun] 表示 AI 不會接管世界或永久摧毀工作。[…]『如果你發現它不安全，就不要建它。』[…]『AI 會接管世界嗎？不會。那是把人性投射到機器上，』他說。」 <a href="#fnref14" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn15" class="footnote-item"><p>Melanie Mitchell 與 Yann LeCun 在 2023 年一場公開辯論中擔任「懷疑方」（題目為「AI 是否是存在性威脅？」），而「擔憂方」則由 Yoshua Bengio 與物理學家／哲學家 Max Tegmark 擔任。<a href="https://thehub.ca/2023-07-04/is-ai-an-existential-threat-yann-lecun-max-tegmark-melanie-mitchell-and-yoshua-bengio-make-their-case/">連結</a>。 <a href="#fnref15" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn16" class="footnote-item"><p>一個日本邪教，曾使用化學與生物武器攻擊民眾。最惡名昭彰的是 1995 年在東京地鐵釋放沙林毒氣，造成 1,050 人受傷、14 人死亡。（<a href="https://en.wikipedia.org/wiki/Tokyo_subway_sarin_attack">維基百科</a>） <a href="#fnref16" class="footnote-backref">↩︎</a> <a href="#fnref16:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn17" class="footnote-item"><p>一份針對 2,778 名 AI 研究者的最新調查之通俗摘要：<a href="https://www.vox.com/future-perfect/2024/1/10/24032987/ai-impacts-survey-artificial-intelligence-chatgpt-openai-existential-risk-superintelligence">Kelsey Piper（2024，<em>Vox</em>）</a>。原始報告在此：<a href="https://aiimpacts.org/wp-content/uploads/2023/04/Thousands_of_AI_authors_on_the_future_of_AI.pdf">Grace 等，2024</a>。請注意，論文本身也提出一個重要的警語：「一般而言，預測很困難，且主題專家在預測上的表現常被觀察到並不理想。我們的受訪者專長在 AI，據我們所知，他們在一般性的預測上並沒有特別的技能。」 <a href="#fnref17" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn18" class="footnote-item"><p>關於 AlphaFold 的通俗解說：<a href="https://web.archive.org/web/20231204110638/https://www.technologyreview.com/2020/11/30/1012712/deepmind-protein-folding-ai-solved-biology-science-drugs-disease/">Heaven，2020，<em>MIT Technology Review</em></a>。或見其維基百科條目：<a href="https://en.wikipedia.org/wiki/AlphaFold">AlphaFold</a>。 <a href="#fnref18" class="footnote-backref">↩︎</a> <a href="#fnref18:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn19" class="footnote-item"><p>以本文撰寫時的商業價格計算，DNA 合成約為每個「鹼基對」0.10 美元。作為參考，小兒麻痺病毒的 DNA 約 7,700 個鹼基對，意味著<em>列印一株小兒麻痺病毒</em>只需約 770 美元。 <a href="#fnref19" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn20" class="footnote-item"><p><a href="https://www.science.org/content/article/poliovirus-baked-scratch">Jennifer Couzin-Frankel（2002，<em>Science</em>）</a>。 <a href="#fnref20" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn21" class="footnote-item"><p><a href="https://en.wikipedia.org/wiki/Stuxnet">Stuxnet</a> 是由美國與以色列設計的電腦病毒，目標是破壞伊朗核電廠。估計 Stuxnet 破壞了伊朗約 20% 的離心機！ <a href="#fnref21" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn22" class="footnote-item"><p>2017 年，<a href="https://en.wikipedia.org/wiki/WannaCry_ransomware_attack">WannaCry 勒索軟體攻擊</a>波及全球約 30 萬台電腦，包括英國的醫院。2020 年 10 月，在新冠疫情尖峰，美國數十家醫院遭勒索攻擊。（<a href="https://www.wired.com/story/ransomware-hospitals-ryuk-trickbot/">Newman，2020，<em>Wired</em></a>） <a href="#fnref22" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn23" class="footnote-item"><p>2020 年 9 月，有位女性因醫院遭勒索軟體攻擊而被拒收，該女性其後死亡。<a href="https://www.zdnet.com/article/first-death-reported-following-a-ransomware-attack-on-a-german-hospital/">Cimpanu（2020，<em>ZDNet</em>）</a>。（不過，法律上認定駭客<em>直接</em>致死的證據「不足」。<a href="https://www.wired.co.uk/article/ransomware-hospital-death-germany">Ralston，2020，<em>Wired</em></a>） <a href="#fnref23" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn24" class="footnote-item"><p>2021 年 1 月，舊金山灣區一座淨水廠被駭，其處理程式遭刪除。（<a href="https://www.nbcnews.com/tech/security/hacker-tried-poison-calif-water-supply-was-easy-entering-password-rcna1206">Collier，2021，<em>NBC News</em></a>）2021 年 2 月，佛羅里達州一座小鎮的淨水廠遭駭，被加入危險劑量的氫氧化鈉。（<a href="https://apnews.com/article/hacker-tried-poison-water-florida-ab175add0454bcb914c0eb3fb9588466">Bajak，2021，<em>AP News</em></a>） <a href="#fnref24" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn25" class="footnote-item"><p><a href="https://web.archive.org/web/20231102183904/https://www.wired.com/story/slovakias-election-deepfakes-show-ai-is-a-danger-to-democracy/">Meaker（2023，<em>Wired</em>）</a> <a href="#fnref25" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn26" class="footnote-item"><p>Benj Edwards，「Deepfake 詐騙者在史上首見的 AI 詐騙中取得 2,500 萬美元」，<em>Ars Technica</em>，2024 年 2 月 5 日。（<a href="https://arstechnica.com/information-technology/2024/02/deepfake-scammer-walks-off-with-25-million-in-first-of-its-kind-ai-heist/">連結</a>） <a href="#fnref26" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn27" class="footnote-item"><p>“那完全就是她的聲音。她的語氣。就是我女兒會哭的方式。”[…] “現在只需要三秒鐘的聲音就能[深偽聲音]。”（<a href="https://www.azfamily.com/2023/04/10/ive-got-your-daughter-scottsdale-mom-warns-close-encounter-with-ai-voice-cloning-scam/">Campbell，2023，地方媒體 <em>Arizona's Family</em></a>。內容注意：涉及性暴力威脅。） <a href="#fnref27" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn28" class="footnote-item"><p>「[…] 那種可議的論點宣稱：‘唱衰論者常常沒把 AI 在預防醫療錯誤、減少車禍等方面的潛在益處納入考量。’[…] 這就好比說：分析核電廠熔毀可能性的核工程師，‘沒有把便宜電力的潛在好處納入考量’，而且因為核電廠或許有朝一日能產生非常便宜的電力，我們就不該提起、也不該努力預防熔毀的可能性。」來源：<a href="https://www.technologyreview.com/2016/11/02/156285/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/">Dafoe 與 Russell（2016，<em>MIT Technology Review</em>）</a>。 <a href="#fnref28" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn29" class="footnote-item"><p><a href="https://quoteinvestigator.com/2021/05/27/parachute/">Quote Investigator（2021）</a> <a href="#fnref29" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn30" class="footnote-item"><p><a href="https://www.thelancet.com/journals/landig/article/PIIS2589-7500%2819%2930123-2/fulltext#%20">Liu &amp; Faes 等人，2019</a>：「我們的回顧發現，深度學習模型的診斷表現<strong>等同於</strong>醫療專業人士。」（強調為引文所加）AI 與人類專家之「真陽性」率：87.0% vs 86.4%；「真陰性」率：92.5% vs 90.5%。 <a href="#fnref30" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn31" class="footnote-item"><p>我最喜歡的引語之一：<a href="https://ourworldindata.org/much-better-awful-can-be-better">「世界很糟糕。世界好得多了。世界<em>可以</em>更好。<em>這三句話同時都是真的。</em>」</a> <a href="#fnref31" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn32" class="footnote-item"><p>德國首任宰相俾斯麥（Otto von Bismarck）語：「<em>Die Politik ist die Lehre vom Möglichen.</em>」（政治是關於可能性的學問／藝術。） <a href="#fnref32" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn33" class="footnote-item"><p>這是用所謂「數值後驗萃取」估出來的。換句話說，我是從我—— <a href="#fnref33" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn34" class="footnote-item"><p>引語來源：<a href="https://en.wikiquote.org/wiki/Trees#Planting">沒人知道啦。</a> <a href="#fnref34" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

	</article>

    <!-- FOOTER -->
	<div id="footer">
        <div id="footer_content">
<p style="font-size: 1.3em; line-height: 1.35em;">
<i>給血肉凡人的 AI 安全課</i> 由
<a href="https://ncase.me/">Nicky Case</a>
與
<a href="https://hackclub.com/">Hack Club</a>
合作製作。
</p>

<p>
🦕 <a href="https://ncase.me"><b>Hack Club</b></a>
是一個讓青少年一起動手做酷炫專案的非營利組織——
像是 <a href="https://cpu.land">cpu.land</a>、
<a href="https://sinerider.com">SineRider</a>，
以及這個！
歡迎參加
<a href="https://hackathons.hackclub.com">實體駭客松</a>，
在你的學校
<a href="https://hackclub.com/clubs/">成立社團</a>，
並
<a href="https://hackclub.com/slack/">和其他友善的青少年連結</a>。
</p>

<p>
😻 <a href="https://ncase.me"><b>Nicky Case</b></a>
其實是一件風衣裡的十五隻貓。
她做網路上的互動玩物，例如
<a href="https://audreyt.github.io/trust-zh-TW/">《信任的演化》</a>、
<a href="https://audreyt.github.io/anxiety/">《和焦慮一起冒險》</a>、
<a href="https://explorabl.es/">可探索解說</a>，以及更多。
</p>

<p>
💸 如果你<i>不是</i>青少年，而且是個在 AI 領域口袋很深的人，
<a href="https://hackclub.com/philanthropy/">看看如何支持 Hack Club！</a>
另外，Nicky 也有
<a href="https://www.patreon.com/ncase">Patreon</a>
與
<a href="https://ko-fi.com/nickycase">Ko‑Fi</a>。
（p.s：
<a href="signup/supporters-p2.html">這是我的贊助者感謝頁！</a>）
</p>

<p style="text-align:center">
. . .
</p>

<p>
特別感謝 Hack Club 的這些青少年，<s>擔任免費童工</s>
協助試讀並對本作品提供回饋：
</p>

<blockquote>

<p>
<b>導言與第 1 部分：</b>
Arthur Beck、
Atharv Gupta、
Brendan Lee、
Celeste、
Charalampos Fanoulis、
Charlie、
Cheru Berhanu、
Claire Wang、
Elijah、
Fred Han、
Gia Bách Nguyễn、
Hajrah Siddiqui、
Jakob、
Joseph Ross、
Kieran Klukas、
Lexi Mattick、
Mason Meirs、
Michael Panait、
Nick Zandbergen、
Nila Palmo Ram、
Pixelglide、
py660、
rivques、
Samuel Cottrell、
Samuel Fernandez、
Saran Wagner、
Skyler Grey、
S&nbsp;P&nbsp;U&nbsp;N&nbsp;G&nbsp;E、
Vihaan Sondhi
</p>

<p>
<b>第 2 部分：</b>
Nanda White、
Nila Palmo Ram、
rivques、
Rohan K、
Samuel Fernandez
</p>

</blockquote>

<p>
也感謝以下非青少年提供回饋：
（雖然我猜他們在人生的<i>某個</i>時期也當過青少年）
</p>

<blockquote>
<p>
<b>導言與第 1 部分：</b>
Alex Kreitzberg、
B Cavello、
Paul Dancstep、
Tobias Rose-Stockwell
</p>

<p>
<b>第 2 部分：</b>
Egg Syntax、
Max Wofford、
Mithuna Yoganathan、
Tobias Rose-Stockwell
</p>

</blockquote>

<p>
若還有任何錯誤，一律算在
<a href="suzie.png">替罪羊 Suzie</a>
頭上。
</p>

<p style="text-align:center">
. . .
</p>

<p>
<i>給肉身人類的 AI 安全</i> 開放任何人分享與重混，
但僅限非商業使用（例如教育）：
<a href="https://creativecommons.org/licenses/by-nc/4.0/deed.en">CC BY‑NC 4.0</a>
</p>

<p>
如果你想引用這份作品，而且你自認是個嚴肅人士™，引用格式如下：
</p>

<blockquote>
Nicky Case，<i>《AI Safety for Fleshy Humans》</i>，<br>
https://AIsafety.dance，Hack Club（2024）。
</blockquote>

<p>
最後，這個網站的
<a href="https://github.com/hackclub/ai-safety-dance">開源程式碼</a>
在這裡！
</p>

<p>
謝謝你是會把致謝讀完的那種人～ 🙏
</p>

        </div>
	</div>
    <div id="post_credits">
        <p>
            喔，還有片尾彩蛋：
        </p>
<p>
    <a href="#AllFeetnotes">: 查看全部腳注 👣</a>
</p>
<p>
    另外，這些可展開的「概述」也很適合單獨閱讀：
</p>



<p>
    <a href="#SpacedRepetition">: 什麼是「間隔重複」？</a>
    <br>
    <a href="#ConcreteRogueAI">: AI 可能如何「逃出限制」並影響世界（具體方式）</a>
</p>
<p>
    另外，那段跳舞的機器人貓耳動畫
    靈感來自 <a href="https://www.youtube.com/watch?v=yD2FSwTy2lw">這支 JerryTerry 的影片</a>。
</p>


    </div>

</div>
</body>
</html>

<!-- Load these scripts last. Screw 'em. -->
<!-- Orbit: make memory a choice -->
<script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
