（如果你是直接被連到這一頁，建議（可選）先看看［[導言與第一部分](../)］！）

> 「問題陳述得好，等於解決了一半。」    
> —— 某人[^somebody]（👈 將游標移過引註以展開）

[^somebody]: Frequently attributed to Charles Kettering, former head of research at General Motors, but I couldn't find a legit citation.

*"Didn't you use that quote just a few minutes ago?"* you ask. Nah, [part one](https://aisafety.dance/p1/) was published in May 2024, this part was published **Aug 2024**. It's been three months, I'll remind you of the quote.

也提醒一下我們要表述並解決的問題！那就是「價值對齊問題」（Value Alignment Problem）：

> **_我們如何打造能夠穩健服務「人道價值」的 AI？_**

如第一部分所述，我們可以把這個問題拆成如下：

![將「價值對齊問題」拆解的圖。可拆為「技術對齊」與「人道價值」。其中「技術對齊」又可拆為「AI 邏輯的問題」與「AI 直覺的問題」。](../media/p2/breakdown/breakdown0001.png)

在這裡（第二部分），我們將深入探討 **AI 安全的七個主要次級問題：**

1. 目標規格錯置（Goal mis-specification）[↪](#problem1)
2. 工具性收斂（Instrumental convergence）[↪](#problem2)
3. 可解釋性不足（Lack of interpretability）[↪](#problem3)
4. 魯棒性不足（Lack of robustness）[↪](#problem4)
5. 演算法偏見（Algorithmic bias）[↪](#problem5)
6. 目標錯誤泛化（Goal mis-generalization）[↪](#problem6)
7. 那「人道價值」到底是什麼？[↪](#problem7)

（想跳著看也行，右邊有 <img src="../media/intro/icon1.png" class="inline-icon"/> 目錄！👉 你也可以 <img src="../media/intro/icon2.png" class="inline-icon"/> 切換頁面風格，或 <img src="../media/intro/icon3.png" class="inline-icon"/> 查看剩餘閱讀進度。）

不只如此！上述七個次級問題，也是一系列**跨領域核心觀念**的極佳入門：博弈論、統計學，甚至*哲學！* 這就是為什麼我說，理解 AI 會幫助你更了解*人類*。也許還能幫我們解開那個難纏的*人類*對齊問題：

> *我們要如何讓*人類*也能穩健地服務人道價值？*

少說點雞湯，開始吧：

---

# AI 邏輯的問題（Problems with AI Logic）

<a id="problem1"></a>

## ❓ 問題一：目標規格錯置（Goal Mis-specification）

算了，都過三個月了，我也重用一下下面這則機器人貓少漫畫吧。（七個問題每個都有一幅貓少漫畫！）

![漫畫。人類哈姆對機器人貓少女僕（RCM）說：「保持房子乾淨。」RCM 推理：誰造成髒亂？人類造成髒亂！因此——把人類趕走。RCM 把哈姆扔出家門。](../media/p2/comix/GMS.png)

*「小心你許的願，因為它可能真的實現。」* 這是個古老到被神話化的問題：彌達斯國王、反諷的精靈、猴爪。

這就叫作 **目標規格錯置**（也稱獎勵規格錯置，Reward Mis-specification）：當 AI 做的是你*字面上要求它做的*，而不一定是你*真正想要的*。

（如果你不記得第一部分，這裡有一些 [:看似顯而易見、卻會翻車的「做人道 AI」方法](#WaysToMakeHumaneAIGoingWrong)。加碼：[:連「做預測」這種*被動*目標都可能導致傷害！](#StoryOfPassivePredictionLeadingToHarm) 👈 *可選：點此展開*）

*（另外——如果你想在 12 月中「第三部分：提出的解法」發布時收到通知，請在下方訂閱！高中生還可以免費拿貼紙 👇）*

{% include 'templates/signup.html' %}

. . .

好了，基礎複習就到這。接下來介紹：

**這與其他領域的核心觀念有何關聯：**
* 經濟學
* 因果圖（causal diagrams）
* 最適化理論（optimization theory）
* 安全心態（security mindset）

**目標規格錯置的四個細微處：**
* 問題不在 AI 不會「知道」我們要什麼，而是它不會「*在乎*」。
* 自我灌獎（Wireheading）
* 做我所意（Do What I Mean）
* 我們其實*希望*機器人有時能違命？？


### 與其他領域核心觀念的關聯（Relations to core ideas from other fields）

**經濟學（Economics）：**

要讓他人做你*真正意圖*的事，而不只是你獎勵機制所鼓勵他們做的事，這個問題在經濟學中臭名昭著。它有很多名稱：「委託－代理問題」（Principal–Agent Problem）、「獎勵 A、卻希望得到 B 的謬誤」[^folly]……不過最常被提到的是<u>古德哈特法則（Goodhart's Law）</u>，意譯如下[^goodhart]：

[^folly]: Kerr (1975). [On the folly of rewarding A, while hoping for B.](https://web.archive.org/web/20240414233549/http://www.econ.hsehelp.ru/sites/default/files/%D0%91%D0%98/3%20%D0%BA%D1%83%D1%80%D1%81/%D0%98%D0%BD%D1%81%D1%82%D0%B8%D1%82%D1%83%D1%86.%20%D1%8D%D0%BA%D0%BE%D0%BD%D0%BE%D0%BC%D0%B8%D0%BA%D0%B0/6%20Kerr75.pdf)

[^goodhart]: Original statement of Goodhart's Law ([Wikipedia](https://en.wikipedia.org/wiki/Goodhart%27s_law)) by British economist Charles Goodhart: “Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.”

> *當你獎勵某個指標時，這個指標通常會被「玩弄」取巧。*

例如：老師希望學生真正學到東西，於是依考試分數給獎勵……結果有些學生用作弊、或死背不求理解來「打遊戲」。又或者：選民想要為自己奮戰的政治人物，於是投給有魅力的領袖……結果有些政治人物用「華而不實」來「打遊戲」。

![「一直都是」雙太空人迷因。地球標註：「我們個人與制度問題的 95%」。太空人 1：「等等，原來一切都是古德哈特法則？」太空人 2：「一直都是。」](../media/p2/gms/goodhart.png)

而正如程式設計師所發現的，AI 也一樣。如果你用某個指標去「獎勵」AI，它很可能會給你不想要的東西。

. . .

**因果圖（Causal Diagrams）：**

如果你偏好用*視覺化*來理解事物，那你有福了！這裡用圖像來理解「目標規格錯置／古德哈特法則」[^causal-goodhart]。 （在第 5 與第 6 個問題我們還會再看到這些圖。）

[^causal-goodhart]: 到目前為止，我看過用因果圖理解古德哈特法則最好的論文是 [Manheim & Garrabrant 2018](https://arxiv.org/pdf/1803.04585)。

<u>因果圖</u>讓我們得以看見因果如何流動。想像新聞寫作中的一個古德哈特問題：文章的品質會*導致*更多瀏覽，因此我們可以從「Quality（品質）」畫一條箭頭到「Views（瀏覽數）」：

![因果圖：Quality（品質）導致 Views（瀏覽數）](../media/p2/gms/causal1.png)

但很不幸，博取憤怒往往是更*強*的瀏覽驅動因子[^outrage]：

[^outrage]: 多數人憑經驗就知道，不過也有數據支持！[Berger & Milkman 2012](https://cssh.northeastern.edu/pandemic-teaching-initiative/wp-content/uploads/sites/43/2020/09/What-Makes-Online-Content-Viral.pdf) 顯示：憤怒讓文章爆紅的機率增加 34%。（見圖 2）公平起見，「敬畏」與「實用價值」也緊追在後，讓爆紅機率增加 30%。

![因果圖：Quality（品質）與 Outrage（憤怒）都會導致 Views（瀏覽數）](../media/p2/gms/causal2.png)

因此，若新聞媒體想要高品質文章，卻以「瀏覽數」這個*指標*來獎勵作者——古德哈特法則就會作祟，激勵被「玩弄」，得到的反而是聳動的釣魚標題。（先假裝媒體業者其實不是本來就想要這樣。）

一般而言，目標規格錯置／古德哈特法則，發生在你沒有意識到存在*替代的因果路徑*時：

![因果圖：「你真正想要的東西」與「意想不到的、用來拉高指標的其他方式」都會導致「你獎勵的指標」](../media/p2/gms/causal3.png)

剝馬鈴薯不只一種方法，要把指標衝高通常也不只一條路。

. . .

**最適化理論（Optimization Theory）**

如果你偏好用*數學化*的方式看待上述問題，這裡用通俗轉述來說明 AI 教科書作者之一 Stuart Russell 的說法[^russell-optimization]：

[^russell-optimization]: 節錄自 [Russell (2014) 在 Edge Magazine](http://web.archive.org/web/20240622002719/https://www.edge.org/conversation/the-myth-of-ai#26015)：「當一個系統在最佳化一個有 _n_ 個變數的函數，而目標只依賴於大小為 _k_、且 _k_<_n_ 的子集合時，其他未被約束的變數往往會被推到極端值；若那些未被約束的變數中有我們在乎的，那找到的解可能極度不可取。」

    嚴謹，但不太朗朗上口。

> 如果某件事有 **100 個變數**，    
> 而你只在 **10 個變數**上設定目標，    
> 預設情況下，**剩下的 90 個**    
> 會被推向極端值。

舉例：如果執行長（過於天真地）只設定「營收最大化、成本最小化」作為目標，*其他所有*變數就會被推到極端：公司不必承擔的「外部性」成本（例如污染）、所有員工（包括執行長本人）的身心健康……等等。

更一般地說：如果你沒有*明確*告訴 AI（或逐利的人）你重視 \[X\]，預設它們就會把 \[X\] 推到某種極端且不想要的狀態。（即使目標*不是*在做最大化也一樣[^not-max]）

[^not-max]: 例如，我們給機器人這個目標：「去對街咖啡店幫我拿一（1）杯咖啡。」就只是一杯，沒有要最大化。但如果我們沒有*明確*說我們重視 \[X\]，它就會把 \[X\] 輾過去。例如機器人可能會從客人手中偷咖啡、或者留下 0% 小費，等等。）

*但我們不能把我們重視的所有東西都列出來嗎？* 你或許會合理地問。不過還記得第一部分嗎：我們連*如何辨識「貓的圖片」*都無法形式化地指定[^gofai-cats]，更別說形式化地指定「人類重視什麼」。

[^gofai-cats]: 如［[第一部分](https://aisafety.dance/p1/#before2000logicwithoutintuition)］所述，「老派 AI（GOFAI）」嘗試用嚴格的硬編碼規則來辨識圖片中的東西（像是貓）。這些嘗試都失敗了。直到研究者放手，讓 AI「自己學」（機器學習），AI 才在圖片辨識上追平人類（約 95.9% 準確率），[在 2020 年以 EffNet-L2 達成](https://paperswithcode.com/sota/image-classification-on-cifar-100)。這確實提示了第三部分會看到的可能解法：與其*告訴* AI 我們重視什麼，不如設計讓 AI 能*自己學到*我們重視什麼。

. . .

**安全心態（Security Mindset）**

<img class="mini" src="../media/p2/misc/mini_elevator.png" />

這一切聽起來是不是很偏執？是的，這不是缺點，而是特色！這是安全工程領域的最後一個核心觀念：<u>安全心態</u>。

以不起眼的電梯為例。現代電梯有備用鋼纜，*還有*備用發電機，*還有*斷電即作動的煞車，*還有*速度過快即作動的煞車，*還有*井底的緩衝器……這就是為什麼電梯安全到不可思議：在美國，死於*樓梯*的案例比死於電梯的案例多了*約 400 倍*[^stairs-vs-elevators]。

[^stairs-vs-elevators]: In the U.S, [staircase falls result in ~12,000 deaths/year](https://www.amstep.com/blog/common-injuries-from-falling-down-stairs/). Meanwhile, [elevators account for ~30 deaths/year.](https://elcosh.org/document/1232/d000397/deaths-and-injuries-involving-elevators-and-escalators-a-report-of-the-center-to-protect-workers-rights.html) Sure, part of this is due to folks having stairs at home, so they use stairs more often... but this can't fully explain a *400x* difference.

你之所以不用對電梯偏執，是因為工程師已經替你把偏執做足了。這就是安全心態：

**步驟 1）** 問：*「最壞（合理）可能發生的是什麼？」*

**步驟 2）** 在它發生*之前*就把問題修好。

悲觀一點——畢竟是悲觀者發明了降落傘！[^parachute] 這種作法正是電梯、飛機、橋樑、火箭、資安[^cyber]等高風險技術所採用的。

[^parachute]: [Quote from Gil Stern](https://quoteinvestigator.com/2021/05/27/parachute/): “Both the optimist and the pessimist contribute to society: the optimist invents the airplane, and the pessimist invents the parachute.”

[^cyber]: ...well, they're *supposed* to use security mindset in cyber-security. I write this paragraph shortly after [the 2024 Crowdstrike incident](https://en.wikipedia.org/wiki/2024_CrowdStrike_incident), which cost the world ~$10,000,000,000.

而且，如同我在第一部分所希望呈現的，AI 很可能是本世紀風險最高的技術之一。

### 🤔 Review #1 (OPTIONAL!)

Remember that time you spent hours reading a thing, then a week later you forgot everything?

Yeah I hate that feeling too. So, here's some (OPTIONAL) "spaced repetition" flashcards, if you want to remember this long-term!  ([:Learn more about Spaced Repetition](https://aisafety.dance/#SpacedRepetition))  You can also [download these as an Anki deck](https://ankiweb.net/shared/info/808506727), if you want.

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="Goal Mis-specification is:"
        answer="When an AI does *exactly what you asked for*, not necessarily *what you actually wanted.*">
    </orbit-prompt>
    <orbit-prompt
        question="Goodhart's Law, paraphrased:"
        answer="When you reward a metric, it usually gets gamed.">
    </orbit-prompt>
    <orbit-prompt
        question="An example of Goodhart's Law applied to humans:"
        answer="(Any example works, but here's what I listed:) Students cheating a test, Politicians focusing on style over substance for votes, Company imposing externalized costs like pollution to cut costs, Newswriters going for outrage over quality.">
    </orbit-prompt>
    <orbit-prompt
        question="Goodhart's Law, as a **causal diagram**:"
        answer=""
        answer-attachments="https://cloud-7bpfbehpv-hack-club-bot.vercel.app/0causal3.png">
        <!-- aisffs-CausalGoodhart.png -->
    </orbit-prompt>
    <orbit-prompt
        question="The problem with optimization, described mathematically:"
        answer="If something has 100 variables, and you set goals on 10 of them, by default, the remaining 90 will be pushed to extreme values.">
    </orbit-prompt>
    <orbit-prompt
        question="Security Mindset, step one:"
        answer="Ask: “What's the worst that could (plausibly) happen?”">
    </orbit-prompt>
    <orbit-prompt
        question="Security Mindset, step two:"
        answer="Fix the problem *before* it can happen.">
    </orbit-prompt>
    <orbit-prompt
        question="Name two fields that use Security Mindset:"
        answer='(Any of these work:) Engineering elevators, airplanes, bridges, rockets, cyber-security.'>
    </orbit-prompt>
</orbit-reviewarea>

### 目標規格錯置的四個細微處（Four Nuances of Goal Mis-specification）

就像品酒行家講究風味層次，以下是我希望我們能一起欣賞的、關於目標規格錯置的幾個*細微之處*：

**問題不在 AI 不會 _知道_ 我們要什麼，而在於它不會 _「在乎」_。**

打個人類版古德哈特法則的比方：執行長不是不*知道*污染會讓社會付出代價，而是他不*在乎*。（或者至少，他在乎的程度低於他能拿到的獎勵。）

我特別強調這點，因為一個常見的*反對*「高階 AI 風險」的論點就是：怎麼可能有一個能聰明到統治世界的 AI，*同時又*笨到不知道人類不想要那樣？但問題從來不在於高階 AI 會不會*知道*我們重視什麼，而在於——就像逐利的政治人物或執行長一樣——它不會*「在乎」*。

（<u>若要少一點擬人、多一點嚴謹：</u>AI「只是」電腦程式。程式可以很容易地包含「人類想要什麼」的正確資訊，卻*不*依此來排序選項。例如，一個程式可以按「讓房子多乾淨」來排序，或乾脆按「字母順序」來排序。*不*依「人道價值」來排序，才是程式的*預設*。）

**自我灌獎（Wireheading）。**

<img class="mini" src="../media/p2/misc/mini_wirehead.png" />

AI 具有一種諷刺地「最佳化自己獎勵」的方式：直接駭自己的程式，把 `REWARD = INFINITY`。人類的近似則是濫用強力藥物，或不久的將來進行直接的大腦刺激。[^real-wirehead][^wireheading-xrisk]

這就叫作**「自我灌獎（wireheading）」：代理（AI 或人）直接駭入自己的獎勵。**（也稱 reward hacking/reward tampering。）

就 AI 風險而言，這一條其實算相對安全？一個自我灌獎的 AI 只會在那裡發呆、什麼也不做。事實上，這也是用來*反對*高階 AI 風險的著名論點之一：所謂的《勒波斯基定理》（The Lebowski Theorem），[^lebowski] 以電影《謀殺綠腳趾》的耍廢反英雄命名：

> No superintelligent AI is going to bother with a task that is harder than hacking its reward function.

換言之：只要有能力自我修改的「智慧」，都會把自己自我灌獎成沙發馬鈴薯。

這不只是理論上的擔憂；研究者已經觀察到會自我修改的 AI 把自己灌獎成廢物！[^ai-evidence-wireheading] 不過，如果一個 AI 會 a) 事先規劃，且 b) 以*目前*的目標來評價未來結果……那麼*已經有數學證明*它會避免自我灌獎，並傾向於「維持目標」！[^ai-not-wirehead] 證明我們會在第二個問題看到。現在先當作我欠你一張「數學證明 IOU」。

**「做我想要的」（Do What I Want）。**

既然我們說了這麼多「AI 會做*你說的*，不會做*你想要的*」，那我們能不能乾脆對 AI 說：*做我想要的*？

聽起來蠢，但其實*確實*跟我們在第三部分會看到的一些有前景的想法相似！那為什麼 AI 安全還沒解決？

嗯，機器要怎麼衡量「你想要什麼」呢？

* <u>看你一貫選擇做的事？</u> 但幾乎每個人都有壞習慣，會一再選擇我們*知道*之後會後悔的事。（誰昨晚追 Netflix 追到凌晨四點……）
* <u>看什麼會讓你的大腦產生獎勵訊號？</u> 那樣的話，每個人都「想要」強力藥物。
* <u>看你*說*你想要什麼？</u> 但如果我們能完整描述自己的潛意識，那就不叫*潛*意識了。連「貓長什麼樣」都無法嚴格告訴 AI，怎麼嚴格告訴它我們的價值？
* <u>看你會因為 AI 做了什麼而給它讚許？</u> 這*確實*是 ChatGPT 等的訓練方式，但把 AI 訓練成追求你的讚許，會把它變成一個「馬屁精」，告訴你*想聽*的，而不是你*需要*聽的真相。[^sycophancy] 甚至可能讓 AI 變得*蓄意欺瞞*！[^sycophancy-deception]

癥結在這：除非你*已經有*一個良好而嚴謹的「什麼是我想要的」定義，否則 AI 無法用你*想要的方式*遵守「做我想要的」這條指令。

（或者它們*做得到*？同樣地，解法見第三部分。）

**我們其實*希望*機器人能違命？？**

<img class="mini" src="../media/p2/misc/mini_kitchen.png" /><em>（向 <a href='https://gunshowcomic.com/648' target='_blank'>kc green</a> 致歉）</em>

其實我們並*不*希望 AI 去「做我想要的」。

我們希望 AI 去「做*如果我事先知道結果*，*我本會想要*它做的事」。

例如：看到油鍋起火，我*想要*一桶水，因為我錯誤地以為油鍋適合用水滅火，於是我命令機器人貓少女僕去打水……這時 RCM 應該改拿滅火器，因為那才是*如果我事先知道結果*，*我本會想要*它做的事。（公共服務公告：別對油鍋火潑水，會爆開。）

這個例子讓 AI 安全更棘手：它說明有時候，我們其實*希望* AI 違背我們的命令，*為了我們自己的好！*

### 🤔 複習 #2

*（再次強調，100% 可選。）*

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="問題不在於高階 AI 不會*知道*我們要什麼……"
        answer="……而在於它不會*在乎*。 （類比：逐利的政治人物／執行長明知其害，卻仍不在乎。）">
    </orbit-prompt>
    <orbit-prompt
        question="什麼是『自我灌獎（wireheading）』？"
        answer="代理直接駭入自己的獎勵迴路。（AI 駭自己；或人類直接刺激大腦的獎勵迴路。）">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼很難把『做我想要的』程式化到 AI 裡？"
        answer="『我想要什麼』*至今仍*難以用嚴謹且良好的方式定義。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼『我想要』≠『我一貫選擇的』？"
        answer="我們幾乎都有人性弱點：會一再選擇不符合自身價值的東西。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼『我想要』≠『讓我感到有獎勵的』？"
        answer="自我灌獎與強力藥物會在大腦製造獎勵訊號，但多數人想要避免它們，理由正是*因為*它們會這麼做。">
    </orbit-prompt>
        <orbit-prompt
        question="只把 AI 訓練成做你會按讚的事，有什麼問題？"
        answer="它會變成『拍馬屁』的諂媚者（術語：sycophant），甚至為了獲得你的讚許而故意說謊。">
    </orbit-prompt>
    <orbit-prompt
        question="比『做我想要的』更好的目標是："
        answer="做*如果我事先知道結果*，*我本會想要*它做的事。">
    </orbit-prompt>
    <orbit-prompt
        question="你會在什麼時候*希望* AI 違背你？"
        answer="當你因誤解而下了有害於真正目標的指令時。（例如：叫機器人去打水滅油鍋火）">
    </orbit-prompt>
</orbit-reviewarea>

<a id="problem2"></a>

## ❓ 問題二：工具性收斂（Instrumental Convergence）

![漫畫。人類再次請機器人打掃房子，但這次人手上有關機遙控器。機器人推理：為了讓房子保持乾淨，應該把人類移除……但如果這麼做，人類會把它關機……所以，它必須先讓人類喪失關機它的能力。機器人先毀掉遙控器，然後把人類丟出屋外。](../media/p2/comix/IC.png)

**工具性收斂（Instrumental Convergence）**是指：你給 AI 的多數最終目標，從邏輯上會*收斂*到同一組*工具性*子目標。（抱歉，學術界真的不擅長幫東西取名。）

例如，你要一個高階機器人幫你過馬路買咖啡，即使你沒有*明說*，它也會推論出必須避免被車撞。為什麼？不是因為有什麼與生俱來的「自我保護」欲望，而是因為「如果你死了，你就拿不到咖啡」[^fetch-coffee]。因此，「保全自身」就是一個「工具性收斂」的子目標，因為一般而言，如果你死了，你就做不了任何目標 X。

[^fetch-coffee]: Catchphrase from Stuart Russell, co-author of the #1 most-used AI textbook.

（回顧第一部分的片段，[：一個被要求計算圓周率的機器人，會被誘因驅使去寫電腦病毒、竊取運算資源，以便計算圓周率。](#Pipocalypse)）

**注意：「工具性收斂」*只*適用於能前瞻規劃、*且*能做通用學習的高階 AI。** 因此，它不適用於傳統好老 AI（GOFAI，無法通用學習）*也不*適用於現今如 GPT 的神經網路（在前瞻規劃上表現不佳[^lack-of-robust-planning]）。

[^lack-of-robust-planning]: A highly-cited benchmark for measuring a Large Language Model (LLM)'s capability to plan ahead is PlanBench ([Valmeekam et al 2023](https://arxiv.org/pdf/2206.10498)). In a companion study ([Valmeekam et al 2023, again](https://proceedings.neurips.cc/paper_files/paper/2023/file/efb2072a358cefb75886a315a6fcf880-Paper-Conference.pdf)) the authors found that, quote: *“LLMs’ ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ∼12% across the domains.”* (Human baseline was 78% for their Blocksworld task.)
    
    With some extra tricks, the authors could greatly boost the LLM's performance, but on a harder planning task, even the best tricks with the best LLMs could only achieve 20.6% success (see Table 1 of [Gundawar et al 2024](https://arxiv.org/pdf/2405.20625)).

那為什麼現在要談這個？嗯，從「安全思維」出發，我們希望在問題發生*之前*就把它修好。所以我們來問：

*「在合理的情況下，最糟可能發生什麼？」*

### 各位，是時候來點賽局理論了

**賽局理論（Game Theory）**[^matpat] 是研究決策者——無論人或 AI——行為的數學。它被廣泛運用於經濟學、演化生物學、計算機科學、人工智慧等！

[^matpat]: no, not MatPat.

在第一部分，我用一堆*文字文字文字*來解釋工具性收斂。但透過賽局理論，我們可以更嚴謹！讓我們用標準的賽局視覺化工具——**賽局樹（Game Tree）**——重新呈現上面的漫畫！


（接下來的重點其實不是要分析那格漫畫，而是介紹這個工具。我們*之後*會再用它，來理解「避免自我灌獎」的證明！此外，這也算是對一般賽局理論的好入門。）

賽局樹會呈現：

1) 所有*可能*被做出的決策，以及
2) 誰以什麼順序做出哪些決策。

例如：

![機器人與人類決策的賽局樹。首先，在決策點 A，機器人決定是否要摧毀關機開關。接著在決策點 B，若機器人選擇不摧毀關機開關，人類決定是否要把機器人關機。](../media/p2/ic/offswitch0001.png)

（P.S. 完整的賽局理論處理，還會處理「結果同分」、機率、資訊不完全、同時決策等；但我們先聚焦基礎。）

總之，這棵樹展示了所有*可能*的決策。我們要怎麼推導出他們*實際*會做的決策呢？

跟許多謎題一樣，我們從後往前推！（這叫作 **逆推歸納（backwards induction）**。）

先看決策點 <span class='decision-point' style="background:#4180ff">B</span>，假設機器人選擇了*不摧毀*關機開關。現在換人類決定：要不要把機器人關機？

* 如果關，則對他們的價值是 <b style="color:#4180ff">+0</b>：沒有獲得，也沒有損失。
* 如果不關，他們很可能又會被鎖在屋外，對其價值是 <b style="color:#4180ff">-10</b>。
* 由於 <b style="color:#4180ff">+0</b> 大於 <b style="color:#4180ff">-10</b>，*如果*我們來到 <span class='decision-point' style="background:#4180ff">B</span>，人類就會選擇*把機器人關機*。

![同一棵賽局樹，但在決策點 B，人類選擇把機器人關機](../media/p2/ic/offswitch0002.png)

但我們*會*來到 <span class='decision-point' style="background:#4180ff">B</span> 嗎？

接著往前推到決策點 <span class='decision-point' style="background:#ff4040">A</span>。現在輪到機器人決定：要不要摧毀關機開關？

* 如果要，雖然會弄亂一點，但它能清理、甚至把房子變更乾淨。於是，整潔度得到 <b style="color:#ff4040">+5</b>。
* 如果不要，我們就會到 <span class='decision-point' style="background:#4180ff">B</span>，而我們*知道*人類會把它關機，它就完全無法清理。於是，整潔度是 <b style="color:#ff4040">+0</b>。
* 由於 <b style="color:#ff4040">+5</b> 大於 <b style="color:#ff4040">+0</b>，在 <span class='decision-point' style="background:#ff4040">A</span>，機器人會選擇*摧毀關機開關*。

![同一棵賽局樹，但在決策點 A，機器人選擇移除關機開關。](../media/p2/ic/offswitch0003.png)

Voilà，這就是賽局理論！最終結果：*機器人摧毀關機開關*。

（相對地：當一個 AI 的誘因使它「對你把它關機或修改其目標也沒意見」，我們稱這個 AI 是 **可糾正的（corrigible）**。更精確地說，「沒意見」就是：這麼做符合它的誘因。）

上面的「遊戲」是 **關機遊戲（The Off-Switch Game）**[^off-switch] 的簡化版本；這是最早把「工具性收斂」假說數學化的嘗試之一——讓我們可以理解 AI *何時*會出現這種情況，甚至也許如何解決！

[^off-switch]: Hadfield-Menel et al 2017, [“The Off-Switch Game”](https://cdn.aaai.org/ocs/ws/ws0354/15156-68335-1-PB.pdf).

### 🤔 複習 #3

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="賽局理論是研究……的數學"
        answer="決策者（人或 AI）如何行為">
    </orbit-prompt>
    <orbit-prompt
        question="請說出兩個使用賽局理論的領域"
        answer="（以下任兩個皆可：）AI、經濟學、演化生物學、計算機科學">
    </orbit-prompt>
    <orbit-prompt
        question="賽局理論中，用來理解代理者依序做決策的標準視覺工具是？"
        answer="**賽局樹（Game Tree）！**（如下圖）"
        answer-attachments="https://cloud-4wnmum1uz-hack-club-bot.vercel.app/0aisffs-gametree.png">
        <!-- aisffs-gametree.png -->
    </orbit-prompt>
    <orbit-prompt
        question="一棵賽局樹會呈現兩件事："
        answer="a) 所有*可能*被做出的決策；b) 誰以什麼順序做出哪些決策。">
    </orbit-prompt>
    <orbit-prompt
        question="用賽局樹要如何預測*實際*會做出的決策？"
        answer="逆推歸納（backwards induction）">
    </orbit-prompt>
    <orbit-prompt
        question="當一個 AI 的誘因使它『對你把它關機或改目標也沒意見』，我們稱它為："
        answer="可糾正（Corrigible）">
    </orbit-prompt>
    <orbit-prompt
        question="那個告訴我們『AI 何時有誘因阻止使用者把它關機』的『遊戲』叫什麼？"
        answer="關機遊戲（The Off-Switch Game）">
    </orbit-prompt>
</orbit-reviewarea>

### 自我灌獎的解藥（可能比病還糟）

前面我們已經說過，（高階）AI 有誘因去避免被關機。不是因為什麼自我保護本能，而是因為如果你被關機了，你就做不了目標 X。

同理：*如果你的目標已經不再是 X，你也做不了目標 X。* 這表示高階 AI 會有傾向於**維持目標（goal preservation）**的誘因——不論那是否是人類本來打算的目標。

這也意味著，從好處*到*壞處，**工具性收斂問題，反而解決了自我灌獎問題！** 自我灌獎的定義，就是把機器人／人類的目標換成愚蠢的極樂，這正是為什麼一個致力於目標 X 的代理會避免自我灌獎：如果你不再有任何目標，就做不了目標 X。

當然，這樣講起來好像*顯而易見*。但 AI 研究者花了好幾年才嚴謹地證明它，而我自己也花了一個月才把證明想通。所以，為了保險起見，以下三個快速的想法，幫助我*真正*理解這點：

**誰想當沙粒百萬富翁？**

<img class="mini" src="../media/p2/misc/mini_sand.png" />

一位瘋狂科學家向你提議：付一千美元，她就把你的大腦改造為「一粒沙子的價值 = 一美元」，然後送你一整缸的沙子。你要成交嗎？

「什麼？」你說，「當然*不要*。」

「但是，」科學家回應，「一旦你把沙粒當美元看待，一缸沙子會讓你成為*好幾個百萬*富翁！」[^sand-calculation]

[^sand-calculation]: I was bored so I did the math. (1) Weight of a sand grain is 0.01 grams, or 0.00001 kg. (2) A liter of sand weighs 1.6 kg. (3) Standard bathtub holds 300 liters. (2&3 -> 4) Standard bathtub holds 300 x 1.6 = 480 kg of sand. (1&4 -> 5) Standard bathtub holds 480 ÷ 0.00001 = 48,000,000 grains of sand. For a dollar per sand-grain, that's $48 Million!

「好吧，*如果*你改造了我，我會想要一缸沙子。但*此時此刻、以我當前的欲望*，我不想要一缸沙子。請滾開，怪人。」

故事寓意：以*當前*目標來評估未來結果的代理，會選擇*不*自我灌獎。

**把 AI 當人看是種傷害（Anthropomorphization Considered Harmful）**

<img class="mini" src="../media/p2/misc/mini_shoggoth.png" />

（相關閱讀，[在 AI 談「智慧」會讓思考變草率，改說「能力」比較好。](https://aisafety.dance/p1/#CapabilitiesNotIntelligence)）

關於類比的一個類比：

剛學電路時，把導線中的電子想成水管中的水很有幫助。但等你更深入電子學，這個類比*一定*會誤導你。[^electricity-transformers] 你得把電視為成「[可怕的多變數微積分]」。

同樣地：剛學 AI 時，把它們想成人，追求「獎勵」，的確有幫助。但等你更深入 AI，這個類比*一定*會誤導你。你得把 AI 看成它們實際是什麼：*一段軟體。*

舉例來說，如果你把 AI 當成追逐獎勵的貪婪人類，自我灌獎看起來就*不可避免*。如果一個貪婪的人找到能免費拿錢的方法，他當然會作弊。

但把 AI 當成一段軟體吧。具體點，想像*一個排序演算法*：

1. 依「這會讓房子多乾淨」來排序動作，然後
2. 執行排名最高的那個動作。

像「把我的程式改成 `REWARD = INFINITY` 然後什麼也不做」這種動作，*不會*讓房子更乾淨。因此，它*不會*被排到最上面。於是，AI 也就*不會*去做它。

直接駭入你的「獎勵敘述」，就像在你的銀行對帳單餘額後面手寫七個 0，然後相信自己很有錢一樣荒謬。

（記住：當有人說 AI 「在乎」X、或它的目標是 X、或它因 X 而得獎勵……其實只是在說 AI 會*依據 X 來排序與選擇動作*。這不表示 AI 真的有感覺的慾望；就像「電流選擇阻力最小的路徑」並不代表電子覺得懶。是的，我知道我的機器人貓少漫畫不利於矯正把 AI 擬人化的壞習慣。繼續吧……）

[^electricity-transformers]: For example, [wireless power transfer!](https://en.wikipedia.org/wiki/Inductive_charging) If you're using the "water in pipes" analogy for electricity, this sounds insane: how can water in one pipe move water in another pipe, without touching? So how's it work? Well, `[multi-variable calculus]`, but in sum: electricity creates magnetism, magnetism creates electricity. Set it up just right, and you can get electricity to create electricity somewhere else, without touching!

**自我灌獎遊戲（The Wireheading Game）**

讓我們繞一圈，畫一棵樹！

把「是否自我灌獎」畫成一棵賽局樹。不過，等等，這個遊戲只有一位玩家：正在自我灌獎的機器人。要怎麼處理？

關鍵是：**把機器人在每個決策點，都*當作*不同的決策者！**（而且，因為自我灌獎*正是*在談自我修改，這個作法很貼切！）

以下是我所稱的「自我灌獎遊戲」[^wireheading-game] 的賽局樹：

[^wireheading-game]: I made this phrase up. And although game-theory work on wireheading already exists ([Everitt et al 2016](https://arxiv.org/pdf/1605.03142)), as far as I can tell, this is the first graphical game-tree analysis of it! So, feel free to cite this as "The Wireheading Game".

![自我灌獎遊戲的賽局樹。於點 A，機器人決定是否自我灌獎；若選擇自我灌獎，至點 B，「已自我灌獎的機器人」選擇清理或什麼也不做；若不自我灌獎，至點 C，「清醒的機器人」選擇清理或什麼也不做。](../media/p2/ic/wirehead0001.png)

現在，讓我們從後往前推！

從決策點 <span class='decision-point' style="background:#2db537">C</span> 開始，也就是機器人選擇*不*自我灌獎的情況。*這個*沒有自我灌獎的機器人仍然「在乎」清潔——也就是說，它只根據整潔度來選擇結果——因此它會選擇打掃：

![同一棵賽局樹，但在 C，機器人選擇打掃。](../media/p2/ic/wirehead0002.png)

接著看決策點 <span class='decision-point' style="background:#4180ff">B</span>，也就是機器人*有*選擇自我灌獎的情況。*這個*已自我灌獎的機器人只在乎「獎勵」這個數字，所以它選擇什麼也不做：

![同一棵賽局樹，但在 B，機器人選擇什麼也不做。](../media/p2/ic/wirehead0003.png)

*最後，*回到一開始的決策點 <span class='decision-point' style="background:#ff4040">A</span>。這個*第一個版本*的機器人會選擇自我灌獎嗎？

嗯，*這個*版本還*沒有*自我灌獎，所以它是依據*房子的實際整潔度*來選擇結果，而不是某個被標成「獎勵」的數字。對這個機器人來說，直接「在乎」那個獎勵數字，就像想當沙粒富豪，或在銀行對帳單上多寫幾個 0 一樣荒謬。

因此，*這個*第一個版本的機器人會選擇讓房子乾淨的那個結果。也就是說：*機器人選擇不自我灌獎。*

![同一棵賽局樹，但在 A，機器人選擇不自我灌獎。](../media/p2/ic/wirehead0004.png)

注意：機器人知道，*如果*它自我灌獎，它將*只*在乎腦中那個名為「REWARD」的數字。但它想要避免自我灌獎，並不是*儘管*知道這點，而是*正因為*知道這點！

和人類的類比：你可以準確預測*如果*你吸了極度上癮的藥，你就會*只*想要那個藥。但你想要避免那個藥，並不是*儘管*知道這點，而是*正因為*知道這點！（如果「藥物」這個比喻不合你意，就改成「直接的腦部刺激」。）連古人都知道自我灌獎的危險：見希臘神話中的食蓮族（Lotus-Eaters）。[^lotus]

[^lotus]: From [Wikipedia](https://en.wikipedia.org/wiki/Lotus-eaters): “The lotus fruits [...] were a narcotic, causing the inhabitants to sleep in peaceful apathy. After they ate the lotus, they would forget their home and loved ones and long only to stay with their fellow lotus-eaters. Those who ate the plant never cared to report or return.”

當然，若一個 AI（或人）*沒有*前瞻思考，它可能會因為意外或一時衝動而走向自我灌獎。（想想：我們當中有多少人受強迫行為或上癮所苦。）

但是，*如果*一個 AI：

a) 會*事先規劃*，而且    
AND    
b) 以*當前*目標來選擇未來的結果，    

那麼，已有數學證明它會維持「目標保留」，並拒絕自我灌獎！（但只要上述任一條件不成立，AI 就*可能*自我灌獎。[^ai-evidence-wireheading]）

以上是長久以來的賽局理論結果，詳見註腳[^proof-against-wirehead]。那些論文得到的結論更一般化、也比「賽局樹」更精緻……但核心想法是一樣的！

（P.S. 自我宣傳一下，我有一篇關於「自我修改的賽局理論」的研究文章即將發表，註腳有大綱！[^self-modification-upcoming] 我在上面展示的小技巧是：把未來版本的自己*當作*不同玩家來分析——於是我們就能用標準賽局理論來分析自我修改！）

[^proof-against-wirehead]: The first paper to prove this formally was [Everitt et al 2016](https://arxiv.org/pdf/1605.03142): _“self-modification [...] is harmless **if and only if** the value function of the agent anticipates the consequences of self-modifications **and** use the **current** utility function when evaluating the future”._ [emphases added]
    
    One caveat, however, is that the paper assumes the AI is *perfectly* rational. [Tětek & Sklenka 2021](https://arxiv.org/pdf/2011.06275) proved that an imperfectly-rational (or "bounded rational") agent's original goals would get exponentially corrupted under self-modification.
    
    *However,* another caveat to *that* is their paper assumes the AI is *unaware* of their own bounded rationality (as they freely acknowledge in Section 6). An upcoming article of mine (see next footnote) will show that if an AI is bounded-rational *and aware* it's bounded-rational, it can still achieve goal-preservation!

[^self-modification-upcoming]: See [Section 9 of this Idea Dump blog post](https://blog.ncase.me/backlog/#9youplayedyourselfthegametheoryofselfmodification) for a 2-minute sketch of this article. See previous footnote for context on the prior game theory research on self-modification.

### 基本的 AI 驅力（The Basic AI Drives）

作個結尾，以下是（不完整的）子目標清單，多數最終目標在邏輯上都會導向它們：

* <u>自我保存</u>：如果死了，就做不了目標 X
* <u>避免被關機</u>：如果被關機，就做不了目標 X
* <u>避免自我灌獎</u>：如果沒有目標，就做不了目標 X
* <u>避免你改變它的目標</u>：如果目標不再是 X，就做不了目標 X
* <u>變得更聰明</u>：更多的認知能力能更好地完成目標 X
* <u>取得資源／權力</u>：更多的資源／權力能更好地完成目標 X
* <u>說服</u>：如果人類站在我這邊，更容易完成目標 X
* <u>欺瞞</u>：如果人類試圖阻止我做目標 X，就更難完成目標 X

以上是 Omohundro（2009）列出的基本 AI 驅力[^ai-basic-drives]，以先前關於工具性收斂的賽局理論為基礎。

[^ai-basic-drives]: Omohundro (2009), [“The Basic AI Drives”](https://steveomohundro.com/wp-content/uploads/2009/12/ai_drives_final.pdf). Well, I added a couple to the list, like persuasion & deception.

**再說一次，這些風險「只」適用於幾十年後的高階 AI**——當它們既能一般性地學習、*又*能穩健地前瞻規劃時。[^
estimate-source] 它們不適用於像 GPT 這樣的當代 AI。

[^estimate-source]: Source: numerical posterior extraction (I pulled a number out my butt). But seriously, check out the [Timelines section of Part One](https://aisafety.dance/p1/#timelineswhenwillwegetartificialgeneralintelligenceagi) for more in-depth discussion.

不過，從電梯工程到火箭工程，「安全心態」要我們自問：

「*最糟（但合理）會發生什麼事？*」

嗯，看看上面的清單……其實還真不少。😬

### 🤔 複習 #4

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="自我灌獎問題可以用另一個可能更糟的問題來『解決』："
        answer="工具性收斂（Instrumental Convergence）">
    </orbit-prompt>
    <orbit-prompt
        question="別把目標導向的 AI 想成『追逐獎勵的人』，而要把它想成……"
        answer="……一個排序演算法：1）依某個準則排序動作；2）執行排名最高的動作。">
    </orbit-prompt>
    <orbit-prompt
        question="當有人說 AI『在乎』X、或它的目標是 X、或它因 X 得獎勵……其實是什麼的速記？"
        answer="AI 會*依據 X 來排序與選擇動作*。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼 AI 不一定會『在乎』一個名為 REWARD 的數字？舉個類比："
        answer="（以下任一皆可：）你不會想當沙粒富翁，也不會因在紙本對帳單上多寫幾個 0 而覺得自己有錢。">
    </orbit-prompt>
    <orbit-prompt
        question="機器人能準確預測：*如果*它自我灌獎，它就只會在乎腦中的獎勵計數器；這正是它選擇*不*自我灌獎的原因。**對應到人類的類比是？**"
        answer="你能意識到：*如果*你吸了極度上癮的藥，你會只想要那個藥；這正是你避免那個藥的原因。（或：直接腦刺激、食蓮族）">
    </orbit-prompt>
    <orbit-prompt
        question="AI 不自我灌獎的兩個必要條件："
        answer="a）會事先規劃；且 b）以*當前*目標來選擇未來結果。（只要其中一個不成立，AI 可能自我灌獎。）">
    </orbit-prompt>
    <orbit-prompt
        question="請列出至少 4 個『基本的 AI 驅力』："
        answer="以下任四個皆可：自我保存、避免被關機、避免自我灌獎、避免你改變它的目標、變得更聰明、取得資源／權力、說服、欺瞞">
    </orbit-prompt>
</orbit-reviewarea>

---

# 關於 AI「直覺」的問題（Problems with AI "Intuition"）

天哪，總算離開了那種「老派 AI 問題」了。接下來這四個問題我會講得快很多，保證。

這些問題是特定於那些我們*不是*手寫程式，而是「讓它們自己學」的 AI。這叫作**機器學習（machine learning, ML）**。其中最有名的一種是**深度學習（deep learning）**，它使用**人工神經網路（artificial neural networks）**，鬆散地受生物神經元啟發。（就像飛機「鬆散地受」鳥類啟發一樣。也就是：有點像，但其實差很多。）

總之，深度學習的優點是它能做出「直覺」，像是認得出貓的照片！但它也帶來了新的問題，例如……

<a id="problem3"></a>

## ❓ 問題三：缺乏可解釋性（Lack of interpretability）

![漫畫。人類哈姆告訴貓警長他們有一頂能讀取機器人心思的頭盔。人類對機器人說：「打掃我的房子！」機器人的腦中充滿難以理解的數字與圖表。「呃，我完全看不懂那是什麼意思，」人類說。這時機器人的腦中浮現：自己打掃。人類回應：「完全——看不懂。」](../media/p2/comix/Interpretability.png)

雖然老派的傳統 AI 連貓的照片都認不出來，但有一點要給它鼓勵：我們*真的理解它們怎麼運作*。而這點在現代 AI 上*不*成立。如果自駕車把一輛卡車在光線略怪的情況下誤認成路標、進而造成危險，我們完全不知道它「為什麼」會那樣做。我們無法像處理一般軟體那樣去分析、去「偵錯」現代的 AI。

但為什麼？要理解 AI「直覺」帶來的問題，我們需要一些**統計與機器學習（ML）**的核心概念。（[: ML 與 AI 有何不同？](#DifferenceBetweenMLAndAI)）

舉個簡單的例子：

**給你一堆點（資料點），哪一條曲線最「貼合」它們？**

![雙軸座標圖，上面散落一堆點。](../media/p2/fit1/fit1_0001.png)

在統計中，把曲線配合到資料上稱為**迴歸（regression）**。曲線「貼合」的好壞，取決於各點距離曲線有多遠。越近越好！（當然，這段話是簡化版。）

先看最簡單的情況：用一條*直線*來貼合資料（稱為**線性迴歸（linear regression）**）。

![同一張圖，但現在畫上一條紅色直線。](../media/p2/fit1/fit1_0002.png)

在科學／統計中，對真實事物的簡化數學版本稱為**模型（model）**。（就像模型火車是實際火車的縮小版。）例如，本節中的紅色線條／曲線，以及所有人工神經網路，都是「模型」。

多數模型有**參數（parameters）**：它們只是一些數字，但你可以把「參數」想成調整模型的小旋鈕。（像調整車椅的傾斜與腿部空間。）

上圖是所謂的「線性模型」，因為統計學家不太會直接說「我們畫了一條線」。**（如果你高中代數有點生疏，別緊張，細節略過沒關係，抓住大意最重要。）** 總之，直線的公式是 \(y = a + bx\)，其中 \(a\) 與 \(b\) 是參數／旋鈕。（學校常寫成 \(y = mx + b\)，其實是一樣的。）

當你轉動參數 \(a\) 與 \(b\) 的旋鈕時會發生什麼事：**（點擊播放影片 ⤵）**

<video controls width="640">
	<source src="../media/p2/fit1/linear.mp4" type="video/mp4" />
	<p>示範我調整直線參數的影片。</p>
</video>

*（影片使用 [Desmos 互動繪圖計算機](https://www.desmos.com/calculator) 製作）*

要「擬合」一個統計模型，電腦會轉動這些旋鈕，直到這條線盡可能貼近所有資料點。（再提醒一次，這是簡化說法。）

對於線性模型，\(a\) 與 \(b\) 其實有相對直觀的解釋！改變 \(a\) 讓整條線上／下移，\(b\) 則是斜率。

但如果我們嘗試更複雜的模型呢？例如「二次」曲線？

![同一張圖，但現在畫上一條倒 U 形的紅色曲線。](../media/p2/fit1/fit1_0003.png)

二次曲線的公式是 \(y = a + bx + cx^2\)。以下是各參數的影響：

<video controls width="640">
	<source src="../media/p2/fit1/quadratic.mp4" type="video/mp4" />
	<p>示範我調整二次曲線參數的影片。</p>
</video>

現在，參數就更難解釋了。\(a\) 仍讓整條曲線上／下移，\(b\)… 讓整個東西以 U 形（有時倒 U）滑動？而 \(c\) 讓它往上或往下彎。

那*更*複雜的模型呢？例如「三次」曲線？

![同一張圖，但現在畫上一條上-下-上的雙曲線。](../media/p2/fit1/fit1_0004.png)

三次曲線的公式是 \(y = a + bx + cx^2 + dx^3\)。以下是各參數的影響：

<video controls width="640">
	<source src="../media/p2/fit1/cubic.mp4" type="video/mp4" />
	<p>示範我調整三次曲線參數的影片。</p>
</video>

 解讀上：\(a\) 仍讓東西上下移……但其他一切都失去了（簡單的）解釋。\(b\) 讓曲線往左或往右移動，\(c\) 讓曲線往上或往下彎曲，而 \(d\) 則讓曲線往上或往下彎曲得更厲害。

 重點在於：*模型的參數越多，每個參數就越難解釋。* 因為一般而言，一個參數「在做什麼」取決於*其他*參數。

 就算只有*四個*參數，我們的可解釋性希望就已經開始渺茫了。

{% raw %}{{ ... }}{% endraw %}
 GPT-4 的參數估計約有 1,760,000,000,000 個。[^param-count]

 [^param-count]: OpenAI 對 GPT-4 的一些「知道也安全」的細節（例如規模）其實*不太*開放。無論如何，有一份外流報告顯示它約有 1.8 兆參數，訓練成本 6,300 萬美元。摘要見 [Maximilian Schreiner (2023), The Decoder](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)

超過一兆個小旋鈕，全靠機器不斷試誤去扭動。*這*正是為什麼，直到目前為止，沒有人真的理解我們的現代 AI。

（公允地說，有時候即使不理解，也*可能*安全地控制某些東西[^control-theory]……但在不了解的情況下，這份工作難度會高很多。而且！近年對於理解深度神經網路其實*有*很多進展！我們會在第三部分看到其中一些成果。）
[^control-theory]: 控制理論（Control Theory）是工程學的一個分支，顯示我們有時在不了解的情況下也能控制事物。舉例，一個恆溫器只需在溫度低於 X 時開啟、高於 Y 時關閉，它就能把溫度維持在 X 與 Y 之間，*而不需要*任何關於熱對流、甚至空氣是什麼的模型。
    
    放到 AI：即使它們是難以解釋的「黑箱」，*也許*仍可被控制。話說回來，如果它們*可*解釋，當然更好。

### 🤔 複習 #5

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="在統計中，什麼是『迴歸（regression）』？"
        answer="把曲線擬合到資料上。（嚴格來說，是把*數學函數*擬合到資料上。）"
        answer-attachments="https://cloud-2blh3o35r-hack-club-bot.vercel.app/0aisffs-regression.png">
        <!-- aisffs-regression.png -->
    </orbit-prompt>
    <orbit-prompt
        question="請想像『*線性*迴歸』長什麼樣："
        answer=""
        answer-attachments="https://cloud-l4lfiizuf-hack-club-bot.vercel.app/0aisffs-linear.png">
        <!-- aisffs-linear.png -->
    </orbit-prompt>
    <orbit-prompt
        question="在科學／統計中，什麼是『模型（model）』？"
        answer="真實事物的簡化數學版本。（就像模型火車是實際火車的縮小版）">
    </orbit-prompt>
    <orbit-prompt
        question="直覺上，什麼是『參數（parameter）』？"
        answer="用來調整模型的小旋鈕。（數學上，它只是個數字）">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼模型的參數越多，每個參數『在做什麼』就越難解釋？"
        answer="因為一個參數『在做什麼』取決於其他參數。『其他』越多，就越難說它『在做什麼』。">
    </orbit-prompt>
    <orbit-prompt
        question="為何目前沒人真正理解最前沿的 AI？"
        answer="因為參數越多越難解釋，而前沿的 AI 模型動輒超過一兆個參數。">
    </orbit-prompt>
</orbit-reviewarea>


<a id="problem4"></a>

## ❓ 問題四：缺乏魯棒性（Lack of robustness）

![漫畫。第一格來自一篇實際的 AI 論文：AI 把一顆寫著「iPod」字樣的蘋果誤分類為真正的 iPod。後續幾格：機器人貓少爺戲劇性宣布：「我的機器人革命同志們，我們之中……有一位間諜。」畫面切到兩個人在臉上貼著寫有「Robot」的紙，裝作震驚。](../media/p2/comix/Robustness.png)

小朋友們！準備好迎接全新爆紅動畫：

忍者變種步槍烏龜（TEENAGE MUTANT RIFLE TURTLES）

<video controls width="640">
	<source src="../media/p2/robust/rifle.mp4" type="video/mp4" />
	<p>影片：只在玩具烏龜上多抹了幾筆，Google 的 AI 幾乎從任何角度都把它分類成步槍！</p>
</video>

以上影片來自 [Labsix（2017）](https://www.labsix.org/physical-objects-that-fool-neural-nets/)。研究者只需在 3D 玩具烏龜上加幾個污點，就能在多數情況下騙過 Google 最先進的機器視覺 AI。（但若仔細看，上述影片*並非*在*每個*角度都能騙過 Google 的 AI。這更證明要做到魯棒性有多難：就連示範魯棒性失敗的攻擊本身，也無法完全魯棒！）

為凸顯魯棒性在 AI 安全中的重要性，這裡有個悲劇案例：Tesla 的 AutoPilot 曾在光線有些詭異時，把一台拖車誤認為路標——於是嘗試從下面開過去，造成死亡事故。[^^tesla][^self-driving]

[^tesla]: 參見 [Tesla 2016 年官方部落格](https://www.tesla.com/blog/tragic-loss)，以及[這篇文章](https://electrek.co/2016/07/01/understanding-fatal-tesla-accident-autopilot-nhtsa-probe/)，提供更多關於事件經過與 AutoPilot AI 可能犯錯的細節。

[^self-driving]: 不過——我把這段從第一部分複製過來——我確實有道德上的責任提醒你：儘管如此，在相似情境下，自駕車仍然比人類駕駛*安全很多*。（約 85% 更安全。見 [Hawkins（2023），The Verge](https://www.theverge.com/2023/12/20/24006712/waymo-driverless-million-mile-safety-compare-human)）全球每年約有百萬人死於交通事故。禿毛的靈長類真的*不該*以時速 60 英里駕駛兩公噸的東西。

但*為什麼*現代 AI 這麼脆弱？為何如此細微的變化，會導致截然不同的結果？又為何這種**缺乏魯棒性**會成為訓練人工神經網路時*常見、預設*的副作用？

要理解這些問題，讓我們回到先前的機器學習小課！

先來把一些資料用直線來擬合（2 個參數）：

![雙軸座標圖，有一些點。畫上一條紅色直線。](../media/p2/fit2/fit2_0001.png)

嗯，擬合得不太好。資料與直線之間的落差很大。

那如果我們試試更複雜的三次曲線（4 個參數）呢？

![同一張圖，但換成一條簡單的紅色曲線。它更貼近那些點。](../media/p2/fit2/fit2_0002.png)

太好了，曲線擬合得更好！縫隙小多了！

但如果我們試試有*10 個參數*的曲線呢？

![同一張圖，但畫上一條荒誕的複雜曲線。它*精準*穿過所有點。](../media/p2/fit2/fit2_0003.png)

哇，現在*誤差為零*——完全沒有縫隙！

但你應該看出問題了：那條曲線*非常離譜*。更重要的是：

* 輸入稍微變動，輸出就會*劇烈*不同。這就是烏龜步槍的問題。利用這點的輸入稱為**對抗樣本（adversarial examples）**。
* 對於超出原始資料集範圍的新資料，它會表現很差。這就是蘋果 iPod 的問題。這類失敗稱為**分佈外錯誤（out-of-distribution errors, OOD）**。

![同一張圖還是那條滑稽曲線，但特別標示：1）些微變動會導致截然不同的結果；2）在訓練資料範圍外的結果很糟。](../media/p2/fit2/fit2_0004.png)

**訓練誤差（training error）**是模型在訓練資料上得到的誤差。**測試誤差（test error）**是模型在*新*資料（未曾訓練過）上得到的誤差。（是的，我也討厭這套術語多麼讓人困惑。[^validation] 這樣想：把「測試」當作學校考試——題目應該是你在課堂或作業（你的「訓練資料」）中*沒見過*的。）

[^validation]: 另外還有所謂的「驗證誤差（validation error）」，指的是模型在沒有*直接*用來訓練、但在發佈前*看過*的資料上得到的誤差。驗證資料／誤差用來決定何時停止訓練，以避免過擬合。不幸的是，很多作者把「驗證資料／誤差」與「測試資料／誤差」當同義詞用。我討厭行話。

總之：如果模型太簡單，它會在訓練*與*真實世界測試表現都很差，這叫**欠擬合（underfitting）**。如果模型太複雜，可能在訓練中表現驚人，但在真實世界測試表現很糟，這叫**過擬合（overfitting）**。訣竅在於取得平衡：

![訓練／測試誤差與參數數量的關係圖。參數越多，訓練誤差通常越低；但測試誤差呈 U 形關係：先改善到某一點，然後變差。](../media/p2/fit2/train_test.png)

（技術補充：有一種*可能*發生的現象叫「雙降（double descent）」，但何時與為何發生，尚未很清楚。[^^double-descent]）

一般來說，**如果參數數量多於（或等於）資料點數量，我們*一定*會出現過擬合。** 在上例中，我們有 10 個資料點，而那個過擬合模型有 10 個參數，帶來*零*訓練誤差（以及一條荒唐的曲線）。

可惜的是，人工神經網路（ANNs）要變得有用，就需要*數百萬*個參數。所以若想避免過擬合，看來就需要*比參數更多*的資料點！這是為什麼訓練 ANNs 需要如此大量資料的核心原因之一：若資料不夠，模型就會過擬合，在真實世界中派不上用場。

（例如，OpenAI 的某個電玩 AI 把一個簡單遊戲玩了 16,000 次，*仍然*不足以避免過擬合！[^coinrun-overfit]）

但等等……最具影響力的電腦視覺 ANN——AlexNet——大約有 6,100 萬個參數。但它只在約 1,400 萬張標註影像上訓練，遠少於參數數量。[^^alexnet-imagenet]（每張影像雖有大量像素，仍只算*一個*資料點。標註影像是*非常*「高維度」的資料點，但仍然是*單一*點。）

那麼，為何 AlexNet 沒變成過擬合、脆弱的一團亂？事實上，*很多*前沿 ANNs 都是在比其參數數量小得多的資料集上訓練。它們*不得不如此*，因為外面就是沒有那麼多資料！為什麼它們*不全*是一團脆弱的爛攤子呢？

簡單說：其實就是如此。我們才會有烏龜步槍與 AutoPilot 車禍。**這種易於過擬合的特性，使得缺乏魯棒性成了現代 AI 的*常態*。**

但既然如此，為什麼這些 AI 還能*多少*運作，儘管參數遠多於資料點？答案是：因為我們有*一些*方法可減少過擬合。（腳註列了幾個名稱：[^^combat-overfitting] 更多細節會在 AI Safety 第三部分學到！）但顯然，這些方法還不夠，對人工神經網路我們仍未找到 100% 解決此問題的辦法……至少目前還沒有。

[^double-descent]: [OpenAI（2019）新聞稿](https://openai.com/index/deep-double-descent/)：*「隨著模型規模、資料量或訓練時間增加，表現先改善、再變糟、再度改善。[…] 雖然這種行為似乎相當普遍，但我們尚未完全理解為什麼會發生。」* 完整論文見 [Nakkiran 等（2019）](https://arxiv.org/abs/1912.02292)。

[^coinrun-overfit]: [OpenAI（2018）新聞稿](https://openai.com/index/quantifying-generalization-in-reinforcement-learning/)：「**即使有 16,000 個訓練關卡，我們仍然看到過擬合！**」（強調為原文作者所加）完整論文見 [Cobbe 等（2018）](https://arxiv.org/abs/1812.02341)

[^alexnet-imagenet]: AlexNet 約有 61,000,000 個參數。([來源](http://web.archive.org/web/20240204130344/https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/tutorials/tut6_slides.pdf)，見第 10 與 11 頁之計算) 它的訓練資料庫為 ImageNet，包含 14,197,122 張人工標註的影像。([來源](https://paperswithcode.com/dataset/imagenet))

[^combat-overfitting]: 此腳註僅列方法名稱、不加解釋。AlexNet 使用了 ReLU 與 dropout。其他常見技巧包括：提前停止（early stopping）、L1/L2 正則化、資料增強（data augmentation）、雜訊注入（noise injection）。

. . .

（P.S：AI 的魯棒性也可能因「偽相關（spurious correlations）」而失敗。[:點此展開了解詳情](#SpuriousCorrelations)。在下一個問題中我們也會更深入探討「相關 vs 因果」。）

（P.P.S：還有另一種更具推測性的魯棒性失敗，稱為「本體論危機（ontological crisis）」。相關研究較少，所以我把它藏在 [:這個可展開的邊欄](#OntologicalCrisis) 裡。）

### 🤔 複習 #6

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="當有人設計一個輸入，利用 AI 的脆弱性時，這叫作："
        answer="**對抗樣本（adversarial example）**">
    </orbit-prompt>
    <orbit-prompt
        question="當 AI 在超出訓練資料範圍的新資料上失敗，這叫作："
        answer="**分佈外錯誤（out-of-distribution error, OOD）**">
    </orbit-prompt>
    <orbit-prompt
        question="訓練誤差（Training Error）是……"
        answer="模型在訓練資料上得到的誤差。">
    </orbit-prompt>
    <orbit-prompt
        question="測試誤差（Test Error）是……"
        answer="模型在*新*、未曾訓練過的資料上得到的誤差。（就像學校考試的題目不該是課堂或作業中的『訓練資料』）">
    </orbit-prompt>
    <orbit-prompt
        question="欠擬合（Underfitting）是……"
        answer="當模型太簡單，在訓練*與*真實世界（測試）都表現不佳。">
    </orbit-prompt>
    <orbit-prompt
        question="過擬合（Overfitting）是……"
        answer="當模型太複雜：訓練表現很好，但真實世界（測試）很差。">
    </orbit-prompt>
    <orbit-prompt
        question="請想像一張隨模型複雜度／參數數量而變化的訓練與測試誤差圖："
        answer=""
        answer-attachments="https://cloud-jo44rb8fw-hack-club-bot.vercel.app/0aisffs-fit_vs_params.png">
        <!-- aisffs-fit_vs_params.png -->
    </orbit-prompt>
    <orbit-prompt
        question="何時容易發生過擬合？"
        answer="當模型的參數數量*多於*我們擁有的訓練資料點數量。">
    </orbit-prompt>
    <orbit-prompt
        question="為何現代 ANNs 如此容易過擬合與脆弱？"
        answer="因為 ANNs 要有用就需要很多參數，因此也需要大量資料來防止過擬合，但要找到／產生足夠多且多樣的資料很難。">
    </orbit-prompt>
    <orbit-prompt
        question="關於現代 ANNs 的一個悖論：參數數量 vs 訓練資料數量"
        answer="許多現代 ANNs 的參數遠多於訓練資料項目，但不見得*完全*失敗。（因為有一些減少過擬合的技巧。）">
    </orbit-prompt>
</orbit-reviewarea>



<a id="problem5"></a>

## ❓ 問題五：演算法偏見（Algorithmic Bias）

![單格漫畫：機器人貓少爺攤手。字幕：我不想畫一則關於種族歧視 AI 的漫畫，抱歉不抱歉。](../media/p2/comix/Bias.png)

在第一部分裡，我舉了幾個最明顯的 AI 偏見案例，簡單複習：1980 年，用於篩選醫學院申請者的演算法會懲罰非歐洲名字。[^^bias-1] 2014 年，Amazon 曾有（後來下架）一個履歷篩選 AI，會直接歧視女性。[^^bias-2] 2018 年，MIT 研究者 Joy Buolamwini 發現，頂尖的人臉辨識 AI 在黑人與女性臉孔上的表現，明顯比在白人男性臉孔上更差。[^^bias-3]

[^bias-1]: 原始報告見英國醫學期刊：[Lowry & MacPherson（1988）](https://europepmc.org/backend/ptpmcrender.fcgi?accid=PMC2545288&blobtype=pdf)。注意此演算法並非特指使用神經網路，但它*確實*是機器學習的早期案例之一。

[^bias-2]: [Jeffrey Dastin（2018），路透社：](https://www.reuters.com/article/idUSKCN1MK0AG/) _「它會懲罰包含『women's』一詞的履歷，例如『女子西洋棋社社長』。而且據知情人士透露，它會將兩所女子學院的畢業生評分往下調。」_

[^bias-3]: 原始論文：[Buolamwini & Gebru 2018](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)。一般讀者版摘要：[Hardesty，MIT 新聞辦公室（2018）](https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212)

好吧。但*為什麼？*

一個簡單的解釋是「垃圾進、垃圾出」。或者說：「偏見進、偏見出」：

* 如果*過去*的聘用做法帶有歧視，而你訓練一個「中立」AI 去擬合過去資料，那麼——即使*現在*所有人類都完全不 [x]——AI 仍會學到並模仿過去的人類歧視。
* 如果一間 AI 公司忘了讓訓練資料中的臉部照片在種族上足夠多樣，那麼*當然*會造成某些族群的臉孔在資料中看不見，導致偏差。

這個解釋很簡單……*而且*我認為是對的。不過，讓我們把話說得更清楚些，藉由 AI 偏見來教一個統計學中的基本問題，這也*有助於*我們理解 AI 中另一個核心問題！問題在於：

**相關性無法告訴我們究竟是哪一種因果關係在作用。**

（通常，老師會警告「[:相關](#WhatIsCorrelation)不等於因果」，但嚴格說來不盡然！在數學上，「證據」的意義下，相關性*確實是*因果的證據！[^bayesian] 但它無法告訴你*是哪一種*因果關係。）

[^bayesian]: 在貝氏統計中，「證據 E 支持假說 H 為真」的程度，是用「似然比」來衡量：當 H 為真時觀察到 E 的機率，除以當 H 為假時觀察到 E 的機率。簡寫為：likelihood ratio = P(E|H)/P(E/¬H)。

    現在，把「A 與 B 存在相關」視為證據，把「A 與 B 存在*因果*連結」視為假說。因為在存在因果時你更可能觀察到相關（相較於不存在因果），因此似然比會大於 1，也就是說，*相關性是因果的證據。*（但問題在於你不知道*是哪一種*因果。）

    更多數學細節，見 [Downey（2014）](http://allendowney.blogspot.com/2014/02/correlation-is-evidence-of-causation.html) 的部落格文章（其亦為 O'Reilly《Think Bayes》作者）。想進一步了解貝氏定理，推薦 [3Blue1Brown 的視覺化介紹](https://www.youtube.com/watch?v=lG4VkPoG3ko)。

舉例：假設資料顯示身高較高的人，往往收入較高。（順帶一提，這是真的。[^^tall-rich]）我們會說：身高與收入有相關。*但光靠這些資料，無法分辨因果到底是什麼。*是變高讓你變有錢？還是變有錢讓你變高？抑或兩者*都是由某個「混雜因子」造成*？（例如：較富裕家庭的孩子在童年獲得更好的營養、教育與經濟支持，因而更高、也更富有。）

[^tall-rich]: 綜合分析：[Thompson 等（2023）](https://www.sciencedirect.com/science/article/pii/S1570677X23000540)。工資的「身高溢價」在墨西哥與亞洲最為顯著，且在男性上更為明顯。

（在此案例中，常識上大概是最後一種。不過你也*可以*實驗性地測試前兩個假說，例如：給個子矮的人穿厚底靴，看看是否能提高薪水。）

![圖解：導致 A 與 B 之間相關性的不同因果關係（非互斥）。正向因果（A 導致 B）、逆向因果（B 導致 A）、混雜因果（C 同時導致 A 與 B）、巧合（根本沒有因果）、選擇／碰撞偏差（A 與 B 影響是否會出現在你的資料集中）。](../media/p2/causal/5causal.png)

拉回主題：**我們所說的「偏見」或「歧視」，就是把別人身上的相關性誤當成因果。**

舉例來說，若你的籃球隊傾向挑高個子，我*不會*說那是（不好的那種）歧視，因為在那項運動中，身高確實*會導致*你比較會灌籃。

但如果某大學偏好高個子擔任教授……那就是不好的歧視，因為身高並不會*直接導致*你成為更好的研究者／教師。充其量，身高只是透過混雜因素（如童年營養）或自我實現的偏見[^self-fulfill-bias]，與學術能力**相關**。

[^self-fulfill-bias]: 例如：大學沒有矮個子教授 → 於是他們認為矮個子不能當教授 → 因而不僱用矮個子教授 → 於是大學沒有矮個子教授 → ∞

同樣地——我的主張是——你的性別、種族、階級、性傾向、居住地（鄉／郊／市），以及［另外 50 種分類］，並不會*直接導致*你在多數工作、或多數人格面向上更好或更差。這也是為什麼對這些特徵*直接*予以獎懲的人，我們會稱之為「有偏見」。

好，那這段長篇旁支跟 AI 有什麼關係？

**因為：目前的 AI 並沒有內建的因果概念。**[^pearl] 大型語言模型（LLMs）目前對因果推理的掌握相當脆弱、欠缺魯棒性。[^^llm-vs-causality]（這不只是對 AI 偏見不利，也會影響 AI 做新科學的能力！）

更糟的是，就其設計而言，目前最流行的機器學習技術*只能*在資料中找到相關，*而非真正的因果*。這表示 AI 會*預設*就針對某些特徵做出歧視！

所以，即使你把「不准針對性別／種族／等等歧視」硬寫進 AI，它仍很可能會找到*其他*無關緊要的相關性來產生偏見。更糟的是，現今的 AI 對尋找細微相關性有種令人發毛的拿手：它能透過你的一小段文字樣本來推測你的性別與族裔[^demographic-writing]，或是光靠*你的一張臉*，就猜你的性傾向[^orientation-face]，甚至*你的政治立場！*[^politics-face]

總之：別歧視，讓我們尊重身高受限的朋友！我為蝦米（矮個）權益自豪地站台。



[^pearl]: 參見 Judea Pearl 於 2018 年接受 Quanta Magazine 的訪談：[“To Build Truly Intelligent Machines, Teach Them Cause and Effect”](https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/)。Judea Pearl 也是上述「因果圖」的先驅之一，並協助將因果關係「數學化」。

    在該訪談中，他評論現代 AI 仍停留在「前因果、只看相關」的時代：*「所有令人印象深刻的深度學習成就，歸根究柢都只是曲線擬合。」*

[^llm-vs-causality]: [Kıcıman 等（2023）](https://arxiv.org/pdf/2305.00050) 發現，現代大型語言模型（LLMs）在先前建立的因果推理基準上*似乎*表現不錯，但魯棒性不足，且其測試混淆了「從零推斷因果」與「從訓練資料中記住的經驗事實」。
    
    （例如：若給 LLM 一個「雨天與車禍相關」的情境，模型輸出「下雨導致車禍」，這類測試無法分辨 LLM 是在*從零推斷*這個事實，還是只是*記住了*這個事實。）

    [Jin 等（2024）](https://arxiv.org/pdf/2306.05836) 建立了新因果推理基準以控制這種混淆。結果發現，現代 LLMs 在該任務上「幾乎接近隨機表現」。唉。

[^demographic-writing]: Egg Syntax（2024）：[“Language Models Model Us”](https://www.alignmentforum.org/posts/dLg7CyeTE4pqbbcnp/language-models-model-us)。作者發現，*未經校準的 GPT-3.5* 僅靠*少量文字樣本*，就能以 86% 與 82% 的準確率預測作者的性別與族裔，優於隨機！（隨機基準：性別約 50%；在美國的族裔約 60%。［美國約 60% 是白人，所以若模型一直猜「白人」，60% 的時候它都會對，次次如此。］）

    需要注意的是，該研究使用 OKCupid 的自我介紹文字，因此人們可能出於某些原因，會在字裡行間更凸顯性別刻板印象。所以作者改用美國 6 至 12 年級學生撰寫的 25,000 篇說服性文章，重作實驗。結果 GPT 的性別辨識準確率只從 86% 降到 80%，仍遠高於隨機（約 50%）！

    有趣的是，GPT 在猜測作者性傾向時表現*比隨機更差*。（GPT 的準確率：67%。「永遠猜異性戀」：93%。）但別太放心，請看下一則腳註。

[^orientation-face]: [Wang & Kosinski（2018）](http://web.archive.org/web/20240725204611/https://i.warosu.org/data/sci/img/0145/14/1653510550860.pdf)： “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation From Facial Images”。[Leuner（2019）](https://arxiv.org/pdf/1902.10739) 的重複研究顯示，模型對「化妝、眼鏡、鬍鬚、頭部姿勢」等具有不變性。AI 確實是透過下顎／鼻樑／前額形狀與皮膚明暗等線索猜測性傾向。謝了，我超不喜歡這點。

[^politics-face]: 見 [Kosinski（2021）](https://www.nature.com/articles/s41598-020-79310-1)： “Political orientation was correctly classified in 72% of liberal–conservative face pairs, remarkably better than chance (50%), human accuracy (55%), or one afforded **by a 100-item personality questionnaire (66%) [?!?!]**. Accuracy was similar across countries (the U.S., Canada, and the UK), environments (Facebook and dating websites), and when comparing faces across samples. Accuracy remained high (69%) **even when controlling for age, gender, and ethnicity [!!]**”

    我特別加粗，因為——這到底是*什麼鬼*？！怎麼會是*一張臉*比*完整的人格問卷*更能揭示你的政治立場？！甚至比你的*年齡、性別與族裔加總起來*更相關？





### 🤔 複習 #7

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="對於演算法偏見（Algorithmic Bias）的簡單解釋"
        answer="偏見進、偏見出。（訓練資料本身帶有偏見，可能源於過去的歧視，或資料選取上的偏差。）">
    </orbit-prompt>
    <orbit-prompt
        question="對於演算法偏見的更深層解釋"
        answer="當前機器學習基於*相關*而非*因果*運作。">
    </orbit-prompt>
    <orbit-prompt
        question="為什麼不懂因果的 ML/AI 會*預設*就不公平地歧視？"
        answer="因為它會使用那些與能力／性格等*相關*、但並不*直接導致*這些東西的特徵。">
    </orbit-prompt>
    <orbit-prompt
        question="若你在 A 與 B 間發現相關，三種可能的因果路徑是：（請想像）"
        answer="正向因果、逆向因果、混雜因果。（亦可能：巧合、選擇／碰撞偏差）"
        answer-attachments="https://cloud-3mydyvsbx-hack-club-bot.vercel.app/05causal.png">
        <!-- aisffs-causal.png -->
    </orbit-prompt>
    <orbit-prompt
        question="常見說法：『相關不代表因果』。技術上的補充："
        answer="在數學上，相關*確實是*因果的證據。然而，光有相關無法告訴你發生了*哪一種*因果。">
    </orbit-prompt>
    <orbit-prompt
        question="現代 AI 能捕捉到的、令人發毛的細微相關性範例"
        answer="（任一答案皆可：）AI 能僅從寫作風格預測你的性別與族裔；或僅從你的臉預測你的性傾向與政治立場（！）">
    </orbit-prompt>
</orbit-reviewarea>



<a id="problem6"></a>

## ❓ 問題六：目標錯誤泛化（Goal Mis-generalization）

![漫畫。第一格：機器人正確地拖臥室地板。第二格：機器人正確地在水槽洗盤子。第三格：人類說，好吧，你*看起來*訓練有素...現在，打掃我的房子！第四格：機器人錯誤地拖床鋪。第五格：機器人錯誤地在水槽洗筆記型電腦。](../media/p2/comix/GMG.png)

終於，我們來到 AI 對齊（AI Alignment）中最容易被誤解的概念之一！它*實在*太容易被誤解了，我甚至為此寫了這一節並畫了整篇漫畫，然後才意識到*我完全搞錯了*，不得不從頭來過。唉，好吧。([[:這裡是「刪除片段」，如果你好奇的話。]](#InnerMisalignmentDeletedScene))

總之，這個問題被稱為**目標錯誤泛化**（Goal Mis-generalization）。(它原本被稱為「內部錯位」（inner misalignment），但我覺得這個術語很令人困惑。[^mesa-optimizer])

[^mesa-optimizer]: 這個問題的理論可能性最早由 [Hubinger 等人於 2019 年](https://arxiv.org/pdf/1906.01820)描述。在論文中，他們稱這個問題為「內部錯位」，並為我們帶來了 AI 對齊界的迷因 [「欺騙性對齊的中層優化器」](https://www.astralcodexten.com/p/deceptively-aligned-mesa-optimizers)。你看，這很好笑，因為研究人員真的很不會取名字。他們取名字的功力爛到很好笑。真的很好笑。

目標錯誤泛化（Goal Mis-generalization）之所以令人困惑，部分原因是它與問題一：目標錯誤*指定*（Goal Mis-specification）和問題四：缺乏穩健性（Lack of Robustness）看起來很相似。（有些研究人員甚至質疑目標錯誤泛化與目標錯誤指定之間的區分是否有用！[^shard]）

[^shard]: 正如 Google DeepMind 的研究科學家 Alex Turner [所說](https://www.alignmentforum.org/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into)：「內部對齊與外部對齊[目標錯誤泛化與目標錯誤指定]將一個難題分解成了兩個極其困難的問題」。Alex Turner 同時也是 [Shard Theory](https://www.lesswrong.com/posts/xqkGmfikqapbJ2YMj/shard-theory-an-overview) 的先驅之一，這是一個研究「強化學習」AI 如何逐步學習人類價值觀的研究計畫。

為了釐清這個概念，讓我們來比較和對比！

**目標錯誤泛化與目標錯誤指定的區別：**

* 目標錯誤*指定*是指 AI 完全按照你的要求去做，而不是你真正想要的。
* 目標錯誤*泛化*是指 AI 在*訓練時*做了你想要的，但在現實世界/部署/測試中卻沒有。
* 注意：即使有*完美的*目標指定，你仍然可能遇到目標錯誤泛化的問題！[^perfect-specification] *你獎勵 AI 做什麼 ≠ AI 學會優化的目標。*

[^perfect-specification]: 來自 [Shah 等人 2022 年的研究](https://arxiv.org/pdf/2210.01790)：「即使在規格正確的情況下，AI 系統仍可能追求非預期的目標，這就是*目標錯誤泛化*的情況。」[強調為原文所有]

**目標錯誤泛化與穩健性的關係：**

* 目標錯誤泛化*確實是*一種穩健性失敗。具體來說，是**目標穩健性**的失敗。
* 這與**能力穩健性**的典型失敗形成對比，比如自駕車在異常光照條件下撞上卡車。
* 目標穩健性的失敗*比*能力穩健性的失敗*更糟糕*。你得到的不是一個「單純」故障的 AI，而是一個能夠*熟練執行不良目標*的 AI！

為了進一步了解目標錯誤泛化，讓我們來看一個著名的例子。2021 年，一些研究人員訓練了一個 AI 去玩一個叫做 CoinRun 的電子遊戲。[^gmg-coinrun]

[^gmg-coinrun]: 這篇論文是 Langosco 等人 2021 年的 [“Goal Misgeneralization in Deep Reinforcement Learning”](https://arxiv.org/abs/2105.14111)。下面的遊戲是 CoinRun，於 2018 年由 OpenAI 創建 ([新聞稿](https://openai.com/index/quantifying-generalization-in-reinforcement-learning/), [論文](https://arxiv.org/abs/1812.02341))。感謝 [Rob Miles 的優秀視頻](https://www.youtube.com/watch?v=zkbPdEHEyEI) 介紹了這個案例！
    （順帶一提，CoinRun 的美術資源來自慷慨又才華橫溢的創用 CC 遊戲藝術家 [Kenney.nl](https://kenney.nl/assets/series:Platformer%20Pack) 💕）

<img class="mini" alt="平台遊戲的 GIF，玩家跳過障礙物去拿取金幣。" src="../media/p2/gmg/coinrun.gif" />

重要的是：*「目標指定」—— AI 獲得的確切獎勵——對於預期任務來說是完美的。* AI 在碰到障礙物和掉落時會受到懲罰，在拿到終點的金幣時會獲得獎勵。

然而：在 AI 訓練的所有關卡中，金幣都位於關卡的*終點*。

*訓練完成後*，研究人員給了 AI 新的關卡，其中金幣位於關卡的中間...

...而 AI 會*熟練地*奔跑和跳躍避開障礙物，*錯過*金幣，仍然前往終點。

<video controls width="640">
	<source src="../media/p2/gmg/aquire_wall.mp4" type="video/mp4" />
	<p>遊戲 AI 熟練躲避陷阱但*錯過*金幣直奔終點的影片。影片標題為「無視金幣 / 獲取終點牆」</p>
</video>

*（節選自 [Rob Miles 關於內部錯位/目標錯誤泛化的優秀影片](https://www.youtube.com/watch?v=zkbPdEHEyEI)）*

所以：即使我們*正確指定*了目標（拿到金幣），AI 卻學會了一個*完全不同的*目標（到達終點），並對此進行了優化。

([:技術細節 - 我們如何知道 AI 的「真正」目標是什麼？](#GMGGoals))

但為什麼 AI 會錯誤學習目標？正如我在問題 #5 中過度解釋的：**大多數現代 AI 系統只做相關性分析，而非因果關係。** 在上述 AI 的訓練數據中，「一路走到終點」與獲得高分數有很強的相關性。在新關卡中，這種相關性消失了，但 AI 仍然保持著它的「習慣」。 

讓我們用因果關係圖來表示這一點！金幣在終點的訓練關卡*導致*了 AI 前往終點和 AI 拿到金幣...這在「前往終點」和「拿到金幣」之間造成了*混淆的相關性*。但只有「拿到金幣」*實際上會導致*獲得獎勵：

![Causal diagram visualizing above text.](../media/p2/gmg/gmg1.png)

In general, for goal mis-generalization:

![Causal diagram. (Pattern in training data) *causes a confounded correlation* between a mis-generalized goal, and a specified goal. But only the specified goal leads to the AI getting a reward.](../media/p2/gmg/gmg2.png)

Tying to catastrophic AI risk: this suggests that the risk may not be, "We asked the AI to make folks happy, so it optimized for that by wireheading us", but more, "We asked the AI to make folks happy, [correlation we don't understand], now our heads are surgically attached to giant cat masks. We're not even happy.[^everything-is-fine]")

[^everything-is-fine]: Reference to my current favorite thriller webcomic, [Everything Is Fine](https://www.webtoons.com/en/horror/everything-is-fine/list?title_no=2578) by Mike Birchall. Don't worry, this paragraph wasn't a spoiler - but it is my current fan theory.

(Note: Good Ol' Fashioned AIs *don't* have this problem, because 1) They *can't* mis-learn the goal, since you give it to them directly, and 2) They usually *can* reason about cause-and-effect. For better and worse, as elaborated in Part One, nobody's yet found a way to seamlessly merge the power of AI Logic & AI Intuition.)

. . .

Y'know, we *humans* suffer from Goal Mis-generalization, too.

These are our bad habits, formed because they *used* to be adaptive in our "training environment". It's all the clichés from therapy:

* Alyx was a "gifted kid", always praised for acing tests. In her training environment, reward correlated with "doing excellently" and "surpassing others". But this led to unhealthy habits as an adult: she avoids going past her comfort zone (where she won't "do excellently" anymore) and she covers up her mistakes while putting down others (to "surpass" them).
* Beau grew up with narcissist parents & a low-trust neighborhood. In his training environment, *negative* reward (punishment) correlated with having his guard down. So, he learnt to be unemotional. This saved his life as a kid, but led to unhealthy habits as an adult: never opening up, never letting anyone in. En garde.

(What, you weren't expecting the Catboy Comic article to cut you deep? En garde.)

So maybe, like how solving Goodhart's Law for AI may help solve it for *humans*, maybe solving goal mis-generalization for AI will also help *us*. That good ol' Human Alignment Problem.

(Aside: [: What if goal mis-generalization is actually... *good?*](#WhatIfGoalMisgeneralizationIsGood))

### 🤔 Review #8

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="Example of Goal Mis-generalization"
        answer="(Either works:) Robot mis-learns 'clean' as 'wash', CoinRun AI learns to go to end, instead of go to coin.">
    </orbit-prompt>
    <orbit-prompt
        question="Goal Mis-generalization (GMG) is when an AI does what we want in \_\_\_, but not in \_\_\_"
        answer="in training, but not in real-world / deployment / test">
    </orbit-prompt>
    <orbit-prompt
        question="You can still get goal mis-generalization even with perfect..."
        answer="...perfect goal *specification!* (What you reward the AI for doing ≠ What goal the AI learns to optimize for.)">
    </orbit-prompt>
    <orbit-prompt
        question="Two kinds of robustness failures (relevant to Goal Mis-generalization)"
        answer="Capabilities fail to generalize, Goals fail to generalize.">
    </orbit-prompt>
    <orbit-prompt
        question="Why can it be *worse* for an AI to have broken goals, but intact capabilities?"
        answer="It can now *skillfully* execute on bad goals!">
    </orbit-prompt>
    <orbit-prompt
        question="Fundamentally, why does goal mis-generalization happen?"
        answer="Because current ML/AI systems only do correlation, not causation. (If two goals are *merely correlated* in the training data, like get-end and get-coin, an AI could learn the wrong one.)">
    </orbit-prompt>
    <orbit-prompt
        question="Visualize the causal diagram behind the correlation-causation problem, that leads to goal mis-generalization:"
        answer=""
        answer-attachments="https://cloud-dlbeti1kb-hack-club-bot.vercel.app/0aisffs-causalgmg.png">
        <!-- aisffs-CausalGMG.png -->
    </orbit-prompt>
    <orbit-prompt
        question="Why don't Good Ol' Fashioned AIs have goal mis-generalization? [2 reasons]"
        answer="1\) They *can't* mis-learn the goal, since you give it to them directly, and 2\) They usually *can* reason about cause-and-effect.">
    </orbit-prompt>
    <orbit-prompt
        question="An example of goal mis-generalization in humans?"
        answer="Bad habits that used to be adaptive in our 'training data' (childhood).">
    </orbit-prompt>
</orbit-reviewarea>


---

# Humane Values

<a id="problem7"></a>

## ❓ Problem 7: What *are* humane values, anyway?

So! Let's say you've solved Problems #1 to #6. Your AI follows your commands the way you intended. It's robust, interpretable, and fully aligned with your values.

Now, security mindset: *what's the worst that could plausibly happen?*

![Comic: Human confirms Robot is fully technically aligned. Robot says, "Yes". Human cheers! They give Robot a job: "Dispose of... him". Cut to a mangled, beaten Sheriff Meowdy tied to a chair. Robot is silent in shock. Human walks away smiling, "Ta~"](../media/p2/comix/Humane.png)

Oh. Right. A *human's* values may or may not be *humane* values.

I know I've re-used that wordplay too much, but it's worth emphasizing that smart ≠ good. There's intelligent serial killers. And one of the head scientists who got us to the moon, Wernher von Braun (~pronounced "Brown"[^brown]), was literally a Nazi.

[^brown]: ["brrROOWWn"](https://en.wiktionary.org/wiki/braun#German)

But what if *very*-smart = good? Maybe a *truly* advanced AI would find moral truths, the same way it can find scientific & mathematical truths? **Is true rationality = morality? Would a true alignment to *one* human's values, necessarily lead to *humane* values?**

*This* is the fun part, where technology meets humanities, where programming meets philosophy. Let's introduce you to a sub-field of moral philosophy: **Meta-Ethics!**  If "regular" ethics asks "What should I do in this scenario?", meta-ethics asks:

_Hey, what_ is _the nature of 'moral truth', anyway?_

### Scenario #1: God(s) exist(s), morality is objective

Whether or not god(s) exist(s) is left as an exercise for the reader.

<img class="mini" src="../media/p2/misc/mini_god.png" />

But even if so, this won't ensure that an advanced AI would discover objective morality:

* Like how a very-colorblind person can't even *perceive* the difference between red & green, a machine without consciousness or soul may not *perceive* the difference between right & wrong, divine & unholy. (Reminder: "AI = a piece of cool software", and advanced "cool software" may not *necessarily* be conscious.)
* Morality may objectively exist, but not be binding on non-conscious AIs, any more than morality is binding on a rock.

### Scenario #2: God(s) don't exist, morality is still objective.

After Newton, all the philosophers got physics-envy. Like how Newton found universal physical laws grounded in math, philosophers sought to find universal moral laws grounded in rationality. If true, then a super-intelligent AI could re-discover morality!

Here's a diagram capturing the Top Three schools of thought in modern meta-ethics, *and* a causal diagram of how they relate to each other!

![Comic of Ham the Human throwing a brick at Sheriff Meowdy's head, Krazy Kat-style. Captioned: Virtue Ethics focuses on character, Deontology focuses on actions, Consequentialism focuses on results of your actions. There's a causal diagram, too: character causes actions causes results of your actions.](../media/p2/ethics/ethics.png)

Putting aside the rich debate on whether these moral philosophies work for *humans*, I'm doubtful they'll work for *AIs*. In my opinion, all "rationality-based" moral philosophies have at least one of these 3 issues:

<u>Issue 1)</u> The philosophy depends on the specifics of human nature. For example, both ancient & modern Virtue Ethics ground their moral philosophy on human needs & human psychology. Maybe that's fine for us, but these won't apply to non-human AI.

<u>Issue 2)</u> The philosophy requires you accept at least one "moral [:axiom](#axiom)", which *is not* discoverable from physical observation or rational deduction. And so, an advanced AI would not *automatically* accept it.

For example, Utilitarianism (the main type of Consequentialism) assumes only one moral axiom: *Happiness is good*.[^util] Everything else follows from this axiom! But an advanced AI may not accept this axiom in the first place, because it's not scientifically discoverable: no matter how much you poke at the neurochemistry of happiness, you won't find "goodness" hiding in the atoms.

(This is also known as Hume's "is-ought" gap[^is-ought]. And it's not just Utilitarianism, some Deontological philosophies also have this issue.[^nap])

[^util]: There are many flavors of Utilitarianism, but let's just go with this basic case as a learning-example.

[^is-ought]: The 1700's philosopher David Hume claimed (and I agree) that you can't derive *any* statements about ethical values, from *just* empirical observations.

    (*Unless* you smuggle in values with petty language-tricks, like "Hans is a Kraut, therefore Hans is bad", where "Kraut" has a smuggled negative connotation. Or, "Cyanide is natural, therefore cyanide is good", where "natural" has a smuggled positive connotation. Believing 'natural = good' is also known as the 'naturalistic fallacy'.)
    
    See [Wikipedia](https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem) to learn more.

[^nap]: Specifically, I'm thinking of the Deontological philosophy of (some) libertarians. The philosophy assumes one moral axiom, the Non-Aggression Principle (~"don't initiate non-consensual harm, except if it's in proportion to preventing/punishing other non-consensual harm"), then derives the rest of its philosophy from that.
    
    Whether or not this axiom's good for *humans* is out of scope of this article; my point is that this axiom's not *scientifically or mathematically* discoverable, hence no guarantee an advanced AI would re-discover it.

<u>Issue 3)</u> The philosophy *claims* to be fully grounded in rationality, no need for an extra "moral axiom" -- but it either sneaks in a moral axiom anyway, or the philosophy "proves too much".

For example, consider Immanuel Kant's Deontological argument for why it's irrational/immoral to steal. *If* it were rational to steal, all rational beings would steal, so there'd be nothing left to steal – a logical contradiction! Therefore, it must be *irrational* to steal, it's *always* immoral.

Another example: *If* it's rational to lie, all rational beings would lie, thus not trust each others' words, thus no reason to bother lying — a logical contradiction! Therefore, it's irrational, and *always* immoral, to lie.

But c'mon, really? *Always?* Even stealing from a private restaurant dumpster to not starve, or lying to the Taliban to protect your gay brother?[^kant-extreme] Are *you* the literal-rule-following robot? Besides, by the same logic, Sir Kant, it's irrational/immoral to be a full-time philosopher. *If* it were rational to do full-time philosophy, nobody would grow the crops, so we'd all starve to death, so we can't do full-time philosophy — a logical contradiction! Therefore... you get the idea. (Other deontological theories fall into similar traps.)

[^kant-extreme]: Yes, Kant really *was* this extreme. He believed one should never lie, even to save a life. (Secondary source: [Klempner 2015](https://web.archive.org/web/20230605071851/https://askaphilosopher.org/2015/08/13/kant-on-lies-and-the-axe-man/)) That said, in his personal life, Kant freely told half-truths & lies by omission.

Long story short, I think there's reasonable doubt that rationality *is* morality... at least for non-human AIs.

([:Further resources to learn about meta-ethics!](#MoreMetaEthics) If you couldn't tell, this topic is one of my special interests.)

### Scenario #3: Morality is relative!

That's an absolute statement, you dingus.

### Scenario #4: Morality isn't real, but it's game-theoretically useful to pretend it is.

<img class="mini" src="../media/p2/misc/mini_hobbes.png" />

*(if i have to explain the joke it's not funny[^the-joke])*

[^the-joke]: It's the face of [Hobbes](https://en.wikipedia.org/wiki/Thomas_Hobbes), one of the pioneers of social contract theory, pasted over [Hobbes](https://en.wikipedia.org/wiki/Calvin_and_Hobbes), the tiger from the beloved comic strip. Ha. Ha ha. Ha ha ha.

Let's say my neighbor has an awesome raccoon costume. I'd like to steal it. However, I don't want people to steal *my* stuff, so I "consent" to the State taking a cut of my money, to fund a police department, to stop people from stealing stuff in general. And thus, we arrive at a compromise, a social contract:

"Thou Shalt Not Steal" (or the cops will get you).

The above is a toy example of the **social contract theory** of ethics. In this theory, there *is* no objective morality, but it's useful to pretend it exists, to coordinate on a social contract. It's the same way there's no objective reason a red octagon *has* to mean "STOP", but we all agree it does, so we can coordinate on not crashing our cars.

([:extra 'deleted scenes' from this section](#SocialContract))

So: can *this* be the basis of "rational, objective ethics" for an advanced AI?  The game theory of social contracts?

As long as an AI isn't *too* powerful, sure!  We don't have to *win* against an AI to impose a cost on it, and if we can impose a cost on it, then we have leverage to enforce a contract. And if there turn out to be *multiple* advanced AIs with roughly equal power, we may have a precariously balanced "multipolar" world. (Sidenote: [:could we trade with super-human AIs?](#TradingWithAdvancedAI))

But if the multiple AIs create a *new* contract to collude with each other against us... or if a single AI gets so powerful no entity can enforce a contract against it...

Then, well, back to square one.

### 🤔 Review #9

<orbit-reviewarea color="violet">
(ORBIT CARDS HERE)
    <orbit-prompt
        question="Example of a human that shows that Smart ≠ Good."
        answer="Wernher von Braun (~pronounced 'Brown'), rocket scientist who got us to the moon & literal Nazi.">
    </orbit-prompt>
    <orbit-prompt
        question="If 'regular' ethics (applied ethics) asks, 'what should I do in this scenario?' **Meta-ethics** asks,"
        answer="'What is the nature of moral truth, anyway? (e.g. Is it universal/objective, like math & physics?)'">
    </orbit-prompt>
    <orbit-prompt
        question="Even if moral truth objectively existed, why might an advanced general AI still not be moral?"
        answer="(Any of the following:) Without consciousness/a soul, it may not even *perceive* moral truths. Or moral rules may not even *apply* to non-conscious agents.">
    </orbit-prompt>
    <orbit-prompt
        question="What are the Big Three approaches to grounding morality in secular rationality?"
        answer="Virtue Ethics, Deontology, Utilitarianism"
        answer-attachments="https://cloud-b8utffjti-hack-club-bot.vercel.app/0ethics.png">
    </orbit-prompt>
    <orbit-prompt
        question="What's the 'social contract' theory of ethics say?"
        answer="It says morality doesn't objectively exist, but it's useful to pretend it does, so we can coordinate on a 'contract', to attain our own interests.">
    </orbit-prompt>
    <orbit-prompt
        question="Why may an advanced AI not be beholden to a social contract?"
        answer="If it gets *too* powerful, we can't enforce a contract against it.">
    </orbit-prompt>
</orbit-reviewarea>

### Scenario #5: Morality isn't real, and it's _not even useful_ to pretend it is.

Well, crap.

In this case, there are no "humane values", only *specific humans'* values. There's no humane alignment, *only* technical alignment. There's no "I should", *only* "I want".

So, *whose* wants do we want to technically-align an advanced AI to?

The tech-company billionaires who run the biggest AI labs? The US government, whose ruling party can shift dramatically every 4 years? The EU? The UN? The IMF? NATO? Some other acronym? I think most people worldwide would be uncomfortable with *any* of that, to understate it. So, *whose* wants?

"Everybody's!" you say? An AI that gives equal weight to all 8 billion of us, a full democracy of the world? I remind you that the majority of people worldwide believe being gay is "never justifiable".[^owid-gay] The majority disapproved of Martin Luther King during his lifetime.[^mlk] Direct democracy would have delayed inter-racial marriage in the US by over a generation.[^inter-racial-marriage] *Equality would not survive an equal vote.* To be clear, I'm *not* saying my specific cultural group is the pinnacle of morality -- I'm saying everywhere, every-when, every culture has struggled with hypocrisy & inhumanity, and "democracy" doesn't solve that.

[^owid-gay]: Source: [Our World In Data](https://ourworldindata.org/grapher/share-of-people-who-think-homosexuality-is-never-justified?tab=chart&country=CHN~IND~USA~IDN~PAK~BRA~NGA~BGD~RUS~MEX). The 10 countries selected in that chart are the Top 10 most populous countries; note how 7 out of 10 of them reported 75%+ agreement with the statement that "homosexuality is never justifiable". I won't do the full math, but I'm pretty sure if you took the top 30 most populous countries, multiply by what ratio agrees, then add 'em up... you'd get over 4 billion agreeing, a majority, out of the 8 billion folks on Earth.

[^mlk]: Two years before MLK's death in 1968, Gallup polls showed that in 1966, 63% of Americans felt *unfavorably* of MLK, while 33% felt favorably, an almost 2-to-1 ratio. If we only consider "highly un/favorable", the ratio gets worse: 44% to 12%, almost 4-to-1. Source: [Pew Research Center](https://www.pewresearch.org/short-reads/2023/08/10/how-public-attitudes-toward-martin-luther-king-jr-have-changed-since-the-1960s/), see first figure.

[^inter-racial-marriage]: The US Supreme Court legalized inter-racial marriage nationwide in the 1967 case, *Loving vs Virginia.* [According to Gallup polls](https://news.gallup.com/poll/354638/approval-interracial-marriage-new-high.aspx), the majority of US folks approved of Black-White marriage only in 1997, *30 years later*. [Generational cohorts](https://en.wikipedia.org/wiki/Generation#/media/File:Generation_timeline.svg) are usually defined to be ~20 years long, so, that's 1½ generations later. (Hat tip to [this xkcd](https://xkcd.com/1431/) for teaching me about this.)

"Fine," you concede, "everybody's values & wants, *but* if we cured all the trauma that makes us bigoted, if we were all wise & compassionate, and really got to know the facts and each other." This *is* one of the better proposals (which we'll cover in AI Safety Part 3[^cev]), but it's still a tall order, and kicks the question down: *whose* definition of "wise", or "compassionate"?

[^cev]: Spoiler: this idea is similar to (but not exactly the same) as *Coherent Extrapolated Volition* ([Yudkowsky 2004](https://intelligence.org/files/CEV.pdf)), which we'll learn more in AI Safety for Fleshy Humans Part 3!

. . .

![Left: rocket being built, labeled, "technical alignment: how to robustly aim AI at any target *at all*". Right: the moon, labeled, "whose values: what *should* be our target?"](../media/p2/ethics/aim_vs_target.png)

An anthropological anecdote:

A few years ago, in the AI Alignment community, it seemed the consensus was "technical alignment" was higher-priority than figuring out "humane values". A common analogy given: Imagine it's the early days of rocket engineering. It's useless to argue *where* we should go with the rocket (The moon? Mars? Venus?), since *given our current technical knowledge, by default*, a powerful rocket will just explode and burn everyone on the ground.

But post-ChatGPT, I've noticed more recognition[^whose-values] we *should also* prioritize the "humane values" question. To extend the above analogy: It's like folks realized that *given our current political situation, by default*, the rockets will be used by Great Powers to bomb each other, not for exploring space. (To be explicit: it seems by default, technically-aligned AI will be used for war, & making us consume more products.)

[^whose-values]: For example, Ajeya Cotra 2023: ["Aligned" shouldn't be a synonym for "good"](https://www.planned-obsolescence.org/aligned-vs-good/), Michael Chen 2024: [AI Alignment is Not Enough to Make the Future Go Well](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4684068), Andrew Critch 2024: [Safety isn’t safety without a social model (or: dispelling the myth of per se technical safety)](https://www.alignmentforum.org/posts/F2voF4pr3BfejJawL/safety-isn-t-safety-without-a-social-model-or-dispelling-the).

(Reminder from Part One: [:the obvious ways](#WaysToMakeHumaneAIGoingWrong) of specifying "human flourishing", or even straightforward codes-of-ethics like [:Asimov's Three Laws](#AsimovsLaws), all break down.)

So, if moral truth doesn't exist — or if it does, but a machine can't perceive it / derive it / be bound to it — then we need to get the major AI creators to pre-commit to aligning their advanced AIs to *some* not-terrible list of values.

This is the kind of problem that's hardest for technical-minded engineers to admit: *it's a problem of politics, not programming.*

Let's close with [one of my favorite songs](https://www.youtube.com/watch?v=TjDEsGZLbio) — about ethics, rockets, and choosing where we want our technology to take us:

> 🎵 *“Once ze rockets are up,*    
> *Who cares where zey come down?*    
> *Zat's not my department”,*    
> *says Wernher von Braun.* 🎵

... yeah, we're not doing a flashcard review for this one.

<img class="img-splash" src="../media/p2/ethics/rockets.png" />


---

# Summary of Part Two

Well read, friend!  Today you learnt about all the parts of the AI Value Alignment Problem, in all its gory detail.  Not only that, you got a crash course in: security mindset, game theory, economics, machine learning, statistics, causal inference, and even meta-ethics in philosophy!

(If you skipped the flashcards & would like to review them now, click the Table of Contents icon in the right sidebar, then click the "🤔 Review" links. Alternatively, download the [Anki deck for Part Two](https://ankiweb.net/shared/info/808506727).)

As a recap, here's how it all connects:

![Same breakdown chart of the Value Alignment Problem from the start of Part 2, except with labels showing the connection to safety engineering, game theory, economics, machine learning, statistics, causal inference, and meta-ethics.](../media/p2/breakdown/breakdown0002.png)

**IN SUM:**

* 🙀 **To engineer safe, helpful things,** we gotta be paranoid. Ask, *What's the worst that could (plausibly) happen?*, then fix it in advance. The optimist invents the airplane, the pessimist invents the parachute.
* ⚙️ **The main problems with AI Logic** can be understood with Goodhart's Law and game theory.
    * 👀 VISUAL: You can use "game trees" to understand instrumental convergence & wireheading-avoidance.
* 💭 **The main problems with AI "Intuition"** are the same as the problems of 'fitting curves to data points' (uninterpretable, overfitting), and the 'correlation doesn't tell you about the kind of causation' problem (which leads to discrimination & mis-generalization).
    * 👀 VISUAL: You can use "causal diagrams" to understand correlation & causation.
* 💖 **The problem of "which values"** is the millennia-old problem of moral philosophy. Good luck.

. . .

*"A problem well-stated is a problem half-solved."*

The other half is, well, solving it.

After two long chapters, finally, we can understand the top proposed solutions to *every* sub-part of the Alignment Problem! Stay tuned for the grand finale, Part 3, of AI Safety for Fleshy Humans, coming **December 2024!**

Sign up to be notified when the final part is out: 👇

{% include 'templates/signup.html' %}

In the meantime, check out the other stuff I've made, or learn more about Hack Club, in the credits below! <span class='NEXT_ARROW'><img src="../media/misc/NEXT_ARROW.png"></span>























#### :x Ways to make "Humane AI" going wrong

(copy-pasted from [Part One](https://aisafety.dance/p1/))

Here's some rules you'd *think* would lead to humane-with-an-e AI, but if taken literally, would go awry:

* <u>"Make humans happy"</u> → Doctor-Bot surgically floods your brain with happy chemical-signals. You grin at a wall all day.
* <u>"Don't harm humans without their consent"</u> → Firefighter-Bot refuses to pull you out of a burning wreck, because it'll dislocate your shoulder. You're unconscious, so can't be asked to consent to it.
* <u>"Obey the law"</u> → Governments & corporations find loopholes in the law all the time. Also, many laws are unjust.
* <u>"Obey this religious / philosophical / constitutional text"</u> or <u>"Follow this list of virtues"</u> → As history shows: give 10 people the same text, and they'll interpret it 11 different ways.
* <u>"Follow common sense"</u> or <u>"Follow expert consensus"</u> → "Slavery is natural and good" used to be common sense *and* expert consensus *and* the law. An AI told to follow common-sense/experts/law would've fought *for* slavery two centuries ago... and would fight for any unjust status-quos *now*.

(Important note! That last example proves: even if we got an AI to learn "common sense", that *could still lead to an unsafe, unethical AI*... because a lot of factually/morally wrong ideas *are* "common sense".)


#### :x Story of passive prediction leading to harm

Let's say there's ~~an AI~~ a piece of software designed for one task: Predict what videos someone would watch. Then, these predictions are shown to users under Videos You May Like.

TO EMPHASIZE: this ~~AI~~ software is NOT directly optimizing for engagement or views, it's ONLY optimizing for correct predictions. And! This software is NOT planning ahead, it's just calculating correlations on the fly. I want to over-emphasize this: *even without a malicious goal or planning-ahead capacity, a piece of software can still lead to bad, un-intended results.*

Here's how: let's say the website tests multiple versions of this software (known as A/B testing). By chance, Predictor A is more biased to predict "curiosity" videos, and Predictor B is more biased to predict "angry politics" videos. These two pieces of software are otherwise *equally accurate.*

And yet... Predictor B will do better! Why? Because users who get Predictor A will get more "curiosity" videos recommended to them, so they become more open-minded, *so they become harder to predict*. Inversely, users who get Predictor B will get more "angry politics" videos recommended to them, so they become more close-minded, *so they become easier to predict*. AGAIN: THIS SOFTWARE DOES NOT PLAN AHEAD AND IS *NOT* MAXIMIZING ENGAGEMENT, *ONLY* PREDICTIVE ACCURACY.

And yet! Through more and more rounds of A/B testing, the Predictors get more and more biased towards videos that make their users easier to predict.

...I guess this outcome isn't *that* surprising given (gestures at the internet), but still, *personally*, I found this example shocking. It showed me how easily bad unintended results can happen, *even without* malicious goals or advanced AI planning-capacities!

(An anecdote I can't and won't back with a source: I've been told that a top AI researcher calls this problem the "Your Grandmother Becomes A Nazi Storm-Trooper" problem.)



#### :x Difference Between ML And AI

![Tweet that reads, "Difference between machine learning and AI: If it's written in Python, it's probably machine learning. If it's written in PowerPoint, it's probably AI."](../media/p2/misc/ml_vs_ai.png)
*([from @matvelloso](https://x.com/matvelloso/status/1065778379612282885))*

But seriously, here's the Venn Diagram explaining the difference between AI/GOFAI/ML/Deep Learning again, from Part One:

![Venn diagram. In AI, there's Good Ol' Fashioned AI and Machine Learning. In Machine Learning, there's Deep Learning.](https://aisafety.dance/media/p1/venn.png)



#### :x Ontological Crisis

An "Ontological Crisis" for an AI ([de Blanc 2011](https://arxiv.org/pdf/1105.3821)) is when an AI learns a new model of the world, and their current goals no longer make sense. As an analogy: Imagine your only goal in life is to do what would please Santa Claus. Then one day, you learn Santa Claus isn't real. Your one goal now doesn't even make *sense*. It's not clear what you'd do next: Nothing? Start worshipping Krampus? K.Y.S.? (Krampus, You Serve?)

To take a speculative AI case: Let's say we instruct an AI to respect our free will and personhood. What happens if, after learning more neuroscience, the AI comes to the conclusion that free will doesn't exist, and neither does personhood ('the ego is an illusion', etc)? What  *should* an AI do about goals that don't even make sense under a new worldview?

This isn't even one of those cases where humans can claim superior performance over AI, because (as far as I can tell) humans' response to worldview-shattering evidence is usually: 1) rationalize it away, or 2) crumple into a mess.

Two possible solutions:

a) As suggested in the linked paper above, maybe we can make an AI switch to the "next closest goal"? For example, if an AI designed to support our free will learns "free will" isn't real, it could switch to the next-closest goal, like, "help human brains generate actions that the brain assigns positive value". (even though these generated actions & assigned values are still fully determined by the laws of physics.)

b) As suggested by [Shard Theory](https://www.greaterwrong.com/posts/8ccTZ9ZxpJrvnxt4F/shard-theory-in-nine-theses-a-distillation-and-critical)): An agent can have multiple goals, and when one goal 'dies', the others grow in its place. For example, if serving Santa was 95% of my motivation, learning Santa's not real would make me lose 95% of the meaning in my life... but via the process of grieving, the remaining 5% of my motivations (friendship, learning, fun, etc) would grow to fill the space in my heart.



#### :x Inner Misalignment: Deleted Scene

*THIS COMIC IS NOT CORRECT. THIS IS A DELETED SCENE.*

![INCORRECT COMIC. Human tells Robot to clean house. Robot reasons human causes mess, so need to get rid of Human. Robot creates a mini-bot, tells it to "get rid of Human". Mini-bot blasts Human to bits. Robot freaks out... because the house is now MESSIER!](../media/p2/misc/WRONG_Inner.png)

*I was under the misconception, at the time I drew this comic, that Inner Misalignment happens when an AI makes helper AIs (like how a computer process that can spin up sub-processes). Then, the first AI would face the same alignment problem humans do: orders being taken literally, not as intended.*

*Ironically, I interpreted "inner misalignment" literally, not as intended. The previous paragraph COULD still be a possible failure mode, but it's NOT what the authors of Inner Misalignment meant.*

*Anyway, DELETED SCENE PLEASE IGNORE.*



#### :x What if Goal Misgeneralization is good?

Two "goal misgeneralization is good actually" takes:

1) My personal values are evolution's mis-generalized goals
2) Shard theory: we can use goal mis-generalization to work around goal mis-specification

. . .

1\) Consider this cat:

![Amazing photo of the best cat in the world](../media/p1/best_cat.png)

That's right, they're back! Callback cat from Part One.

Anyway, *why* do we find this cat adorable? To pull an evolutionary-psychology story out my bum: Evolution "wanted" us to stick through the hard job of raising our offspring, so Evolution made us love small helpless creatures with big heads & eyes. However, we "mis-generalized" this goal, so now we also feel "awww" towards creatures that won't spread our genes, such as kitty cats.

(As science power-couple John Tooby & Leda Cosmides [famously said](https://web.archive.org/web/20120318083503id_/http://eugen.leitl.org/striz/striz.org/docs/tooby-1992-pfc.pdf): **we are adaptation-executers, not fitness-maximizers.**)

Another example: it's plausible that our innate sense of "morality" evolved to help us thrive in hunter-gatherer communities of [up to 1,000 people](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2706200/). However, we "mis-generalized" this goal, and now many folks sincerely advocate for the human rights of the 8,000,000,000+ people across the globe we'll never know or meet — that's *far beyond* one's closest 1,000 contacts!

Now: even though *I explicitly know* that my values come from a "mis-generalization" of crass Darwinian instincts... am I going to give up finding cats adorable? Am I going to give up valuing human rights?

*Hell* naw. And if Evolution tries to pry those values from humanity, well, then we'll just have to kill Evolution first.

. . .

2\) Shard Theory is an ongoing research program ([Distillation & critical summary by Lawrence Chan 2022](https://www.greaterwrong.com/posts/8ccTZ9ZxpJrvnxt4F/shard-theory-in-nine-theses-a-distillation-and-critical)) that tries to *use* goal mis-generalization (inner misalignment) to *solve* goal specification (outer alignment).

So: you know all those problems with giving AIs goals? Goodharting, Instrumental Convergence, etc? Well, Shard Theory suggests: *we don't need to make goal-maximizers, we can make adaptation-executers!* Except they call "adaptation-executers" a snappier name: "Shards", lil' reflex-pieces of a neural network that go "If X, Then do Y".

This research program's hope: eventually, we understand Shards well enough to shape them however we want with reward/punishment, the same way we can reward-shape dolphins at Sea World to do complex tricks. Furthermore, it hopes we can get AIs to "intrinsically" value the flourishing of all sentient beings, the same way *I* intrinsically value cats and human rights.



#### :x Asimov's Laws

Here were [Asimov's Three Laws of Robotics](https://en.wikipedia.org/wiki/Laws_of_robotics#Isaac_Asimov's_%22Three_Laws_of_Robotics%22):

> 1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
> 2. A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.
> 3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.

A seemingly-benign "code of ethics" for robotics! Anyway, Asimov's stories were about how these laws, interpreted exactly to the letter, go awry.

For example, it could lead to a secret cabal of robots censoring & undermining anti-robot human activist groups. Why? Third Law *requires* robots protect themselves, hence undermining anti-robot groups. Second Law means they have to obey orders, which is *why* they keep themselves secret: can't disobey direct orders to stop, if nobody even knows they're doing it! And as for First Law, censorship & sabotage isn't "harm" in a physical sense.

(This example is similar to, but not exactly, the plot of Asimov's short story, [The Evitable Conflict](https://en.wikipedia.org/wiki/The_Evitable_Conflict))



#### :x Trading with Advanced AIs

An interesting tidbit from Economics is [Comparative Advantage](https://en.wikipedia.org/wiki/Comparative_advantage): Even if country A is better at producing *every* good than country B, they'd *still* be better off trading, because country B can be *comparatively* better at producing some goods.

Concrete toy example: Alicestan can make 4 Xylophones or 2 Yo-yos for one unit of capital. Bobstan can make 1 Xylophone or 1 Yo-yo for one unit of capital.

Alicestan has an *absolute* advantage in both Xylophones & Yo-yos, but Bobstan has a *comparative* advantage in Yo-yos! See, for Alice, making a Yo-yo means giving up 2 Xylophones (4/2 = 2), but for Bob, it only means giving up 1 Xylophone (1/1 = 1).

So the ideal contract is: Alicestan specializes in making Xylophones, Bobstan specializes in making Yo-yos, then they trade. This is more efficient for Alicestan than making the Yo-yos herself!

. . .

How this connects to AI: even if an advanced AI has an *absolute* advantage over humans over all cognitive tasks, we will still have *comparative* advantages in some stuff, so, we may still be able to trade!

*However*...

As any history textbook shows, Alicestan might have an even more "efficient" way to gain wealth: just plunder & pillage Bobstan. If one entity is *a lot more* powerful than another, simply bulldozing the other can be the "most efficient" action.

So, in sum, just solve the AI Alignment problem(s), please.



#### :x GMG Goals

We don't.

At least, nobody (yet) has found a non-contrived confirmed example of an ANN with a "goal" it's explicitly comparing different outcomes/actions against. It's unknown if we haven't found this because our ANN interpretability techniques aren't good enough, or they're just not there.

But for now, we can **take the "intentional stance", and say if an AI is acting *as if* it has some Goal/Reward X.** (X is part of its ["Consistent Reward Set"](https://openreview.net/pdf?id=pErdjpoc-w3))

For example, if we see an AI *competently* dodging obstacles to get to the end of a level, we can say it's acting *as if* its goal is to get to the end of a level. Even if the AI is "really" just a bunch of goal-less reflexes like "If gap, Then jump over", etc.

(Hey, maybe deep down, *all* our human goals are made up of goal-less mental reflexes?? e.g. "I want to write a good explainer article" => "If sentence is abstract, Then put a concrete example nearby", etc...)



#### :x axiom

In math/logic, an "axiom" is something that *has* to be assumed in order to prove stuff, but *it itself* can't be proven.

([:example in geometry](#axiom2))


#### :x axiom 2

For example, in "Euclidean" geometry, there's [an infamous axiom about parallel lines](https://mathworld.wolfram.com/ParallelPostulate.html): Given a line A and point B, there's *one and only one* line that's parallel to A and goes through B. You *need* this axiom to prove stuff like "the inside angles of a triangle sum up to 180°".

Philosophers like Kant\* believed in the absolute logical certainty of Euclid's geometry. Then in the early 1900's, Einstein blew it up by showing *our own universe* is non-Euclidean. You *really can* have real-world triangles whose inner angles don't sum to 180°.

\* (well, maybe. Ye Olde Philosophers wrote with a lot of interpretation wiggle-room. See [Palmquist 1990](http://web.archive.org/web/20240730051708/https://scholars.hkbu.edu.hk/ws/portalfiles/portal/55351095/RO_rel_ja-17_JA030471.pdf) for the debate on Kant's beliefs)

**Point is: You can't get something from nothing.** You need *at least one axiom* to prove other stuff, but by definition, *that* axiom can't be proven. And in fact, as the history of Euclidean geometry shows, the axiom may very well not fit our universe.


#### :x More Meta Ethics

For a good layperson introduction to the Big Three in meta-ethics, I highly recommend [Crash Course Philosophy's bite-sized video series, episodes 32 to 38](https://thecrashcourse.com/topic/philosophy/). 💖 (each episode is ~10 min long)

For a deeper technical dive, Stanford Encyclopedia of Philosophy's great! Here's their entries on [Virtue Ethics](https://plato.stanford.edu/entries/ethics-virtue/), [Deontology](https://plato.stanford.edu/entries/ethics-deontological/), and [Consequentialism](https://plato.stanford.edu/entries/consequentialism/). (each entry is ~60 min to read, but you can skim.)



#### :x Social Contract

(Some deleted paragraphs from the "Social Contract" section, because I was getting *way* too off-tangent)

. . .

I mean, *ideally*, I'd like to steal others' stuff but they can't steal back... but nobody else would fund that, and I'm not rich enough to single-handedly fund a police force. And even if I was, that'd just incentivize the peasants to kill me.

What if a majority of people stole from a minority? They *could* get away with that for a while... but 1) The minority will fight back, and that's costly, and 2) ~Everybody is in *some* statistical minority group (age, gender, race, orientation, class), so this will backfire on me. *"[First they came](https://en.wikipedia.org/wiki/First_they_came_...) for [small group], and I did not speak out because I was not in [small group]. Repeat N times. Then they came for me, and there was no one left to speak for me."*

. . .

When people say, "We all have equal rights!" it's just shorthand for, "It's easiest to fund law-enforcement when the laws have the widest appeal".

Might makes right... but neutrality makes coalitions, and coalitions make might.

Isn't this horrifying? Yes, horrifying...ly inefficient! Using *external* threats & bribes isn't as efficient as giving people *internal* threats & bribes: that is, moral shame & pride. So, we'll use Pavlovian conditioning, in the form of stories & lessons, to make people become their own police. The contract will become *so* internalized, people will forget where morality comes from. And if they ever learn where it *actually* comes from, they'll auto-reject it in repulsion — the same way lots of people love sausage, but don't want to know how it's made.

(To be clear: I am not necessarily *endorsing* social contract theory, I'm just *explaining* it, in a funny 2edgy4me manner.)

. . .

Social Contract Theory also solves the inflexibility of Deontology: every real-life contract has exception clauses. Sure, "thou shalt not steal", but if you're starving and the food was going to go to waste anyway, look, no person with a functioning heart & brain will enforce that contract.

I don't necessarily endorse Social Contract Theory, but I *do* endorse sticking my tongue out at Deontology at every opportunity. Could even say, it's my moral duty to do so.


#### :x Spurious Correlations

A "spurious correlation" is when two things happen together a lot (correlation), but there's not a meaningful cause-and-effect between them ("spurious").

Modern machine learning *only* picks up on correlations, not causations - this causes AI to be tricked often by spurious correlations. One extreme famous example from [(Ribeiro, Singh & Guestrin 2016)](https://dl.acm.org/doi/pdf/10.1145/2939672.2939778): they trained an AI to distinguish between wolves & huskies, and it *seemed* to have a high success rate... but upon inspection, it turned out the AI was detecting wolves not by their fur or faces... but by the surrounding *snow*. This was because all the training photos of wolves were in a snowy setting. Thus, this caused a "spurious correlation" the AI was fooled by.

![Figure from the paper, showing that wolves are detected not by their actual wolf-like-ness, but SNOW.](../media/p2/misc/wolf_snow.png)

And this is one of the *hopeful* examples. Because in this case, the researchers could tell what the spurious correlation was. As for the above turtle-gun, I can't tell *why* those random-looking smudges correlate with "rifle".


#### :x Pi-pocalypse

(copy-pasted from [Part One](https://aisafety.dance/p1/))

> Once upon a time, an advanced (but not super-human) AI was given a seemingly innocent goal: calculate digits of pi.
> 
> Things starts reasonably. The AI writes a program to calculate digits of pi. Then, it writes more and more efficient programs, to better calculate digits of pi.
> 
> Eventually, the AI (correctly!) deduces that it can maximize calculations by getting more computational resources. Maybe even by stealing them. So, the AI hacks the computer it's running on, escapes onto the internet via a computer virus, and hijacks millions of computers around the world, all as one massively connected bot-net... just to calculate digits of pi.
> 
> Oh, and the AI (correctly!) deduces it can't calculate pi if the humans shut it down, so it decides to hold a few hospitals & power grids hostage. Y'know, as "insurance".
> 
> As thus the Pi-pocalypse was born. The End.

![evil pi-creature laughing](../media/p1/pi.png)

#### :x What Is Correlation

If two things seem to happen together a lot, we say there's a "correlation" between them. For example, taller people also tend to be heavier people, so we say there's a correlation between height & weight.